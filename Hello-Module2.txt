	Date : 06/04/2024


Series : one dimesional structure - single raw from data frame
Numpy : export your pd data frame to numpy array to pass to the ML algo

heatmap : color of cell will show average value at each point. 
jointplot : scatter plot with histogram in one.
pairplot : every possible combination of attribute visualization at once(so many graphs in one)

$$$$$ prepatring data and creating model at high level

- df.describe() will help to check missing value for each column
- df[['colo1','colo2','colo3','colo4']].values => convert pd dataframe to numpy array
- apply scaler to normalize
- create_model func => for keras model defination, compile model and then return model
- kerasclassifier is used to wrap tensorflow keras model in it. Which is called estimater
- result of KerasClassifier. Which will be passed to cross_val_score fun
- for evaluating model cross_val_score from model_selection(which will do train-test split and train model, evaluate on test data)
- result of cross_val_score can  be used for getting average of all those evaluation(called accuracy)


$$$$ Type of Data

## quntitative dataS
- discrete data : iteger data - no of children, no of purchase, no of head flip, page load no
- continuouse data : infinite possibility of data number like rain fall, salary, time, height,  stock price
## categorical dataS - nominal comes from "name" to help you remember
- qualitative data (data dont have mathematical meaning)
- binary(0/1)
- colors
- we can assign nos to them  but no doesnt have any meaning
## ordinal data
- mixture of categorical and numerical data
- category which have num assigned to it which has actual meaning
- like rating 1,2,3,4,5


$$$$ Data Distribution

## Normal Distribution - bell shaped curve centered around 0  - for continous data
- probability dencity function
- between mean and one/two/three deviation diff no of %(in decrese mode) of values are lies in that range
- solid curve decribe probability of range of values happening in continuous data

For Discrete Data
(
## Probability Mass Function(if Discreate -integer data) its Poisson distribution
- with help of histogram - bin setting(for int values)
- still will form normal distribution 
- solid curve decribe probability of given discrete value occuring in dataset

Poisson Distribution
- some series of event happend which is successful or failure (average of no of event - descreet) over the time or distance

Binomial Distribution
- Also Discrete probability distribution
- no of times head/tail comes while flipping

Bernoulli Distribution
- special case of binomial distribution
- has a single trial(n=1)
- binomial dist = sum(bernoulli distr)
)

$$$$ Time Series

Trends 
- data points go down/up/straight over period of time
Seasonality
- data point go down/up/straight at some specific time period(month/year/weeek)
Both can be together
- subtract seasonlity from data you will have trends
Noise
- some variation which are random in nature called noise
additive model
- Time series data = Trends + Seasonality + noice
- seasonal variation is constant
multiplicative model
- Time series data = Trends * Seasonality * noice
- trends variation amplify seasonality and noise so multiplication
- trends increase seasonality increase


$$$$$$$$ Amazon Athena

- Query service for s3 - no need to load data
- ad-hoc query against data in S3
- Presto under hood - Presto is a distributed query engine for big data using the SQL query language
- serverless
- support json, csv, parquet, avaro, orc
- unstructured, semi strucutured , structured data formate support
Usage
- query web log, cloudtrail, cloud front, vpc, or data before storing to redshift
- integration with jupyter, zeppline kind of notebook
- ODBC/JDBC (java/open db connection) protocol use to integration with visualization tool like quick site

How integrated with aws

amazon s3 + aws glue + amazon athena + amazon quicksight

data in s3
create metadata by aws glue - define columns n all 
athena perform query based on the columns
based on the query we can have some visulization of data

charges - 5$/terabyte
add/update/delete query no charge
cancel/succes query charge
fail query no charge
column formate data save money - parquet/orc(30-90% better performance)

# Security

- IAM policy, s3 bucket policy, ACLs
- amazon athena full access
- AWSQuicksightAthenaAccess
- in s3 at rest encrypt athena result - SSE-S3, SSE-KMS, CSE-KMS
- cross acount access in s3
- transport layer security encrypt between athena and s3 in transit


$$$$$$ Amazon QuickSight

- non  technical user can use it. No developer logic needed
- analytics and vsualization tool
- any employee any browerser, device person can perform analysis and get insight of data
- serverless
- inside aws or outside aws data can be used for visualization

DataSource

- Redshift
- Aurora/RDS
- Athena
- EC2 hosted db
- file based - excel, csv, tsv, log format
- data/column transformation can be done before applying quicksight
- aws IoT analytics can be done on QS
- little ETL for data preparation

Under the Hood Engine - SPICE

- Super Fast, Parallel, In memory, Calculation Engine
- data will be imported in to spice
- colmunar store in data engine , in memory
- superfast queries on large dataset
- 10GB of spice per user
- scalable for hundread of thousands of user

Use cases
- visualization/ad hoc exloration 
- Dashboard and KPI generation
- exploration from s3 log, any db on premize, redshift, athena,s3, rds, saas app like salesforce, any jdbc/odbc data

ML features(insights)
- anamoly detection(aws'random cut forest algo)
- forecasting feature also use same random cut algo by aws
- for time series data handle anomaly detection remove them and impiute missing values too
- auto narratives : telling narrative about data in plain language so we QS can built rich dashboard so AI will create dashboard for you

QS Q
- ML powered feature
- NLP business question will be asked to QS Q and it will generate report based on that
- what are the top selling items in florida
- lot of pre-work involved here
- region specific
- personal training is required to specify how to use it.
- dataset topic should be defined & dataset and fields should be NLP friendly, how to handle date must be defined

QS paginated Report 
- intro in nov 2022
- many pages 
- many dashboard associated - fancy 

QS Security
- multi factor security enabled
- VPC connectivity
- Row level security(dataset row level security rules will be defined) + Column level Security(Enterprise edition) - field specific
- add QS ip address range into db security group
- private VPC access is also possible in QS - by Elastic Network Interface(ENI)
- AWS direct connect is also there to use on-premise data

QS User Management
- user who can access your QS dashboard page- as charge per user request 
- user via IAM or email sign up
- you can delete user whenever needed(admin)
- single sign on capability - SAML
- active directory integration - in enterprise edition
- MFA

QS Pricing
- annual base
	standard : 9$/u/m,enterprise : 18$/u/m,enterprise with Q : 28$/u/m
- monthly base
	standard : 12$/u/m,enterprise : 24$/u/m,enterprise with Q : 34$/u/m
- spice charge - beyond 10 gb : $0.25 standard, $0.38 enterprise  - per gb / m
- additional charges per paginated report, alert, anomaly detection, q capacity, readers, reader session capability
- In Enterprise edition you will get Microsoft Active Directory Integration

QS Visual Types

- AutoGraph - based on properties of data - not filtered... you can use or apply your changes on top of it
- for comparison : Bar chart
- for Distribution: box plot and histogram
- for checking changes over time : line graph, line chart, area line chart, stracked area line chart
- for correlation :  Scatter Plot and Heat-map(gene expression values defined by heat map)
- for aggregation : tree map(hierarichical data with nested rectangles), pie chart
- for tabular data : pivot tables (summarizing data) aggregate tabular data in arbitary way, measuring the value of intersection of two dimensions, apply statics on it, inshort multi dimensional data where we can apply staticstics on it
- KPIs - A key performance indicator (KPI) is a quantifiable measure of performance over time for a specific strategic objective 
	- comparing key values to target values 
- GeoSpatial Charts(maps) : 
	data over map in form of dot default, size of circle specify value for that location
- Donut chart : comparing values items for dimensions => items takes portion from whole dimension(percentage val) - precision value is not importaznt - percentage of total amount will be shown by this chart
- Gauge Chart : like dashboard of your car - 270 degree circle - how much thing will u measure like fuel in tank
- Word cloud : word cloud specify - diff words/phrases appears in you dataset and size shows density of appeareance


$$$$$$$ EMR- Elastic Map Reduce

- Managed Hadoop Framework - under the hood - hadoop is there - hadoop framework will run under EC2 instance
- many other services are there except Hadoop cluster on EMR
- like spark, Hbase, Hive, Presto, Flink & more also comes with MapReduce in EMR
- EMR notebooks are there run EMR cluster(all EMR resources will be available for notebook) also
  integrated with other AWS services too
- EMR cluster will use all resources parallely to do your compute job for large data on cluster computers
- A cluster on EMR is collection of EC2 instances
- each EC2 instance is called as node. 
  nodes are three types(kind of role) : Master Node, Core Node, Task Node
  Master Node :  Manage cluster by running software component for coordinate the distribution of data and tasks
 		 Track the status of task and monitor the health of cluster
		 if you have enough processing to run on single machine then you can have one master node as cluster
		 leader node called too
  Core Node : software component which run task and store data on hadoop distributed file system
	      one core node is needed if multinode cluster is there
  Task Node : that s/w component which only run task not store data on hdfs, 
	      spot instances, they dont effect on storage of your cluster so you can add or remove whenever you need
	      used only for computation, if you have massive computation task(needed only once to run) you can do it by task node 

# Transient vs Long running cluster 

Transient Cluster : automatically terminate once all defined task run on it has been completed. 
	     	    you can use it if you have predefined tasks and once they will complete cluster will be terminated
		    predefined tasks - loading data, preprocessing data, storing output and remove cluster
Long-Running Cluster : when you have ad hoc query to run or experimenting with dataset when u dont know how much time it will take or steps not pre-known u can use this and once your task done you can terminate the cluster. Here you can constantly interact with application running on cluster.

- once cluster is created its required framework and application can be selected for it. 
- you can choose to manage The master node to do it by EC2 instance
- Or you can do it via commandline - the steps to do confuguration should be specified in CMD

Integration

- amazon ec2 instance made up of nodes for cluster
- amazon VPC is virtual network in which ec2 instance will be launched
- s3 to store input/output data
- cloudwatch to monitor performance of cluster and config the alarm
- IAM will give permission to access cluster
- cloudtrail to audit the request made to your service
- aws datapipeline to schedule and start the cluster

EMR Storage

- as hadoop under hood, default storage is HDFS - hadoop distributive File System
- distribute data across every instance in cluster - multiple copy will be saved on instances so, in failure of instance data will not be lost
- every file will be stored as block and distributed accorss hadoop cluster
- default size 128MB if block
- this storage will be destroyed once you terminate your cluster - still block are local so faster access - if you dont want to keep data just want to use to run it then you can use this storage - data will be on same node where app is running

-EMRFS : provide s3 access for your app. which will be fast. and data will be on s3 - not going to lost. s3 consitency it is called. DynamoDB can be used to track that consitency. 
- local filesystem can be used to store data localy on node - put it on master node and make available for any node 
- Elastic Block Storage for HHadoop Distributive File System 

Pricing 
- charges per hour of EMR. EC2 cahrges are there too
- New node provision if core node fails
- can add/remove task node if want to add/remove addional capacity
- resize the core node on running cluster


$$$$ Hadoop

- consists of some modules like MapReduce, YARN, HDFS
- FS and OS level stuff, java archives file and scripts required for start hadoop 
- all things stored on HDFS
- HDFS have all the features above, but will lost data if instance will be closed
- still it can do caching of intermidiate result during mapReduce processing, can handle i/o workload too
- Yarn - Yet another resource negotiator : available from hadoop 2.0
- to manage cluster resources for multiple data processing framework
- MapReduce : software framework to  write app which process vast data in parallel on large cluster having required hardware to do job
- MR  - consists mapper function and reduce functions write in code
- map will maps the data to set key-value pair for intermidiate result also do transform, reformate the data or extract the data  for your exp data analysis
- reduce will combine those results and apply algorithm and produce final output from it.
- Apache spark - nowadays provide faster alternative for MapReduce spark - open source distributed processing system - for big data workload
- utilize caching, optimize quering from data of any size by using directed acyclic graph technology for speed
- it use HDFS and YARN and works well than MapReduce
- it has apis from python, scala, java & R
- it supports code reuse across multiple workloads like batch processing, quering, ML, graph processing, realtime analytics
- mostly not used for online transaction processing and batch processing
- used for data transforming 
- spark works like independent processes works on cluster
- main program (driver program)- spark context object - spark job run under it
- spark context connect to different cluster managers who allocate resources across the applications,
- managers are like spark, yarn.. you can have your own one (if you are not on hadoop cluster)
- on this connection spark will have executors on nodes in cluster
- executors  are processes that run the computations and store data for your applications
- aplication code will be sent to the executors and in final step spark context will send task the executors to run 
- spark made up from diff components like - spark streaming, spark sql, Mllib, graphX
- spark is responsible for memory management, fault recovery, scheduling, distributing, monitoring jobs, interacting with storage system
- it has apis from python, scala, java & R
- it run resilient distributed data set called RDD - logical collection of data partitioned across diff compute nodes
- spark sql - query engine - fast quering - 100 times faster than mapreduce, have cost based optimizer, column storage and code generation for fast query
- it supports JDBC, ODBC, JSON, HDFS, Hive ORC, Parquet files, supports hive table with hive q
- it provides dataframe for python and datset for scala like dataframe in pandas or db table in relational db
- SQL commands can be used in spark cluster it will transform into query and execute result across entire cluster
- Spark Streaming  : fast scheduling, streaming analytics with the help of same code for batch processing, it use mini batches to do analytics
- Can get realtime data from twiter, kafka, aws kinesis, flume, hdfs, zeroMQ
- MLlib - ML library for spark
- GraphX - distributed graph processing framework in spark - computer science graphs(graphs of users in social network) 
- provides ETL and exploratory analysis of graph data structure 
- MLLib  : provides many machine learning algorithm - which implemented like distributed and scalable- kind of parallel processing on all cluster
- but all algo will not support that particular processing so spark provides algo's alternative to run on all distributed computers from cluster
- classification task - logistic regression- naive bais algo - can do regression across cluster
- recommandation engine built on Alternating Lease Square(ALS)
- K-Means clustering implementation built in
- For topic modeling it has LDA - to extract topics from documents by unsupervised manner
- ML workflow utilities like - pipelines, feature transformation, persistence
- SVD(Singular Value Decomposition) means convert matrix in linear approximation which specify meaning structure of matrix, PCA and statics (distributed implementation of all three)
- by MLLib many algo will run on cluster without it they wont
- massive dataset can run & train ML model across the entire cluster
- Spark Structured Streaming
	- means real-time data : you can access streaming data from continuously updated data table across the time
- Spark + Kinesis
	Kinesis producer publish data sent to spark and you can query on those real time data
- Spark + Zeppline(notebook)
	- can run spark code
	- direct execute sql query against SparkSQL
	- make visualization by chart and graph of data
	- more like make spark as data science tool for data scientist


$$$$$ EMR Notebook, Security and Instance Type

- like zeppline notebook in spark
- access to s3, cluster
- hosted on VPC, and can be accessed from aws only

Use case :
- create apache spark application and run query on it
- supports python, pyspark, spark sql, sparkR, scala
- in anaconda it is available so exploratory data analysis can be done on it with spark dataframe
- notebook is not in cluster so you can spin up cluster from notebook
- no charge for EMR customer, multiple users can use this notebook and explore spark on it

EMR Security

- IAM policies. can apply tagging based on cluster to access cluster by this policies
- Kerberos - password incryption so p-data will not sent unencrypted -secret key cryptography technique used here
- SSH(secure socket shell) - to connect to cluster via cmd, IAM roles - kerberos or ec2 key pair will be used to do client authentication
- IAM roles like EMR service roles, auth roles, service linked roles how emr will access other aws services
- every cluster in EMR must have service role and a role for amazon ec2 instance profile  - policies are giving permission on those roles
- auto scaling role if your cluster has automatic scaling
- service linked role is used for EMR if it lost ability to clean up EC2 resources
- EMRFS service role is required to handle file system request on S3
- If EMR integration with lake formation for automatic cluseter creation you can specify secuirty configuration as litte json snippet
- Apache ranger integration from EMR - open source tool for data security on hadoop or hive

Instance Types

- Master node : if < 50 nodes then m4.large OR if > 50 node then m4.xlarge = total no of nodes on cluster
- core & task node : general purpose solution  => m4.large = if something is cpu intensive but cant be accelerate on GPU
- if task depends on external dependecy like web crawler = then small instance like t2.medium will work 
- more performance m4.xlarge
- high computation job - cpu optimized isntance type
- database or memory caching app then high memory instances
- heavy network / cpu intensive app line NLP or ML = cluster computer instances
- Some AI stuff by using NN network then GPU instances(compute) like - g3,g4,p2,p3
- Spot instance - for task node. As its data will lost. 	
		- dont use on master or core node (unless you are testing or very cost sensitive app) 

$$$$$ Feature Engineering

- apply you knowledge of domain of data and extract better feature to train your data
- Applied Machine Learning = Feature Engineering
- select features wisely
- to do feature extraction we can use PCA - required much computational power - its a dimensionality reduction unsupervised technique 
- features created by it will just extract essence of feature - artifical features - not actual features
- other UnSupervised technique is K-means clustering
- send data in both algo and get smaller no of features by this technique
- large features means curse of dimensionality

*** Imputing Missing Data
- Mean, Median and Mode value.
- mean is average, median is robust to outlier, Mode is most occurance value for categorical feature
- droppig row or using mean value to fulfill imputation is not best choice
- if you are dropping some rows just take care those rows should not be biased - like highest salary/lowest salary, so poor/millinior people
- some techniques :
	KNN : find k nearest neighbor
	      get the similar rows as row having missing data init. Get the average value from those rows and impute it	- eucledian distance will measure
	      better approach  for numerical data. Not for categorical data as you can not find distance based nearest (one way though humming distance)

	Deep Learning : For categorical data we can create one DLM to impute missing value - lot of computations and code and complexity yet best
	
	Regression : Multiple regression on other features in your dataset. By regression you can find linear/non-linear relationship between missing 
		     feature and other features
		     Advance technique MICE(Multiple Imputation By Chained Equation) - advanced so can be best technique to impute missing data

- Better way is just get more data


**** Handling Unbalanced Data

- mainly a problem with nueral network
- unbalancing of positive and negative data like fraud detection - least case are fraud
- Oversampling
	- duplicates the samples from minority class
	- at random
- Undersampling
	- remove the samples from majority class - not good though at moral level
	- throwing away data is not right thing todo unless you have big data and your hardware cannot handle it. You can do that- avoid big data scaling
- SMOTE - Synthetic Minority Over-Sampling Technique
	- it use k-nearest neighbor algo to artificaially generate samples from minority class - based on KNN-result(mean of neighbors)
	- for unbalanced data SMOTE is good choice


***** Adjusting Thresholds	

- set some threshold value based on that you can say probability of positive or negative case
- if too many false positive increase threshold 
- it will decrease false positive but can be increase false negative too
- choose threshold based on cost of FP vs FN


***** Handling Outliers

first know some definations - 

$ Variance

- Variance means - how spread-out the data is
- variance = average(squared(difference(mean))) = sigma ^2
- first find mean value
- then find diff from each value - mean value
- then find square of them
- last find average of those - that is your variance

$ Standard Deviation

- SD means Square-Root of vairance = sigma
- if variance is 5.04 SD = 2.24
- SD is used to identify outlier 
- one SD of mean => if mean is 4.4 then one SD is (4.4 + 2.24)  & (4.4 - 2.24)
- you can specify that no of SD from mean and get to know outlier - like 1,2,3 SD away from Mean

$ Dealing with outlier

- think and make sense when trying to remove outlier from data - how it will effect whole model
- ouliers are so much from same person rating or least from person rating
- SD is one way to find out outliers
- AWS has random cut forest algo to detect anomalies(outliers) in data or you can use 1.5 qunatile value to find outliers

$ Binning
- Take numerical data and transform into categorical
- put each numerical data into certain range of category
- why to do it : if some uncertainity in your values or measurement then applying binning
- quantile binning : no of data point in each Q-bin will be same. so each Q-bin has the same size - it categorize the data by their place in data distribution

$ Transforming
- apply some function to features to make it better suited for training
- exp(x)=e^x => exp(1)=2.718281828459  =>>>> where e is the base of the natural logarithm, 2.718281828459 <= called logerithmic transformation
- numerical feature X can be represented by  SquareRoot(X) and X^2 too <= by this we can learn super and sub-linear function
- so all three form can be passed to algo to get better result - X example apply in youtube recommandation system - for how long has it been since watch video

$ Encoding
- transforming data into new representation which is required by model
- very common in Deep learning
- OneHot Encoding 
- in bucket so many categories. For values your category value will be 1 and other category will have 0
- in DL all categories are represented by individual output neurons

$ Scaling and Normalizing 
- lot of model require their feature data normally distributed around 0(NN Model)
- so those features can be comparable as they are in same  -1,0,1 range
- otherwise feature with larger magnitude will have more wightage - not appropriate 
- scikit has MinMax scaler , standard Scaler

$ Shuffling
- shuffle the training data
- otherwise model will remember the order and performance will not as much good as we need


****** Ground Truth

- sometimes you dont have training data at all. 
- you need human to generate it Exm : training the image classification, tag the objects in images
- ground truth will manage who will label your data for training 
- along with that GT will create its own model, and that model learn from those labels
- as times goes on the model does not need to send all the images to human for labeling.. it can do by it self... it will just send those images which have ambiguous data in it will be send to human only... model becomes better by it self
- like this it will reduce labeling job by 70 %
- GT plus is AWS service where by some high level charges people from AWS will manage whole GT labeling task for you
- any pre-trained model or unsupervised learning technique can be used to create this labels

who are these labelers
- amazon mechanical turk
- your team
- professional labeling companies

$ other ways to generate labels

- services by AWS

1. Amazon Reckognition
- Aws service for image reckognition
- Automatically classify the images


2. Comprehend
- Aws service for text classification and topic modeling
- classify text topic or sentiment associate with it



*********** TF-IDF 

- TF : how often the word occurs in given document
- DF : how often the word occurs in all documents set which are given. examp: wikipedia/webpage

Multiplication of 

TF-IDF = TF * (1/DF) = it gives you how important and unique this word for this document

- word frequency is exponential means it can increase explonentially as get more word in document. Its not fixed.
- so we will use log for IDF - to give better weight to the word
- TF-IDF has some issues
	- it assumes documents is just a bag of words so it wont pay attension to the relationship between words 
	- other problems like synonyms, abbreviation, tences, misspelling many more
- Extension of TF-IDF is extended to bi-gram and tri-gram approach, n-gram too
practicle based on TF-IDF

- find TF-IDF for each term in corpus
- for given search word sort TF-IDF score for that search term 
- display result

TF ======= (no of occurance of word in doc / total no of words in doc)

IDF ======  log (total no of doc / no of doc having that word)


Shape generated by TF - IDF === (no of doc, no of unique word)


### Apahe Spark

- distributed processing framework
- used for ML, graph analytics, stream processing by using Amazon EMR cluster
