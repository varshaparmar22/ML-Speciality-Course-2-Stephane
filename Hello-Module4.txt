Date: 21-05-2024

SageMaker : intend to handle whole machine learning workflow

To get better resultant model
- we have to fetch data, clean and prepare it
- train the model, evaluate the model
- deploy model, evaluate result in production

All these stuff manage by SM

In SM

1. s3 will have training data
2. training code will in in docker container as docker image
3. start model training by using 1 & 2 step
4. that trained model will be stored in s3 as model artifact
5. inference code is also stored in docker image container
6. by using that model artifact and inference code from image container
7. model will be deployed in production environment
8. any new request comes from outside world to generate prediction will invoke


How to use SM

- simple by using SM notebook(hosted on EC2 instance where you specify)
- use s3, scikit_learn, tensorflow, spark
- builtin models can be used in notebook too 
- can do hyperparameter tunning automatically - find best HP
- do training, deployment and make predict at scale

From console
- we can do training as well as HP tunning

Data Format works well with SM
- RecordIO/Protobuf : enable user to store large amount of data to store in binary format so it is more compact and faster to read
- can be used athena, redshift, EMR, amazon keyspace(=apache Cassandra) DB
- can be use scikit, numpy, pandas from notebook

Processing
- you can create processing docker image and use it 
- use s3 data and send to processing docker container or SM builtin container 
- output will be sent to s3

Training
- Url of s3 bucket having training data which are processed for training
- Computational resources for ML will be used here
- Url of s3 bucket to store the training output.
- elastic container registry which has training code will be used to do training
- other training option like built-in training algorithms are there to use
- use spark mllib, custom code by tf, pt, mxnet, sklearn, RLEstimator for reinforcement learning, xgboost, huggingface, chainer
- your own docker image, or algo purchased from aws
- Use Amazon FSx for Lustre to accelerate File mode training jobs.

Deployment
- save the model you just trained to s3
- four ways to deploy :
1. Persistent Endpoint - whenever inference request will come the model will do prediction. Server will be stayed open
2. SM Batch Transform : whole dataset batch inference will be generated and then  server will be closed.
3. Serverless Inference :  when endpoint can handle cold start problem, & idle time between traffic spike
4. Asynchronous inference : large payload up-1GB, long processing time, near real-time latency

options are available along with deployment in SM
- inference pipeline - for more complex processing of inference result and tie them together
- SM Neo to deploy model on some endpoint application(edge devices)
- Automatic scaling - for persistent endpoint it will increase endpoint to give inference on demand
- shadow testing - evaluate new model against currenly deployed model to check for errors
- elastic inference - for accelerating inference for DL model only

$$$ Incremental Training

- CreateTrainingJob request
- InputDataConfig : ----   ChannelName = "model", ContentType = "application/x-sagemaker-model"
- new model + expanded dataset when underlying pattern  was not accounted in previous training and which resulted in poor model performance.
- Use the model artifacts or a portion of the model artifacts for training job. You don't need to train a new model from scratch.
- Resume a training job that was stopped.
- Train several variants of a model, either with different hyperparameter settings or using different datasets.

$$$ Transfer Learning

 trained using transfer learning when a large number of training images are not available

$$$$$$$$$$$$$$$$$$$$$$$$ SM algos $$$$$$$$$$$$$$$$$$$$$$$

1. Linear Learner - weight related all things will apply

- training data should be fit as per line
- prediction based on that line
- regression(numeric) prediction and multi & binary classification(categorical) prediction
- for classification it use linear threshold function

* inputs
- RecordIO wrapped protobuf (float32) & csv
- file/pipe mode 

$ preprocessing

- training data must be normalized so will be in same weight
- you can do by your own or LL will do it automatically
- shuffle the data

$ Training

- choose optimization algo like adam, adagrad - form of SGD
- multiple model will be optimized in parallel and choose more optimal from that
- L1, L2 regularization will be applied(to prevent overfitting) where L1 do feature selection, L2 will weight all features more smooth
- SGD : Stochastic Gradient Descent 

$ Validation
- most optimal model will be selected from this multiple training

### HP 

@ Balance multiclass weight

-gives each class equal importance in loss function

@ Learning Rate, Mini Batch size will be optimal to choose

@ L1 
@ L2(Weight Decay)

@ target precision - precision will be hold on that value and recall will be maximized
@ target recall - recall will be hold on that value and precision will be maximized
- these tp, tr is for binary class classification

### instance Type
- single or multiple machine which can have one cpu or gpu only
- multiple gpu on single machine will not help

LL => handwriting recognition problem (classification) work line NN


2. XGBoost  ---- SSSSS

eXtreme Gradient Boost

- group of decision  tree
- new tree correct the error of previous tree
- it is fast
- wins lots of Kaggle competition
- use gradient decent method to decrease loss once new tree will be added
- can be used for classification and regression problems too - it called regression tree
- Its robust handling of a variety of data types, relationships, distributions.
- The variety of hyperparameters that you can fine-tune.
- You can use XGBoost for regression, classification (binary and multiclass), and ranking problems.
- you can use XGBoost as a built-in algorithm or framework. 

### data formats for training and inference:
-text/libsvm (default) : after label column, features the zero-based index value pairs. format:<label> <index0>:<value0> <index1>:<value1>
-text/csv
-application/x-parquet
-application/x-recordio-protobuf

### how to use
- Booster.save_model => to save model in internal binary format <= new version   |  >= Versions 1.3-1
- in python we can serialize/deserialize xgboost model by pickle <= old version
- in SM notebook it is available by sagemaker.xgboost
- it can be used as in-built SM algo -so you can use xgboost docker image elastic registry container and deploy it to your training host for larger training job
- For increased performance - use File mode

### Hyper Parameters

*Alpha - L1 Regularazation - larger value - more secure model
*Lambda - L2 Regularazation - larger value - more secure model
*num_round-no of training round & num_class - num of class=> when multiclass classification is there = multi:softmax or multi:softprob. - required params
*base_score- initial prediction score, global bias
*booster- gbtree, dart (for tree model), gblinear(linear function)
*colsample_bylevel/_bytree/_bynode - subsample ratio of column by level,tree,node
*Eta - step size shrinkage for preventing overfitting- increase it for overfitting - shrink the weight of features
*eval_metric - rmse: for regression, error: for classification, map: for ranking
*Gamma - minimum loss reduction for creating new partition for tree-increase for overfitting
*max_depth - 0 means no limit,default is 6, more deep value lead to overfitting of model
*tree methods - auto, exact, approx, hist, or gpu_hist.
*Subsample - prevent overfitting- increase  for that - more samples on each tree, so tree will generalize on overall more data
*scale_pos_weight - when unbalanced pos/neg cases you can set balance between that, can be set to sum(neg)/sum(pos)


Increasing 'gamma' makes the model more conservative by controlling the formation of new nodes, which can help reduce overfitting by penalizing complex models.




#### Instance Type
- its memory bound not compute bound
- so no gpu, you can use m5 for memory optimization
- but in xgboost 1.2 => single gpu(p2,p3 type) training available, you have to set tree_method to gpu_hist, trains more quick and cost effective
- in xgboost 1.2-2 => P2,P3, G4dn,G5
- in xgboost 1.5+ => distributed training is also available(cluster of many G5), for that set some params like
	=> tree_method=gpu_hist
	=> use_dask_gpu_training = true
	=> distribution = fully_replicated(to distribute training data on cluster) <= with Dask, otherwise shardedbyS3Key
        => only supports for csv and parquet input	
	=> with dask supports only pipe mode


3. Seq2Seq 

- input/output = seq/seq
- for machine transalation, text summarization, speech to text, Video captioning
- implemented with attension by using rnn and cnn

### input data
- only support recordIO-protobuf 
- tokens must be in integer
- sample code provided in SM to convert word tokens in integer in recordio-protobuf format.
- need train, validation and vocab channel to work otherwise error

- for inference application/x-recordio-protobuf or application/json
- for batch transform => application/jsonlines
- input and output will be in same format

### how to use
- takes lots of time even in SM(large training host available in SM)
- pre-train models are available as well as public dataset for translation task

### HyperParameters

* batch_size
* optimizer_type - adam, sgd, rmsprop
* learning_rate
* num_layers_encoder
* num_layers_decoder
* optimize_on : accuracy(compare agaist validation daataset), blue_score(compare against multiple reference translation), perplexity(cross-entropy) - last two are main for machine translation problem 	

### instance type
- deep learning algo so only GPU
- only single machine but can have multiple gpu on single machine

4.DeepAR 
- S
- for forecasting one dimensional time series data
- RNN
- can train deepAR model with several related time series
- stock price prediction, serverload, demand of product, request of web pages 
- it can learn seasonality and frequency from series data well
- out-perform ARIMA and ETS

### input types
- jsonlines input like Gzip or parquet file format. diff extension need to specify in SDK in python except these
- each record has "start" : timestamp, "target" : time series value, "dynamic_feat" : dynamic features like promotion applied on product while purchase, "cat" : categorical features
- vector of static - time independent - cat
- vector of dynamic - time dependent - dynamic_feat
- use all files from folder for training specified - except those that start with a period (.) and those named _SUCCESS.
- target & dynamic_feat has same length
- it forecast only for categories that have been observed during training. 
- cat id discreet value
- If the algorithm is trained without cat and dynamic_feat fields, it learns a "global" model
 
jsonlines

{"start": "2009-11-01 00:00:00", "target": [4.3, "NaN", 5.1, ...], "cat": [0, 1], "dynamic_feat": [[1.1, 1.2, 0.5, ...]]}
{"start": "2012-01-30 00:00:00", "target": [1.0, -5.0, ...], "cat": [2, 3], "dynamic_feat": [[1.1, 2.05, ...]]}
{"start": "1999-01-30 00:00:00", "target": [2.0, 1.0], "cat": [1, 4], "dynamic_feat": [[1.3, 0.4]]}

### how to use
- always include entire dataset for training, testing and validation
- use entire dataset as test data set
- remove last datapoints(prediction_length) and set it for training
- evaluation will be done in test on withheld datapoints
- use < 400 as prediction length
- train on many time series which are related not on one. to get full power of deepAR

### HP

- epochs, mini_batch_size, learning_rate, num_cells(no of neurons to use)
- context_length : no of time points model will see before making prediction
	=> will be smaller than seasonality period
	=> model will lag every year data point value anyhow - and pick up seasonality - so  send all data for training
	=> The model uses data points further back than the value set in context_length for the lagged values feature.

### Instance type
- CPU, GPU, sigle machine or multi machine
- aws recommend use cpu instance like ml.c4.2xlarge, ml.c4.4xlarge
- use gpu only for large deepar model or when minibatchsize is > 512
- for inference supports only cpu
- for tunning model more larger instance will be needed
- right now DeepR required observation no at least 300

5. BalzingText 

- for text classification - predict label for given sentence - supervised learning technique(labels should be pass along) - useful for web search, information retrieval, ranking, and document classification. work for only sentence not for entire document
- generate word2vec - create vector representation of word- nearby vector for similar kind of word - work on word only- downstream NLP tasks, such as sentiment analysis, named entity recognition, machine translation, etc - find similar kind of words
- word2vec will find relationship between word in a sentence.  word embeddings capture the semantic relationships between words.
- The BlazingText algorithm is not parallelizable.


### training inputs

for supervised mode in text classification 
- one sentence per line, that sentence will have __label__[label_value] followed by sentence which space separated tokens in lower case
- also support augmented manifest text format - means text format created by SM which specify details about sentence. like json format
{"source": "Linux ready for prime time", "laranking, and document classification.bel":1}

for word2vec mode
- it takes text file which has one sentence per line


### How to use

* Word2Vec's modes
 - CBow - word order not matter
 - Skip-Gram
 - batch Skip gram : distributed computation over many cpu - gram technique order metters
 - only train channel supported 

### HyperParameters

*word2vec
 - mode as above
 - learning rate
 - window_size
 - vector_dim
 - negative_samples

*text classification
 - mode : supervised
 - learning_rate
 - epochs
 - vector_dim
 - word_ngrams
	

### instance type

word2vec
- for cbow and skipgram => ml.p3.2xlarge
- any signle cpu or gpu instance will work
- for batch_skipgram => signle or multiple cpu instance will work
- in batch_skipgram if multiple cpu then s3 distribution type = fullyreplicated, algo will take care of distribution process

textclassification
- if training data < 2gb then c5 <= cpu
- for large dataset => ml.p2.xlarge, ml.p3.2xlarge <= can be used gpu

model artifact for inference

* for word2vec
- vectors.txt, vectors.bin
- send word and get vector file in json

* for text classification
- model.bin
- send sentences and get label with prediction score

6. Object2Vec

- it is an embedding layer 
- create low dimension embedding of high dimensional object
- take features of object and get low dimenation vector representation of that object
- it can work on image, document, or any kind of object
- so more general purpose
- preserve semantics relationship between pair of the objects
- this vector representation can be compare and shown how related they are
- so it can compute nearest neighbour of object for clustering purpose
- visulaize cluster
- genre prediction
- recommendation - product, user

###  input type for training
- tokenize data into integers
- send the pair of tokens or sequence of tokens
	=> sentence-sentence
	=> label-sequence for (genre-description)
	=> product-product
	=> customer-customer
	=> user-item
find relationship between them and train model

### how is it used

- here it is pair based 
- so two encoders will be used 
- each encoder will take one input and send encoder result to one comparator 
- encoder types will be Hierarchical CNN, bi-directional LSTM, average pooled embedding
- comparator is send the result to feed forward neural network and generate label & score

### HP

- normal deep learning HPs
batch_size, learning_rate, dropout, early_stopping, epochs, activation_function, optimizer, layers, weight_decay
type of encoder network like hcnn, bilstm, pooled_embedding 

### instance type

for training
- single machine can be used only
- cpu or gpu or multi-gpu on single machine
- start with cpu type instances
  cpu - ml.m5.sxlarge, gpu-ml.p2.xlarge, for large if needed ml.m5.4xlarge, ml.m5.12xlarge
- gpu type p2,p3, g4dn, g5

- for categorical label generation => loss = "cross_entropy"
- for regression score/rate generation => loss = MSE

for inference - specifically recommended ml.p3.2xlarge
- for GPU set INFERENCE_PREFERRED_MODE to
   GPU optimization:encoder embedding 
   GPU optimization:classification or regression

7. Object Detection - S

two variants used in SM

input = image
output = all object specify their category and confidence score


1. Maxnet

can use 
- CNN with SSD(single shot multibox detector) algo
- CNN with VGG-16 or ResNet-50
- transfer learning/ incremental training mode will be used here with pre-train models initial weights
- to avoid overfitting - use rescale, flip, jitter

2. TensorFlow

can use 
- ResNet
- EfficientNet
- MobileNet
models

### training input 

for MxNet
- RecordIO
- ImageFormate(jpg or png) with imageformat supply json file like - {"file":url,"image_size":{width:100,height:100,depth:5},"annotations":{data about boxes of object},"categories":{categories to identify}}
- you can also train in pipe mode using the image files (image/png, image/jpeg, and application/x-image), without creating RecordIO files, by using the augmented manifest format.
- application/x-recordio in pipe mode - with augmented manifest format
{"source-ref": "s3://your_bucket/image2.jpg", "bounding-box":{"image_size":[{ "width": 400, "height": 300, "depth":3}], "annotations":[{"class_id": 1, "left": 100, "top": 120, "width": 43, "height": 78}]}, "bounding-box-metadata":{"class-map":{"1": "cat"}, "type": "groundtruth/object-detection"}}
- order of these attributes is matter in pipe mode
- train and validation channels are required
- train_annotation and validation_annotation <= as it is object detection so object annotation are required
- .rec file format
- RecordIO or image formats for input data

for TensorFlow - specific to it

### HP

-batch_size,
- learning_rate
- optimizer

### instance type

for training
- GPU use mainly
- multi GPU or multi machine GPU
- for smaller ml.p2.xlarge/ml.p3.2xlarge
- for bigger ml.p2.16xlarge/ml.p3.16xlarge, G4dn, G5

for inference
- GPU or CPU can be used
- for cpu => m5
- for gpu p2,p3,G4dn,g5


8. Image Classification - S

- image classification tell you stuff is in the image or not - like its a cat/not

### how to use

MXNet

- full training mode
	initialize with random weight and start training
- transfer learning mode
	initialize with pre-train model's weight
	can add new layer on top of it which is fully connected there initialize with random weight
	train with new data
- default image size is 224X224 with 3 channel RGB
- validation and train .lst file will be needed for training as only three tab separated column in it
- 1. image index, 2. category label index, 3. image file path

for augmented manifest file format for pipe mode - no need to convert in recordio format
{"source-ref":"s3://image/filename1.jpg", "class":"0"}
{"source-ref":"s3://image/filename2.jpg", "class":"1", "class-metadata": {"class-name": "cat", "type" : "groundtruth/image-classification"}}
for multi-class 
{"image-ref": "s3://mybucket/sample01/image1.jpg", "class": "[1, 0, 1]"}
{"image-ref": "s3://mybucket/sample02/image2.jpg", "class": "[0, 0, 1]"}
- multi-hot label-format is used here "class": "[1, 0, 1]" <= category 0 and 2 both will be considered here
- "application/x-recordio; label-format=multi-hot"

TensorFlow

- mobileNet, Inception, Resnet, EfficientNet
- top classification layer available so you can add more layers and get new inference from model too

### Hp
- learning_rate
- batch_size
- optimizer

for optimizer
- weight_decay
- Beta1, Beta2, Gamma, eps
- based on TF and MxNet

### instance Type

for training
- GPU use mainly
- multi GPU or multi machine GPU
- p2,p3, G4dn, G5

for inference
- GPU or CPU can be used
- for cpu => m5
- for gpu p2,p3,G4dn,g5
- The input image is resized automatically for inference


9. Semantic Segmentation - S

- pixel level object classification
- in object detection boxes around object, in image classification labels
- it is useful for self-driving car, medical imagine diagnostics, robot sensing
- produce segmentation mask for image based on pixel

### inputs 

- jpg or png with annotation associated with them
- labels are there in annotation
- augmented manifest image format will be support in pipemode - data can be used directly from s3 here to generate inference in pipemode
- for inference accept Jpg image
- It tags every pixel in an image with a class label from a predefined set of classes.
- with Augmented Manifest,  RecordWrapperType= "RecordIO" and ContentType = application/x-recordio.

What is an augmented manifest file?

An augmented manifest file is a labeled dataset that is produced by Amazon SageMaker Ground Truth.


### how is it used

- MXNet Gluon framework and the Gluon CV toolkit
- 3 algo to choose
	1. Fully Convolution Network
	2. Pyramid Scene Parsing
	3. DeepLabv3
- choice of backbone(encoder)
	1. ResNet50
	2. ResNet101
	- both train on ImageNet database
- with this algo you can do incremental training or training from scratch

** encoder will create activation map of features - can use model artifact as it is pre-trained
** decoder is there too will create segmentation mask from that activation map of features - never pretrained

### HP

- learning_rate
- optimizer
- epochs
- batch_size
- algorithm  -as above
- backbone   - as above


### inatance type

training
- only gpu support
- p2,p3,G4dn, G5
- on a single machine only

inference
- either cpu or gpu 
- cpu => m5, c5 ,  gpu => p3 or G4dn
- image and an AcceptType specify for inference
- AcceptType = image/jpeg or image/png or application/x-recordio-protobuf
- return will be jpeg seg-men-mask, png-seg-men-mask, recordio file with class probability encoded
- images will be taken from s3

For training
s3://bucket_name
    |
    |- train
                 |
                 | - 0000.jpg
                 | - coffee.jpg
    |- validation
                 |
                 | - 00a0.jpg
                 | - bananna.jpg
    |- train_annotation
                 |
                 | - 0000.png
                 | - coffee.png
    |- validation_annotation
                 |
                 | - 00a0.png
                 | - bananna.png
    |- label_map
                 | - train_label_map.json
                 | - validation_label_map.json 

10. Random Cut Forest - U - only cpu

- anomaly detection
- unsupervised
- unexpected spike in timeseries
- breaks in periodicity
- unclassified datapoints
- assign anomaly score to each datapoints

### training input

- recordIO-protobut or CSV
- can use file or pipemode
- test channel available on which you can compute accuracy, f1, recall, precision on labeled data(anomaly or not)
- it is unsupervised model so no actual testing but test channel is there
- The train channel only supports S3DataDistributionType=ShardedByS3Key and the test channel only supports S3DataDistributionType=FullyReplicated

### how it is used

- it create forest of trees
- if you add datapoints in DT and that data point stays closer to root and would not form other node/dt from it than its anomaly
- data sampled randomly on DT
- available from stream of data in kinesis data analytics
- it takes series of data and check if it is anomaly or not

### HP

- num_trees: increase to reduce noise
- num_samples_per_tree : ratio of anomalous to normal data = 1/num_samples_per_tree - to do better job for algo

### instance type

- cpu type is used only
- so m4, c4, c5 for training
- for inference ml.c5.xl



11. Neural Topic Model - U

- get what document about
- organize/classify/summarize the document  based on topics
- unsupervised technique so not assigning topic but just organize in one group 
- algo called neural variational inference
- can parallelized training across multiple GPU instance

### input type

for training

- it is unsupervised
- training, (testing, validation and auxiliary - optional)
- RecordIo-protobuf or CSV
- word must be tokenize and create vocab.txt file <- only with word but in same integer sequence that model will follow to show result
- auxiliary channel(content type text/plain) <=> vocab.txt
- file or pip mode can be used - pipe is faster
- latent representations of documents are expected to capture the semantics of the underlying documents
- based on those latent representation grouping will be done for documents
- return application/json or application/x-recodio-protobuf & topic_weight vector for each observation

### how to use

- how many topics you want define it
- two topic modeling algo in SM
1. Neural topic model
2. Latent Dirichlet allocation


#### Hp

- num_topics
- batch_size
- learning_rate


### Instance Type

- GPU or CPU
- for training GPU is recommended
- for inference CPU 
- CPU is cheaper



12. LDA - Latent Dirichlet Allocation - U

- not deep learning
- not using neural network
- unsupervised - so no labeled data
- it used more commonly for topic modeling but it can be used for other things rather than words like grouping of customer based on their purchase, harmonic analysis of music
- it is more general purpose for grouping thing based on some common features they shows
- it use perplexity (inability to understand) and topic coherence to put document in particular topic
- perplexity is metric for language modeling - it gives per-word-accuracy - it is inverse - so minimize it
- so coherence is followed here to identify topic for given document
- LDA has better perplexity, NTM has high topic coherence - so choose based on hardware - GPU, CPU
- topic models aim to minimize perplexity and maximize topic coherence.
- return application/json or application/x-recodio-protobuf & topic_mixture vector for each observation

### inputs

- train channel and optional test channel as unsupervised
- RecordIO-Protobuf or CSV
- word count(important) in document + integer representationv- in csv format
- pipe mode supports only in redordIO format


### how is it used

- unsupervised so generate as many topic as you want
- test channel is option and generate per-word-log accuracy measurement
- function same as neural topic model but cpu based so more cheaper and efficient

### HP

- num_topics
- alpha0 = concentration parameter, if small generate sparse topic mixture, large (>1)generate uniform topic mixture
- not a deep NN

### Instance Type

- single cpu for training/inference


13. KNN - K nearest neighbour - S

- classification or regression  
- it use distance metric and find nearest neighbour 
- for classification it use most common label
- for regression it take average of value
- it use all features as it to calculate nearest neighbour
- it Is lazy algorithm

### inputs

for training

- train channel contains your data
- test channel used to find accuracy or MSE
- recordIO-protobuf or CSV
- in csv first column is label
- file or pipe mode 

### how is it used - three steps for training

- first do sampling - over/under
- do dimensionality reduction - methods like - random projection and the fast Johnson-Lindenstrauss transform.
- indexing, serializing and querying model to get k no of neighbour


### HP

- k
- sample_size

### Instance type

training 
- on cpu or gpu
- m5, p2

inference
- cpu or gpu
- cpu gives low latency(delay of transfer)
- gpu give high throughput(test million of samples very rapidly) on large batches



14. K-Means Clustering - U

- unsupervised clustering
- divide data into k number of groups
- you define attributes based on which algo will do grouping
- based on Euclidean distance find data points are similar and assign then in cluster
- it calculate center point of each cluster too
- here in sm it provide large scale(web-scale) clustering on large dataset
- elbow method - (within-sum-of-squares) can be used for finding intial k value
- multiple models with diff k value will be trained and choose optimal value of k in sagemaker 

### input

- train channel, test channel is optional as unsupervised
- train shardedbys3key, test FullyReplicated
- formate recordio-protobuf or csv
- pipe or file mode


#### how is it used

- each datapoint will be placed in n-dimensional space = n is no of features
- get a center of all k cluster algo will work
- extra cluster center is new thing introduced by SM to improve the accuracy
- first SM start with that no of cluster K and reduce it down to k
- K = k*x <= x is center of cluters... so it is square root thing
- by algo find out initial centers of clusters
- for that random or K-means++ approach is applied
- K-means++ approach will make initial center of clusters far apart
- go through the training data and calculate clusters's centers
- by using Lloyd's method it will reduce down K to k no of cluster with k-means++


### HP
- K - elbow method is used to find K value initially
- mini_batch_size - important to remember
- extra_center_factor
- Init_method = random or k-means++

k-means returns a closest_cluster label and the distance_to_cluster for each observation.

### Instance type

- CPU or GPU
- but CPU recommended

if want to use GPU then
- one GPU on one machine
- so use large GPU instance
- p2,p3,G4dn, G4 are supported


15. PCA - U

- Principal component analysis
- dimensionality reduction technique
- project higher dimensional data into lower dimension like 2D plot with minimum loss of information
- reduced dimensions are called components
- first component has largest possible variability - capture variability of data (importance)
- second component has second largest possible variability - capture variability of data (importance)
- it is unsupervised technique - so no need to train just use it get less no of features

### inputs

- recordIO-protobuf or CSV
- pipe or file mode

### how is it used

- it create covariance matrix first
- and use Singular Value Decomposition to distil it down

two type to use

1. Regular
- for sparse data and moderate no of features and observation
2. Randomize
- for large no of observation and features
- use approximation algo

### HP

- algorithm mode : Regular/Random
- Subtract_mean : use unbiased data


### Instance Type

- GPU or CPU
- based on input data


16. Factorization Machine - S

- specialized in classification or regression in sparse data
- FM use for recommendation system more as it has sparse data
- why sparse 
	- as it is pair wise item-user kind(so it should have atleast 2 no of dimension)
	- user will not interact with most of page, click or product so sparse data
- It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets
- pair-wise (2nd order) interactions between features.
- In regression mode, - RMSE. - LIKE movie rating- rate value
- In binary classification mode - Binary Cross Entropy (Log Loss), Accuracy and F1 Score.
- it is not multi-class model
### inputs

- RecordIO-Protobuf with float32
- CSV is not practical here... where only space filled up as sparse data

### How is it used

- find the factors based on which we can map users to items, product, page, click, rating(regression) etc(create matrix which do that mapping)
- click prediction and item recommendation.

### Instance Type

- CPU or GPU
- CPU recommended
- GPU works well when only dense data(which is not sparse)


17. IP insights - U

- It will check usage pattern of IP address and find fishy behaviour
- it will find fishy behaviour from some anamolous IP address
- like login or creating resources from account from some anamolous IP address
- it is kind a security tool
- flag something suspicious and shut down session

### input
- username
- account id
- no need to pre-process data
- training channels are there
- but as unsupervised validation channel is option
- validation is used to compute AUC score
- CSV only accept with entity(username/accountid) and ip address
- supports only file mode

### how is it used

- it is neural network
- NN is used to form vector representation of entities and IP address
- entities are hashed and embedded so need sufficient large hash size
- negative samples generated during training by randomly mapping entities and IPs
- embedding representation of ip address will created n can be used for other algo's downstream task

### HP

- No of entity vector - set twice to unique entity - hash size
- vector_dim - size of embedding vector, large will form overfitting
- epochs, batch_size, learning_rate

### Instance Type

- CPU or GPU
- NN so GPU is recommended 
- p3 or higher
- multi-gpu on one machine can be used too
- cpu size depends on no_of_entity_vector, vector_dim


18. Reinforcement Learning

- learning about virtual environment & how to navigate through that environment in optimal manner
- agent explore some space
- based on agent action he will get reward or penalty and it will learn based on those action
- once exploration is done entirely agent will yield fast online performance
Example to use reinforcement learning
- supply-chain management
- HVAC system
- industrial robotics
- dialog system
- autonomous vehicle

Type of RL

Q-learning

- environment state = s
- action = a
- for state-action pair some value = Q

- Reward for good move in state Q+
- Penalty for bad move in state Q- 

- so by using sate-action pair Q value we can inform agent for future choices
- it will build a little memory if it get reward or penalty the previous state will be considered in memory based on that state and given action agent get penalty or reward
- here it will do back-propagate to keep track of those reward or penalty and keep in memory for learning
- store Q values associated with each state and action and use to inform in future
### exploration problem

how to explore all state efficiently
1.
- choose state-action based highest value of Q
- but by that we can miss a lot of path
2. 
- epsilon term - random number term
- choose random dice number
- if it is less than epsilon than don't follow highest(best) Q (1- step), choose random path of state-action pair to try, not to choose best state-action pair
- this way exploration will never stop
- choose epsilon wisely


### Markov Decision Process

- mathematical framework for modeling decision making process where outcome of DMP is partly random and partly under the control of decision maker
- MDP is discrete time stochastic control process

### Reinforcement Learning in SM

- It use tensorflow or mxnet which use deep learning technique to do RL
- it require toolkit and environment like custom, open-source or commercial for implementation
- amazon Sumerian, AWS robomaker 

### training

- exploration training can be distributed 
- environment rollout can be distributed 
- over multi-core on one PC or entire fleet of multi-instance(multiple-PC)


### HP

- No specific HP to tune
- You can use SM HP tunning capability to set and tune HP

### Instance Type

- its Deep learning 
- so GPU 
- with multiple instance and core feature capability


19. Automatic Model Tunning

HP Tunning
- kind of experiment on value and get optimal value
- but in SM it is available by automated feature which is self learning algo too

### what to specify for AMT

- algorithm to tune
- HP, range of HP value and metrics for optimization
- SM do HP tunning job
- you can set no of HP combination to tune based on your cost
- SM will run training instance in parallel (but a little no) to make it quick as possible
- by this you can get best set of parameters
- cool thing is the AMT will learn by this process over time 
- means it know which combination give positive effect on result and which not
- so it will not try all of them
- by that it save lots of resources required for HP tunning

### Best Practise for ATM - Remember it

- don't optimize too many HP at once, try to choose those HP which u think will have most impact on accuracy or metric you optimize for
- ranges of HP should be limited too.. no crazy value pass
- use logarithmic scale for HP values you want to explore like if HP's value like 0.0001 to 0.1 then log will be more faster
- don't run too many training job concurrently as AMT learn from those job and if they are so many in parallel it cant learn properly. You can use 1-2 training job concurrently run
- make sure if training running on multiple instance at the end they should report correct object metric to optimize for

$$$ types of HyperParameter Tunning Job

1. Grid Search
- work on categorical parameters only
- don't specify no of tunning job its automatic based on no of categorical parameters

2. Random Search
- random combination of params
- not depends on previous job result - so concurrent - fast
- train on as much combination as you want

3. Bayesian Optimization- regression based
- treat hyperparameter tunning like regression problem
- specify metric to tune on
- next set will be chosen based on current tunning model evaluation
- it is used in SM hyperparameter tunning 

4. HyperBand & HyperBand with early stopping
- it dynamically re-allocate resource after each epoch run in training job based on performance
- automatically stop training which is underperform while tunning params set
- work for image classification which publishes accuracy metrics after every epoch.

- with early stopping advanced mechanism with hyperband
- TrainingJobEarlyStoppingType = OFF <= default, when using HyperBand with EarlyStopping

- Bayesian will take more time than random


20. Apache Spark & SageMaker

- Apache spark is good at data processing and it distribute that processing over many clusters so it has fast capabilities
- SM-Spark library is there to connect and capabilities of SM and spark

- In apache spark all pre-processing of data will be done like mapping and reduce
- at the output of spark it will create dataframe object 
- instead of using spark MLlib we use SageMakerEstimator for training
- which provide PCA, XGBoost, KMeans Clustring kinda
- SageMakerModel will be used to create inference
- The SageMaker Spark library is available in Python and Scala

### how is it work

- use SM notebook or zeppline notebook(available on EMR)
- connect it to EMR cluster which run spark - called spark cluster
- dataframe - the output from spark have a features column which is vector of double
- and optional label column of double
- call fit on SMEstimator 
- call transform on SMModel
- which is used for generating inference
- work with spark pipelines too



21.SageMaker Studio

- visual IDE - billable
- interated with jupyter notebook
- so you can share notebook with other users 
- hardware configuration you can change which are managed by aws

### SageMaker Experiments

- organize
- capture
- compare
- search 

all your ML jobs
- which model did best you can compare by this


22. SageMaker Debugger


- it save model's state over time(periodic interval) when it is under training phase
- you can define rules by this 
- debug job will run for each of those rule and if it hits it log/fire cloudwatch event
- it Is integrated in SageMaker Studio as Debugger Dashboard
- it can auto generate detailed training reports

three built in rules for debugger

1. Monitor system Bottleneck
2. model framework(TF,MX etc) operation
3. model parameter debug

### what is supported by Debugger

Framework & Algorithm

FM -
MxNet
TensorFlow
PyTorch

algo - 
- XGBoost
- SM generic estimator(high level interface for SM training - custom containers for training)

### API for debugger

- you can create any set of rules by api like
1.CreateTrainingJob
2.DescribeTrainingJob

for Debugger

- to access training data SMDebug client library is there

### SMDebugger insight Dashboard 
- all graph associated with training
### DebuggerProfilerRule
- Profiler Report, Hardware system metrics(GPU, CPU usage), Framework Metrics(stepoutlier, allframeworkmetrics) 
### built in action for receive notification or stopping the training
- StopTraining(), Email(), SMS()- work by SNS(simple notification service)
the rules you set for debugger
### profiling system resource usage and training


23. SageMaker AutoPilot/AutoML

it automates
- algo selection
- data preprocessing
- model tunning
- all infrastructure selection for you

all trial and error for you it will do

### Workflow

- get data from s3
- select target column
- automatic model creation
- create notebook-code for those - you can do your refinement & control over it
- you can select a model from recommended models from model leaderboard
- deploy and monitor that model & refine by notebook if needed

### summary

- you can add human guidance
- you can write code in given notebook by AWS SDK

Problems it can work for
1. Binary classification
2. Multi-class Classification
3. Regression

Algorithm type
1. XGBoost
2. Linear Learner
3. Deep Learning(MLP)
4. Ensemble Methods

### input
- Tabular CSV or Parquet

### Training Mode

1. HPO - Hyper Parameter Optimization

- select best algo from above
- select optimized HP(for that run upto 100 trial to select best)
- Bayesian optimization if dataset is <100mb
- multi-fidelity if data set is >100mb
- early stopping if needed

2. Ensembling

- use auto-gluon library(many tree based on NN models) and train several models
- can run 10 trails at once with diff models and Parameter settings

3. Auto

- if dataset size >100mb => HPO
- if  dataset size <100mb => Ensembling
- if autopilot can not figure out size of dataset for some reason then HPO is default
Reasons:
- if s3 bucket dataset hidden in VPC 
- s3 datatype is manifest file
- s3URI contains more than 1000 items

### Autopilot Clarify

- it gives you explanation about how each feature take part into prediction
- so specify what kind of bias exist on that feature for given model, you can get to know that
- you CAN do further analysis for that
- it use Shap Baseline or Shapley Values which is in terms create list of features and importance of that feature in prediction made by model
- so based on that you can get to know which feature you have to check and if it is bias, you can correct it.


24. SM Model Monitor

- it use cloudwatch and give you alert on quality deviation on deployed model
- you can visualize data drift(moving toward something slowly) - based on that your model works well but not as expected
- detect outliers, anomalies, new features
- can managed by console or dashboard or SDK

### monitor + clarify
- as clarify detect potential bias from data
- it helps model monitor to monitor bias & new potential bias and their effect on prediction 
- and by cloudwatch set alert on that
- clarify also helps to explain model behaviour by specifying which features plays  most for given model prediction

- data in s3 and secured
- monitor schedular will schedule monitor job
- monitor will give metrics which are connected to cloud watch
- which gives you notification and you can set alarm for it by set of rules and take action to re-train or audit data
- it integrated with TensorBoard, Tableau, quicksight or just go to SM studio for visualization of all metrics

###Monitoring Type

1. Drift in Data Quality
- means as new data comes in can change mean, min, max, deviation value 
- you can set some baseline for them while setting up monitoring

2. Drift in Model Quality
- you can set some baseline required model accuracy, recall, precision, RMSE
- can integrate with Ground Truth Label(check what human are saying about model prediction  based on data and what your model is predicting)
3. Bias Drift
as specified in clarify
4. Feature Attribution Drift
- more specific
- use NDCG(Normalized Discounted Cumulative Gain) score 
- compare feature's attribution towards prediction for train data vs live data


25. Deployment SafeGuard

### Deployment GuardRails
- it used for asynchroneous(whenever model gives response at its pace)/real-time inference endpoints
- gives control over shifting traffic to new model
	three type of control - say blue fleet is old, green is new fleet
	1. all at once: shift everything , monitor and terminate blue
	2. Canary : shift small portion and monitor and check okay than terminate blue
	3. Linear : shift traffic slowly linearly spaced and monitor model performance meanwhile
- auto rollback : things goes wrong - rollback to blue fleet which is working
- a/b testing => production variant
- canary => testing new feature on small scale
- rollout => systematic updates to all users

### Shadow test

- shadow variant of new model will be created and traffic sent to it
- compare its performance with current production one and monitor it by SM console and decide when to put it in production 


26. SM Canvas

- no code setup for ML for business analyst
- data preprocessing, model building, making prediction etc
- upload data in csv format
- select column to predict
- build model and make prediction
- work on only classification and regression problem
- can do data pre-processing, joining from multiple columns, handles missing val, outliers, duplicates
- SM studio enables you to share dataset and model with others

### fine points

- you can not upload local file, file must be inside s3 bucket as canvas works under virtual domain, a isolated environment
- can integrate with OKTA SSO - single sign on
- user has to sign in once and they can use this service without entering signin details again
- canvas situated in domain, that domain should be updated manually. It is not automatic
- data can be imported from Redshift too
- time series forecasting can be predicted but need to give IAM permission and enable it for user who is predicting it
- can run under VPC
- 1.90$ perhour + no training cell charges

27. Bias Metrics in Clarify

Metrics are below - specify some bias in data and which needs to address

1. Class Imbalance(CI)
- data points which are from some class more than others, like age is class more data points where people are in their 20-30,less data in 50-60

2. Difference in proportions in Labels(DPL)
- imbalance of positive outcomes from different demographic group
- like 20-30 age people pay their bill on time more than 50-60

3. Kullback Leibler Divergence(KL) and Jensen-Shannon Divergence(JS)
- based on demographic user group how much difference in outcome distribution
- one group got more approved loan than other

4. LP-Norm(p-norm)(LP)
- p-norm difference between distribution of outcome from different demographic user diverge

5. Total Variation Distance(L1-norm)(TVD)
- l1-norm difference between distribution of outcome from different demographic user diverge

6. Kolmogorov-Smirov(KS)
- Maximum Diverge between outcome distribution between different demographic user
- same idea flip around in revrese from above

7. Conditional Demographic Disparity(CDD)
- Some demographic user group has more rejection than acceptance(disparity of outcome as whole or based on subgroup)


28. SM Training Compiler

- its only integrated with AWS Deep Learning Container(DLC) no other containers
- compile & optimize training job only for GPU instances
- can accelerate training upto 50%
- convert model into hardware optimized instructions
- tested with huggingface transformer library - so works pretty well with it
- not compatible with SM distributed training library
- best practise
1. ensure GPU instance are used, like p3,p4
2. PyTorch models must use PyTorch/XLA's save function for model
3. enale debug flag in compiler_config parameter to enable debugging


29. Feature Store

- feature means processed columns which are used to train model and make prediction
- in ML fast and secure access is required to feature to train and make prediction
- they should be organized and sharable across the different models 

### Where feature data comes from like from anywhere

- SM data wrangler, glue databrew, EMR, glue, SM processing, sm pipeline, stepfunction, lambda, sm training, sm hosting, sm batch transform
- SM Feature Store sit in middle of it and get and store features from those

### how it organize your data

- feature groups are there to organize them
- record identifier, feature name, event time

### Data Ingestion

@ Stream Data
- streaming data source like kinesis, apache kafka, spark, data wrangler 
- PutRecord API & GetRecord API to write and read data in SM FS
- its a online store - streaming store
- It is online repository for features
@ Batch Data
- its a offline store
- which create glue data catalog in s3 
- we can access those data anytime 
- it is batch/offline repository of features
- it can be accessible by anything which can access s3 

### security

- everything is encrypted in rest and transit
- can work with KMS master key
- IAM based security
- You can use AWS PrivateLink to make it secure


30. SM ML lineage Tracking

- it automatically create and store your ML workflows(MLOps)
- it keep running history of models
- tracking for auditing and compliance
- automatically track or manually create tracking entities for tracking
- integrate with AWS Resource Access Manager for cross account lineage
- in form of artifact all result of ML process is stored and it can be tracked by SM ML LT

###Lineage Tracking Entities

Trial Component
- processing job, transforming job, training job
Trial
- many trail components forms Trail
Experiment
- many trails form Experiment
Context
- logical grouping of entities
Action
- workflow step,model deployment
Artifact
- object or data like s3 bucket or image in ECR
Association(between entities)
- has association type like
contributedTo, AssociatedWith, DerivedFrom, Produced, SameAs


### Quering Lineage Entities
- you can querying the lineage entities by LineageQuery API from Python - available in SM SDK
- querying like do things like find all models or endpoint which use given artifact
- product visualization by using external visual helper class from GitHub or any

31. SM Data Wrangler

- it prepare tabular and image data for ML
- visual interface in SM studio to prepare data for ML 
- import data
- visualize data(check data) - you can check outliers by using this too 
- transform data(more than 300 transformation are there to choose from) or
  you can transform data by using pandas, PySpark, PySpark SQL
- Quick Model feature to train a model with some less no of data and get result
  by this you can transform data in different way and get better result
- it can take data from anything like
  s3, lakeformation, SM feature store, athena, redshift or any source which can be connected by JDBC like databrick or SaaS
- it will give output to A SM Processing, A SM Pipeline, A SM Feature Store
- mainly it will create python code for all ETL job for data - jupyter notebook output it will give
- so it will produce code that will go into pipeline
- SM processing job or autopilot job can be called automatically for implementing ETL job


### TroubleShoot
- studio user should has proper IAM role to access DW
- Data Source permission is set for DW access -  you can use amazonsagemakerfullaccess policy
- EC2 instance limit
  if you are getting error "the following instance type is not available..."
  means you have to increase your quota - like choose bigger instance  like m4,m5 xlarge or something
- go to Service Quota => A SM => Studio KernelGateway Apps running on ml.m5.4xlarge


#### t-SNE 

- unsupervised
- use for data exploration
- dimensionality reduction
- visualize data in high dimension

- used for dimension reduction but update the dimension(features)

### LDA - linear discriminant analysis

- dimensionality reduction technique
- update dimension 
- so relevant features from dataset is gone 

### Training Mode

1. file mode : all the data will be stored in file at once and then pass to training from s3
2. pipe mode : data as per model setting will be taken from s3 and then put in training as needed - pipe is more efficient than file
- if training taking too much time we can use pipe mode not file mode

@@@ Log Loss/ Cross Entropy
LogLoss – Log loss, also known as cross-entropy loss, is a metric used to evaluate the quality of the probability outputs, rather than the outputs themselves.
-it measures the difference between the discovered probability distribution of a classification model and the predicted values.



"Action": [
      "sagemaker:action1",
      "sagemaker:action2"
]
"NotAction": [
      "sagemaker:action1",
      "sagemaker:action2"
]

SageMaker does not support resource-based policies.

SageMaker-studio-classic only support service linked policies - so its partial support.

SageMaker supports service roles.

Authorization Based on SageMaker Tags.

SageMaker does not support specifying resource ARNs in a policy.

Using Temporary Credentials with SageMaker
- temporary security credential with aws sts api operation - AssumeRole, GetFederationToken

- keras, gluon are API for tensorflow & maxnet respectively ==== to create ML models
- pytorch is dynamic graph = creat model
- tensorflow is static graph  = create model
- scikit-learn === ML library - algorithm Library
