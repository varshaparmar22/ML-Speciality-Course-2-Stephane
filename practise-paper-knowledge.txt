=== https://tutorialsdojo.com/aws-cheat-sheets-aws-machine-learning-and-ai/?src=udemy

- to load data in redshift from s3 by  using firehose, firehose should be allowed with s3:PutObjectAcl, which is specified in allow section of s3, IAM role

- lambda will allocate cpu power based on memory configuration

- firehose buffersize = 1-128MB, buffer Interval = 60-900 sec

- Glue support csv, json, avro, parquet, orc for write(output)&read both,  &  groklog, ion, xml for read only

- KDA can do data transformation by SQL or apache Flink and use lambda function to transfer data to anything

- we can do periodic update in weight of model based on recent data - maybe useful in fraud transaction

- ROC AUC score  =  equally care = positive & negative class ===> FPR, TPR

- reg:linear, reg:logistic are XGBoost HP

- use random forest DT instead of only DT to reduce overfitting and generalize well

- drop-out will be used for neural network - say neuron


- if large data source so many buckets can be there, so resource based policy will be bad choice, IAM role based policy will be better approach to secure data
  just tie object with role and object/bucket are secure.

- access key & secret access key are used for application running outside of aws environment. Not with aws services

- The CloudHSM module is used to generate encrypted access keys.

- For categorical features we can use Deep learning technique for imputing missing values

- if K increase variance decrease in - K-fold cross validation

- K-fold validation round will have very similar error rate to other round then data will not have bias

- Tiered Depoyment : most powerful instance for complex task line, simple instance for simple task line - cost effective

- model compression :  give even performance on all simple & complex task so not that effective for different kind of task if you r tailored

- scikit cross_validate method is useful for checking model's precision while hP tunning

- decoupling is important for not overwhelm the lambda or polly with many client request. - or any aws services

- to predict output based on multiple independent variables(like car specification) multi-variate regression will be useful

- SimpleImputer will use default mean value for imputation of missing data

- OneHotEncoder has drop methodology like None, First, Array. 

- Binary classification model produce binary label(0,1) and strength of that prediction as output

- SoftMax activation function convert vector of numbers into vector of probabilities - probability distribution on each class - total sum of probability=1

- LL has predictor_type HP -required , values : binary_classifier, multiclass_classifier, regressor

- LL : PT = b_c then loss => auto, logistic, hinge_loss, m_c => auto, softmax_loss, r => auto, huber_loss, squared_loss, quantile_loss, absolute_loss etc

- hinge_loss can be used for LL - binary_classifier or SVM(support vector machine) algos

- eval_metrics is evaluation metric for k-means clustering algo, values = msd, ssd, [msd, ssd]

- multi-class classification means choose from many category associated with given datapoint

- cartesian product = categorical feature and capture their interaction like hardcover_romance, softcover_romance

- for content_type parameter text/csv default value of label_size = 1, no need to specify for supervised algo.

- Random Search allow to run multiple concurrent hP tunning job so it is very fast not effect the search operation performance

- future price prediction is time series data prediction so validation will be for time-series cross validation

- cross_entropy_loss is used for binary classifier.

- inference container must accept post request to the invocation endpoint

- model quantization reduce (the inference time and model size) on minimal accuracy loss

- if model learns very slow = low learning rate than outlier wont effect to model's accuracy

- same inference endpoint we can deploy two variant of model for a/b testing - setting will be in configuration

- AWS Glue Findmatches has params : precision-recall='precision'/'recall' and cost_accuracy='accuracy'/'lower cost' => as per requirement 
  lower cost will speed up the transformation process by giving low lost/loss, but if you need accuracy more than set to it

- the record from lambda function transform by using firehose , the transformed record result should contains 1. result (status) 2. recordId 3. data

- Firehost alone can not do AKMS encyption of data it need KDS or S3 to do it

- to manage models experiments like search tag, hp, algo of model <= Amazon SM model tracking capability

- completeness_score is useful for scoring of clustering problem, check properly whether it is binary classification of clustering problem or clustering

- if you have legacy main frame  (means on-premise) data, store it on cloud first by using aws storage gateway with file gateway configuration + NFS (network file system)connection. It is helpful in real-time data integration/transportation

- objective in XGBoost algorithm's values are "multi:softmax", "multi:softprob"

- if GT used to label small subset of data then train the model and again use GT for hard to identify data and train model with exiting and GT result data

- aws lake formation will  crawl through your data source and move data in data lake, using ML algo to clean and classify those data

- EMR, Glue, KDF can also create data lake but more custom code require by them instead aws lake formation is just built for that purpose by using glue capabilities

- FindMatches has params : precision-recall, accuracy-cost, set this parameters to precision/recall and (accuracy/lower cost => if you want accuracy more or speed up training) 

- if no need to realtime inference then update in fields of inference will be better achievable with batch transform.

- AW glue dnamicFrame will allow each record can be have differ schema who has self-describing information along with them

- When using Glue findmatches ML transform capability of glue data file should be utf-8 encoded with BOM(byte order mark) and in csv format

- AWS datasync will be used to ingest data from NFS-network file system 

- you can call lambda function when creating object in s3 bucket

- Inference request serialization (handled by you say lambda code)

- Inference request deserialization (handled by the algorithm)

- Inference response serialization (handled by the algorithm)

- Inference response deserialization (handled by you)

- IoT core is useful for receiving IoT device messages so it can interact with aws services

- Kinesis data stream can be useful for streaming messages from IoT core And stream them to SM inference endpoint by lambda

- sagemaker.analytics havin trainingJobAnalytics to do visualization

- metric:msd, metric:ssd are metric for k-means clustering

- you can use online testing with live data for creating driverless technology for car

- in object detection we need to give annotation data about bounding box for which we want label - for tumor detection it wont useful

- In comprehend use createDocumentClassifier will take categories the you provide to classify
- to get intent of comment you need to identify subject of comment too like person, organization

- if you want to get grouping kind of some data then use k-means - k will be detected by algo
- if you want to find similar data point, not putting them in group then use k-NN

- N-gram technique is useful for finding phising email

- if you don't have existing model use transfer learning technique, if you have model and just new similar kind of data comes then use incremental training

- annotation consolidation is a feature of ground truth to combine annotation from multiple labeler to get probability of correct label

- when we create training job in SM we need to specify regex(not metrics name direct) for metrics that we want for training script will write into log to get visualization

- AWS IoT analytics you can use instead of aws kinesis analytics for data enrichment

- IoT core will send sensor data to IoT core or AWS Kinesis Analytics for processing/aggregation/encryption/translating/transforming etc

- estimators are the method which useful for estimating unknown parameters - it is one component for HP tunning 

- Multilayer perceptron  algo is useful for speech recognition and translation problems

- if your model has outlier problem and overfitting then you can use L1 - lasso regularization to reduce effect of outlier and also do generalization

- training metric visualization will be on aws management console, Python SDK api, cloudwatch dashboard

- HP tunning job : CreateHyperParameterTunningJob => pass json object as value for HyperParameterTunningJobConfig

- in SM two HP tunning algo : Random, Bayesian: in most models Bayesian will take less no of tunning job to reach to optimal HP settings

- if you want algo for managing some data center cooling system then Reinforcement learning technique can be useful - use data like chiller, pump, coolunit

- seq-seq is useful for machine translation and question-answering system

- GT's annotation consolidation feature has function like bounding box, sematic segmentation, image classification, text classification, named entity recognition = to ensure accuracy

- if value of alpha (regularization) parameter increase it makes model more conservative in XGBoost

- conservative = having less complex model by reducing overfitting

- Semi-Supervised learning is used when you have a dataset with both labelled and unlabeled data from which you train your model.

- Spark MLJob will have MLeap format container, SparkML Serving Container & MLib library which can help to process data which can go into pipeline

- if your model has several outliers and model is solving regression problem then use MAE - Mean Absolute Error metric as it can handle outlier more than MSE better

- Built in Transform feature is in AWS Glue

- SM CLI UpdateEndpoint command is useful for switching from existing endpoint to new endpoint

- response element of UpdateEndpoint command is => EndpointArn

- CreateModel is command to create new container for your inference (if only one then Primary otherwise only Container(for pipeline))

- When you create custom algo in SM following tobe specified
  1. docker container for training and inference
  2. specify HP
  3. metrics - to optimize and send log data to cloudwatch
  4. instance type for training and inference

- No such thing like distributed inference

- finding a specific species from image is binary classification problem

- Entropy = measure of randomness in feature

- want to check co-relation between feature use pairplot and co-relation metrics

- swarm plot = categorical scatter plot

- catplot = shows relationship between categorical var(1 or more) & numerical var

==========================================================================================================================

- one https sm endpoint having multiple variants, traffic distributed on them by using https endpoint configuration setting

- if Mlops+Pipeline
  = Mlop will create new model version
  = you have to approve that version
  = then Mlop will deploy it in statging environment
  = then use CodePipeline to approve that Deploystaging stage
  = then Mlop will deploy it in endpoint

- minimum viable products = validating customer's need and demand kind of product so probability of each product if they are mvp

- autopilot will give accuracy, f1-score for binary/multiclass classification, not auc, pr auc kind

- for metrics if NDGC-normalized discounted cumulative gain as it is gain maximize it, (rmse, mse, mae - minimize), accuracy, f1 kind maximize, map - mean average precision - maximize

-QS supports athena, aurora, s3, opensearch, redshift, spectrum, IOT analytics, databrick, exasol, MariaDB, ms sql, MySQL, oracle, PostgreSQL, presto, snowflake, starburst, trino, Teradata, timestream. It not support dynamodb directly

- auto scaling feature has option like target-scaling(as per requirement) , step-scaling(for fix no of instance scale in circumtance)

- Data Wrangler - (target leakage visualization - data correlated to target label strongly) (Quick model visualization - give importance score for each feature)

- for k-means(its distance based) - msd(mean squared distance)minimize, ssd(sum of squared distance)minimize

- for notebook instance - 
	once start(open) 
		- it and all related files will be saved in ML storage volume
	once stopped
		- snapshot will be taken for ML storage volume, but os configuration settings, custom liberary installed will be lost
	once terminates
		- snapshot & ML storage volume will be deleted
	save between start & stop
		- use lifecycle configuration policy for os configuration and update changes

- SM notebook + lifecycle configuration to maintain data and settings between start and stop

- Apache SparkML serving containers are used to deploy Apache SparkML inference pipeline serialized with MLeap Format

- Kinesis Analytics with lambda will be used to ingest streaming data into feature-store & Apache Kafaka directly useful

- Kinesis Firehose can also be useful for ingesting data into offline/online SM feature store

- To connect SM custom model inference result to quicksight, QS has a augmented sagemaker feature and json file with metadata to specify to QS how to use model data - its a fastest way to augment SM ML inference to QS - can be used in customer churn visualization

- to reduce collinearity between features in regression kind of problem use PCA, LDA will be used for multiclass classification problems to reduce feature or collinearity

- SM has only SM Python SDK no any other

- you can use PySparkProcessor or SKLearnProcessor or SParkJarProcessor packages to do feature engineering 

- if mqtt messages are there they must be received by aws IoT core and then can be transfer to kinesis firehose or apache kafka, both has IoT core action to transform those data for further transfer or analysis

- Kinesis data firehose supports compression of data by using aws lambda

- estimator extends to transformer object, so batch transformer is useful for making batch prediction

- Kinesis Data Analytics & quicksight both run SM's random cut forest algorithm for anomly detection, but if you want to do visualization instantly then use only quicksight

- Content based filtering = relies between similarities of features of items
- Collaborative Filtering = relies on preference from other users on similar items(collaboration of interest in same items)

- SageMaker notebook instances run on EC2 instances running within an AWS SM service account. - ACTUAL 
- it provide internet gateway for default access 
- if you want to attach it to VPC then internet-gateway won't come in picture.	

- CloudWatch keep the metrics-statics for 15 months
- from CW console search is limited to last 2 weeks
- CW metrics(for sm jobs and endpoints) 1 sec is resolution and lifespan is 3 hours

- for loading large training data use pipe mode - so data will not be loaded in training instance it just use from source directly

- deep AMI is useful for launching EC2 instance(main task) with required framework already on it.

- if resources are not from same VPC then use deny action for fine security requirement

- Apache flink has drop and combine features built in to use 

- Target Encoding = replacing categorical value with mean value of target var for that category, it can lead to overfitting for low frequency categories.

- Amazon Fraud Detector = rule based model for fraud detection  == supervised learning + Unsupervised learning to detect fraud = with help of rules
  so can use training data with label as well as training data without label - not necessarily required labeled data

- for image based algorithms data distributiontype will be fullyreplicated

- for training in pipe mode use contentType = application/x-recordio, training is possible in file and pipe both mode, in file mode it can be image/jpeg, image/png, application/x-image
same will be apply for inference.

- when HP tunning obj created pass estimator with it along with HP required pm settings
  objective metric you wish to solve to, and resource configuration details, such as the number of training jobs to run in total and how many training jobs can be run in parallel.

 - if near-relatime or realtime streaming then small batch size need to follow not large batch dataset - and use PutRecord API not PutRecords API

- KDA is not ingest data directly, it need KDS or KDF

- KDF has built-in feature for data transformation in parquet/orc format - so leat effort

- IoT Core if service from aws which send IoT sensor data, for IoT data IoT analytics will be better than KDA

- IoT Greengrass is used for deploying model on edge device for getting inference

- face landmark are dot mar return by Aws reck to crop/morph/apply filters on face

- Co-variance matrix shows relationship between features numerically, pairplot shows visualization of pair of features

- Training is HP Tunning Process

- SM GT has all image classification, object detection, sematic segmentation, named entity recognition(for unstructured text to classify them in predef cat) kind of related features to do labeling

- CloudHSM is used to generated encrypted access key

- ECS Fargate is also runs on aws cloud environment, when you want more control over infrastructure for you app like ec2 instance, storage volumne,  networking then use only, otherwise aws provided service are there which do pretty good job for processing, streaming, transforming data without much intervention in infra.

- if IoT devices want to communicate with each other then IoT Greengrass will be helpful for it.

- Amazon Sumerian - we can create VR application - augmented reality(AR) apps - IDE with all library, VR characters, apis, editor

- VPC peering is only for client account peering on premise. Not for aws managed service account in which you SM VPC runs

- line chart ==> when need to compare measured values over the given period of time - comparison changes over period of time

- for Image classification = train can be application/x-recordio, but inference will be on application/x-image  <=== content type

- estimator extends to transformer, batch transformer will do batch transform job
- predictor object will make prediction request to SM endpoint
- multiDatahModel object will deploy multiple model at same endpoint

- for Inference request from API 
  = API Gateway will receive API call & invoke lambda
  = lambda will invoke inference endpoint
  = SM hosting will process that inference request from endpoint

- Net Promoter Score KPI shows how likely a current customer would recommend your company’s products.
- Customer Profitability Score KPI shows how much profit a customer contributes to your company’s profits 
- Conversion Rate KPI shows how many leads were converted to customers

-When performing offline testing of your models, you deploy your trained models to alpha endpoints

- Clustering Problem Score Types ===> completeness_score, rand_score, adjusted_mutual_info_score

- interface-enpoint , gateway-endpoint is useful for creating connection between your VPC and sagemaker services(like api/runtime)

- transit gateway is useful for creating communication between your VPC and on-premise network, not sagemaker service(like api/runtime)


- s3 bucket file size limit => 50MB

- Kendra extract text from document should not exceed 5MB
- Kendra support PPT, word, text, pdf, html file formats

- annotation consolidation = automated process = probabilistic estimation for correct label by multiple GT workers - to correct mislabeled object

- Glue FindMatches precision-recall parameter to ‘recall’ minimizes false negatives 

- Glue FindMatches precision-recall parameter to ‘precision’ minimizes false positives

- Glue FindMatches= But, setting the accuracy-cost parameter to ‘lower cost’ favors cost or the speed of running the transform at the expense of the transform’s accuracy. 

XGBoost
- if want to get features based on total_gain across all split in which given feature is used, use gbtree_booster
  = call get_score
  = importance_type = total_gain
- importance_type will be in only => base learner and tree booster
- total_gain => gain across all split
- gain => average gain 

######################################################################################################################

- To visualize relationship between two features Scatter-plot will be best choice 
  = shows linear-nonlinear relationship
  = outliers
  = strength of the relationship

-LR is High ==== The model will overshoot the minimum, oscillating around it and potentially diverging

- With more epochs, the model has more opportunities to fit the training data closely, even learning noise and irrelevant details, hurting its generalization.

- More epochs are generally helpful for underfitting if the model hasn't converged.

- More epochs increase the computational cost as the model requires more updates.

- Gaussian distribution == Normal Distribution======================================== co-co is for continuous variables
- Covariance/Pearson correlation co-efficient <= (for) Gaussian distribution 
- Spearman's correlation co-efficient <= (for) Non-Gaussian distribution 
- Polychoric correlation co-efficient <= (for) getting relationship between variables gathered via survey like rating of movie

- co-relation co-efficient values
  => 0 = no co-relation
  => > 0.5 = positive co-relation
  => < -0.5 = negative co-relation

- Naive Bias Algorithms
  1. Multinomial NBA : check frequency of word from your vocab in given observation
  2. Bernoulli NBA : check yes/no appearance of word from your vocab in given observation
  3. Gaussian NBA : work for continuous values in observation not discreet value

- Support vector machines (SVMs) are a set of supervised learning methods => classification, regression and outliers detection.
- SVM have applied 5 fold cross validation

- if neural machine translation is asked then only cnn can be used for that problem otherwise always use RNN/LSTM/GRU

- Bayesian HP tunning will required less n of tuning job to search best HP

- You cannot stream data directly into Kinesis Data Analytics. You would have to stream your sensor data into either Kinesis Data Streams or Kinesis Data Firehose first

- Kinesis Data Stream can not write data directly in s3, it need consumer library application to receive data and write to s3/elasticsearch/any

- Lake formation has findMatches capabilities to match data even when they don't has common identifier as well from different site/store they are from

- Elastisearch don't has data matching capabilities

- audio streaming for transcribe can be done efficiently by http/2 - it has retry logic built in to handle error, while web socket can be used but it will not have that retry logic - so you need to implement it
 == StartTranscriptionJob API => batch transcription
 == StartStreamTranscription API => for streaming audio transcription

- Then use Kinesis Data Firehose and Lambda (also both managed services) to first convert the data from CSV to JSON (Kinesis Data Firehose transformation requires that the data be in the JSON format) then use the Kinesis Data Firehose parquet transformation to convert the data to parquet.

- Kibana is best used as a visualization tool with AWS Elasticsearch

- With multivariate imputation, you use other variables in the data set to predict missing values.

- The SageMaker Ground Truth Named Entity Recognition labeling task is used to extract information from unstructured text and classify it into predefined categories.

- Nomalization : change equal data distribution around mean, so can be problematic for diff scale data like weight, price

- MinMaxScaler : it will scale data on same scale mostly -1,0,1 so when diff scale data appear like price, weight then use this

- Target Leakage : when some data in training dataset - strongly co-related to target label, but it is not appear in real-world data, metrics to check for TL => classification : AUC ROC curve, regression : coefficient of determination or R2 metric

- Sci-kit has two metric when you want to rank feature relationship to target
  = for regression => mutual_info_regression
  = for classification => mutual_info_classif

- In SM specially your data should be in S3 for training not anywhere else to train model - then that data will be loaded to notebook(say studio) environment

- In inference container - You need to remove the Horovod operations. as It used by TF for gpu based distributed training. 

- for getting fast and accurate inference - Use accuracy for your evaluation metric and runtime as your satisficing metric

- SageMaker Studio has images are built on Docker images, not AMIs. to train, deploy mdels - container images

-  You would have to create a custom image to use TensorFlow in SageMaker Studio. no available by default

- Clustering is done using unlabeled data.

- large mini-batch size = high computational demanding matrix multiplication

- The model type available for the Fraud Detector service is the ONLINE_FRAUD_INSIGHTS model.

- In SimpleImputer transformer default strategy is MEAN

- shuffle the data to prevent overfitting and to reduce variance.

- If SMOTE and GAN is there for oversampling then GAN will be more appropriate to create samples 

- Random sampling is exact copy of random sample data

- Glue ETL can call any service like comprehend by using schedular event.(can be useful for text analysis by comprehend)

- Define a JSON object and pass it as the value of the HyperParameterTuningJobConfig to the CreateHyperParameterTuningJob.

- conservative = having a less complex model by reducing a possible overfitting. = simple model = less features


#############################################################################################################################

- High bias can lead to underfitting, which is when the model fails to perform well on both seen and unseen data(train/validation/test)
  = to reduce bias get more data
  = as model is not capturing pattern

- Logarithmic Transformation can be useful for removing effect of outliers from data & do normalization for skewed dataset

- To configure a Docker container to run as an executable, use an ENTRYPOINT instruction in a Dockerfile: ENTRYPOINT ["python", "custom-algorithm.py"]

- RUN command ===> build a Docker image.

- The CMD command just allows you to set a default command that can be overridden from the command-line interface.

- AWS Batch are queued and executed based on the assigned order of preference. AWS Batch dynamically provisions the optimal quantity and type of computing resources based on the requirements of the batch jobs submitted. It also offers an automated retry mechanism where you can continuously run a job even in the event of a failure 

- AWS Fargate does not support GPU-based instances.

- Athena charges you by the amount of data scanned per query. You can save on costs and get better performance if you partition the data, compress data, or convert it to columnar formats such as Apache Parquet.

- Naive approach  === features are strongly independent from each other

- Full Bayesian approach == the features can be co-linear or not to each other === model can handle those datas well(co-relation coefficient can be vary 0.1-0.95)

- amazon personalize = historical data, real-time data, real-time event(user interaction) can be recorded(by using event tracker) to make personalize system up to date

- amazon personalize has algos ===> USER_PERSONALIZATION, PERSONALIZED_RANKING, RELATED_ITEMS - algos are used to train for given use-case

- Exponential transformation - can not handle skewness of data - it just increase value not normalize it

#######################################################################################################################################################

- metric => time-ordered set of data points that are published to CloudWatch

- log => generated data tied to a specific event that happens over time. 

- There are no CloudWatch Amazon Resource Names (ARNs) for you to use in an IAM policy. Use an * (asterisk) instead 

- Amazon EventBridge is a service that builds upon the CloudWatch Events service API. 
extends the capabilities of CloudWatch Events by enabling customers to connect data from their own apps and third-party SaaS apps

- Logarithmic scaling only works for values greater than 0. --- like  0.00000000000001 is > 0 

- ML, matrix multiplication is a compute-intensive operation used to process sparse or scattered data produced by the training model.

- AWS Lambda has an execution limit of 15 minutes- so cant handle long-running tasks, default timeout = 3sec

- Data Pipeline regularly copies the full contents of a DynamoDB table as JSON into an S3.Exported JSON files are converted to comma-separated value (CSV) format to use as a data source for Amazon SageMaker.

- Levenshtein distance :  just tells you the closeness of two strings like read, raed - exchange/replace one word and get another 

-  t-SNE  ----- t-distributed stochastic neighbor embedding (t-SNE)

- Random cropping ======= data augmentation technique 

- wan to get realtime insight with sql query then Kinesis Analytics will be the option

- KCL is just a client-library used for writing consumer applications for Kinesis Data Streams. 

- Kinesis Data Firehose is not a valid streaming source for a streaming ETL job in AWS Glue. KDS is

- amazon-kinesis-analytics-can-now-pre-process-data-prior-to-running-sql-queries == by lambda function

- K-fold like 4-fold 1 model (0-25 % data), 2 model(25-50 % data), 3 model(50-75 %), 4 model(75-100 %) each create AUC score

- when model is overfitting in training - you want to send email to users
  ===> write code to transfer training metrics to cloudwatch by CloudWatch API operations ===== in training script
  ===> lambda function can be used only when you want to do some other stuff like stop the training if overfitting

- CloudWatch event will not capture any event which occur during model training - so cannot create any event rule

- Elastic Network Interface (ENI) === virtual network card (NIC) 

- when model performs well in training, validation, testing but at deployment state the model perform poor == use data augmentation technique for specially images like crop, flip, change color of image by AT and train model ==== get more data

- csv is default data storage in s3

- if want to visualize model's performance with HP values 
  for classification
   - use scatter plot for AUC curve and HP values 

- to convert feature into binary format - one hot encoding is one technique - mainly for categorical feature
- label encoding = convert categorical data into integer format like (0,1,2,3,4...)
- target encoding = take one numeric target value and replace each cat value corresponding probability to that target value

- to make container NVIDIA-docker compatible
  = install cuda toolkit in container
  = Cuda toolkit has runtime library
  = give capability to container to leverage NVIDIA GPU
- EC2 is a server, while containers are applications that run in servers

- Collaborative filtering is based on (user, item, rating) tuples. Users with similar tastes are more likely to have similar interactions with items they haven’t seen before.
  = diversity (how dissimilar recommended items are)
  = serendipity (a measure of how surprising the successful or relevant recommendations are)
  = novelty (how unknown recommended items are to a user)
  = factorization machines
  = cold start problem
  = needs large amount of interaction data

- content-based filtering - Also like if you watching romance since last week, it suggest romance more to you called content based filtering
- currently RNN & transformer based models are also used as they - check sequence of user actions & do evolution for recommendation

- in XGBoost - Adjust the balance of positive and negative weights by configuring the scale_pos_weight hyperparameter. if Recall/Precision is high

- MAP (Mean Average Precision) - Ranking Problem

- CO-EFFICIENT = MULTIPLICATION  ====> value between [-1,0,1], [-1,0]=> negative, [0]=> no correaltion, [0,1]=> positive
  ==== negative means if one increase other decrease, 
  ==== positive means if one increase other increase 

- measuring accuracy of model RMSE is used not Residual plot.
  == residual plot will be for each observation, while RMSE for final accuracy of model
  == Residuals represent the portion of the target that the model is unable to predict- under-estimating, over-estimating

- Glue Job Working
  To start a job when a crawler run completes, create an AWS Lambda function and an Amazon CloudWatch Events rule. To trigger that lambda function

- For training, data will be downloaded in EBS volume attached to training instance first.

- in File mode - data will be stored in EFS or FSx for Lustre ----- specifically aws Sagemaker - so no need to download in instance's EBS volume - both system integrated with s3, so direct access from s3  --- both can be helpful for HP tunning. Multiple iteration - so no need to download data in EBS

- splitting data into batches will be helpful when memory limitation is there for training

- Amazon Managed Service for Apache Flink - Kinesis Data Analytics - use standard SQL for analysing the streaming data

- kNN at a major disadvantage of having a costly inference for larger datasets - as n no of operation and n no of memory it require	

- amazon forcast provide facility to fill missing values in dataset by three type ---- 
1. back filling
2. middle filling
3. future filling

- Amazon Redshift ML is a feature in Amazon Web Services (AWS) that allows users to create, train, and deploy machine learning (ML) models using SQL
- Redshift ML, the input data can be processed directly within the Redshift cluster, reducing the time and resources required to move data between Redshift and Amazon S3. Additionally, Redshift ML is designed to perform fast, in-database machine learning, which can help speed up the prediction generation process and reduce the overall cost of inference.
- use RS ML feature when you want to transfer data to s3 from redshift, so no need to do that just use SQL on rs data and create, train and deploy SM ML model get faster inference on data -  fast, in-database machine learning called

- Amazon Redshift Streaming : ingest the streaming data from streaming source like Kinesis Data Stream and Kinesis Aalytics

- Kinesis data firehose need JSON format to convert data into parquet/ORC - for csv first convert in json then parquet/orc

- AWS streaming Engines : Kinesis Data Stream, Amazon Managed Streaming for Apache Kafka
- from these 2 engines streaming data can be ingested by Redshift Streaming Ingestion
- Kinesis Firehose directly not loading data into redshift, first it will load data in s3 by batches and then by using COPY command it will load data into redshift

- Amazon Textract's AnalyzeDocument API operation reviewed by humans.

- pre-built TensorFlow and MXNet docker containers can be useful for training on local machine - pull them on local
- pip install -U sagemaker - use SM python SDK on local by using this command
- eager execution - programming env in TF for executing operation without building graph
- EC2 instance image can not be downloaded into local machine
- Boto3 ==> AWS SDK for Python, use API to work with aws services

-A VPC endpoint enables private connections between your VPC and supported AWS service
- many EC2 instances can be launch in one VPC

- NACL(subnet level - in side VPC) and security group(resource level - in side VPC) are required by aws resources, not by VPC

- Record processing and reading will be done by KCL
- Record writing will be done by KPL

-  DynamoDB Streams - capturing streaming data from a DynamoDB table.

- Data Firehose buffers incoming data up to 3 MB by default. 

- AUC measures the ability of the model to predict a higher score for positive examples as compared to negative examples.

- We want to produce forecasts for a time series with little or no existing historical data - deepAR - can handle cold start problem

- Apache HBase is just a non-relational distributed database 

- The content-based filtering algorithm ==> predictions based on the product's attributes.

- RBF kernel is used to map 2d data into 3d in SVM

- Scikit-learn CountVectorizer -- word count on how many times a word appears in a document.
