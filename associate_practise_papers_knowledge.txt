revisit == 7

- SageMaker Data Wrangler’s corrupt image transformation to get realworld kind images having blurriness, noise, resolution changes(create corrupt images)

- SageMaker Data Wrangler - such as 
missing values, 
duplicate rows, 
drop field, 
data types,  
filtering, 
joining, 
removing columns 
detect anomalies/outliers, 
feature corelation
pattern detection
visualization
class imbalance - balanced data feature(random oversample, random undersample, SMOTEs)
vectorize text, 
featurize datetime, 
encoding categorical, 
image augmentation, 
image resize, enhance,
PCA,
train,test,validation split are covered.
data leakage in your data. - 300 prebuilt PySpark transformations - It can write query by using sql to get data from variety of sources, it is data visualization tool too, can handle any data formate

- Apply Extreme Gradient Boosting (XGBoost) - handle imbalanced datasets effectively through regularization, weighted classes, and optimized computational efficiency

- adaboost - adaptive-self corrective algo every iteration give more weightage to wrongly predicted data points and correct itself
- gradient boosting - work based on loss function and correct it on next model - sequential
- extreme gradient boosting - parallel so more speed, computation is distributed & cache optimization - it is extension  of GB

- IAM group can not attach with any resource(like notebook instances)
- IAM group have members
- Create roles and assign permissions to them
- Assign roles to users or groups - a single role can be assigned to group too
- Creates IAM group with users who are allowed to assume IAM roles
- Use the iam-group-with-assumable-roles-policy
- policy can be attached to role/individual user

- To grant permissions to any resources like SageMaker notebook instances, you must use IAM roles or directly attach policies to individual IAM users. 

-  DW == does not have built-in capabilities for detecting or grouping duplicate records, especially those requiring fuzzy matching. Data Wrangler is better suited for cleaning, visualizing, and transforming datasets before training machine learning models, but it lacks the machine learning-powered deduplication features provided by AWS Glue FindMatches.


- R2 is co-efficient of deteremination = correlation co-efficient = for regression, value -1 to 1, -1=bad model, 0=not that good, 1=better model, explained variability of dependent var(target)

- balancedaccuracy gives better measure of accuracy when imbalanced dataset like spam email

- inferencelatency available only in ensembling mode

- logloss = crossentropy loss = quality of probability output rather than actual output, for classification problems, 0 is better

- s3 express one zone = directory bucket = support file mode, fast file mode and pipe mode 

- FSx for luster and EFS both will be mounted on training instance first
- FastFile model work well with large files
- scale_pos_weight in xgboost helps to handle class imbalance\

- when IOT sensor send abnormal reading - RCF will use - anomaly detection - identifying unusual patterns that precede failures.
- classify image by image classification, detect people/object from image by object detection algo

- Amazon SageMaker’s multi-model endpoint allows you to deploy multiple models on a single instance. little latency issue it has. for choosing correct model for getting inferenc

- Bedrock does not support fine-tuning the base model within its interface. You need to create your own private copy of the base Foundation Model and then fine-tune this copy with your custom dataset.

- Fine-tuning trains a pretrained model on a new dataset without training from scratch. This process, also known as incremental training, can produce accurate models with smaller datasets and less training time.

- savings plan - commitment of 1-3 years and 72% SAVING ON on-demand instances
- on-demand instances - spikey, short term, unpredictable workload, no upfront cost, commitment

Prunning(feature selection) : identify most impactful features and eliminates the irrelevant one

- Stacking involves training a meta-model on the predictions of several base models. This approach can significantly improve performance because the meta-model learns to leverage the strengths of each base model while compensating for their weaknesses. Random forest mainly used as meta model - can capture the relationships between the predictions of models - capturing relationship between model is important - combining models
- simple voting will not capture that relationship between models
- bagging and boosting use to correct same kind of models prediction by correcting errors or using diff datasets

- Random search works well when a relatively small number of the hyperparameters, for large no of HP - Bayesian

- SM ML Lineage tracking will store info of ML workflow steps... with that info we can recreate ML workflow
it will keep track of all metadata of ml model training to deployment
- SM lineage tracking is main feature for auditing and compliance and experiment is entity of lineage tracking
- Keep running history of ML model for auditing and compliance


- SM pipeline - built-in versioning, lineage tracking, and support for continuous integration and delivery (CI/CD), visualize by DAG.

- Glue Databrew store result back in s3

- provisioned concurrency available in serverless inference, so when you have predictable burst in traffic, keep endpoint warm and resources ready for work

- Use AWS CloudFormation with nested stacks to automate the provisioning of SageMaker, EC2, and RDS resources, and configure outputs from one stack as inputs to anotherto enable communication between them
In cf all components can be treated as stack and create its template and reference it whenever needed, like LB stack template call it everytime whenever you create any ec2 instances... this feature is in CF... if want inter-stack communication then go for CF. = output of one stack will be input of another stack.
- CDK, Beanstalk does not have inter-stack communication facility - they are for deploying app but lack of this feature

- Create a Docker container with the required environment, push the container image to Amazon ECR (Elastic Container Registry), and use SageMaker’s Script Mode to execute the training script within the container
- Script mode enables you to write custom training and inference code & execute it while still utilizing common ML framework containers maintained by AWS.
- SageMaker supports most of the popular ML frameworks through pre-built containers, and has taken the extra step to optimize them to work especially well on AWS compute and network infrastructure in order to achieve near-linear scaling efficienc'y. These pre-built containers also provide some additional Python packages, such as Pandas and NumPy, so you can write your own code for training an algorithm. These frameworks also allow you to install any Python package hosted on PyPi by including a requirements.txt file with your training code or to include your own code directories.
- script mode is inevitable approach to write custom container with your requirements and dependencies -Bring your own container - BYOC

- SageMaker Model Registry to register, track, and manage different versions of models, capturing all relevant metadata, including data sources, hyperparameters, and training code, Manage the approval status of a model, model card

- if SM pipeline is there then callback step is important to check previous step is completed and new one about to start now - By using a callback step, the SageMaker pipeline waits until the one step is complete. like finish aws glue operation in sm pipeline than do next step

- multiple layers of security measures is best in every situation security for gen ai apps

- Data drift occurs when the distribution of the input data changes over time, while model drift happens when the model’s underlying assumptions or parameters become outdated. To address data drift, you should use SageMaker Model Monitor to track changes in input data distribution. For model drift, you should periodically retrain the model using the latest data

- Amazon SageMaker Model Registry also integrates with AWS Resource Access Manager (AWS RAM), making it easier to securely share and discover machine learning (ML) models across your AWS accounts, group can be created for model group too which has resource arn n we can attach policy to group 
- SageMaker Model Registry enables centralized management of ML models, including organizing models into model groups, tracking versions, and enforcing governance policies. By using a resource-based policy, the company can grant cross-account access. A resource-based policy is attached directly to the SageMaker Model Registry resource to define access permissions for specific AWS accounts or organizational units (OUs). This approach ensures secure cross-account access without additional setup of resource shares. It also provides granular permission control for actions like registering, accessing, and deploying models.


- Use Amazon CloudWatch to monitor the API call metrics for the SageMaker endpoint and create an alarm to send notifications through Amazon SNS when the call count breaches the threshold
Amazon CloudWatch is the most suitable solution for this scenario because it provides:
Metrics for API call counts - CloudWatch automatically collects invocation metrics for SageMaker endpoints, including Invocations, InvocationErrors, and Latency.
Alarms - Alarms can be created to monitor thresholds for metrics, such as the number of API calls.
Notifications - When a threshold is breached, the alarm can send notifications through Amazon Simple Notification Service (SNS).

- Use Amazon SageMaker Debugger to debug and improve model performance by addressing underlying problems such as overfitting, saturated activation functions, and vanishing gradients, GPU underutilization.

- If Gen AI model - use EKS as it has high scalability, can provide tens of thousands of active containers for gen ai application  workload demand, give complex orchestration feature and full control over deployment

- Implement a tree-based model like XGBoost with early stopping and hyperparameter tuning, balancing accuracy with reduced training time and computational cost with large dataset
- SVMs with nonlinear kernels can be very accurate but are computationally intensive, particularly with large datasets, for fraud detection.

- if data size limited take more care of data augmentation  --- for overfitting it will work too
- Ensembling is also a technique to reduce overfitting --- may can be one option without much specification of requirement of it
- pruning - feature selection... select relevant features more and less relevant feature will be eliminated - for overfitting it will work too

- to aggregate data from on premise database, cloud based db(rs,dynamodb), s3 in centralize location in s3 lake formation is useful , permission and security also established

- data wrangler can get data from aws cloud & databrick/saas - which can be connected by jdbc... for on premise PostgreSQL or other relational db it wont work

- The oscillating pattern of the loss values during training and validation suggests that the learning rate is too high- overshoot the optimal solution

- SageMaker Pipelines with conditional steps to implement manual approval workflows for model deployment - approved models are specified in sm model registry..  SM pipeline has integration with sm model registry - approval by human or registry logic --- Supports manual approval workflows within automated pipelines.
Integrates seamlessly with SageMaker Model Registry to manage approved models.
Reduces operational overhead by automating model deployment with built-in approval checks.
'- lineage tracking is valuable for auditing and reproducibility, it does not include built-in mechanisms for validating or approving models for deployment.

- target scaling use cw metric to check - avg val of invocationperinstance/cpu utilization , and target value like invocationperInstance
- step scaling - under condition scaling

- Amazon Comprehend ImportModel API operation to import model from one to another account, no need to download data inanother account, set resource based policy in first account, account should be in same region

- Shapley values : assigning each feature a contribution score to the each prediction
- Partial Dependence Plots (PDP) : help understand the overall relationship(marginal effect) between a feature and the model output across the entire dataset, holding all other features constant. - PDP is a global method, while Shapley values provide local explanations.

- model registery has Model registry collection --- while ML pipeline run all the models(based on version) run inside it will be placed under group of models, and put that group under registry
- Model Registry ===> Model Registry Collection ===> Model Group ====> v1,v2,v3... Model with diff versions(based oneach Ml pipeline run) were to train and solve to given problem

$$$$ ==== Amazon SageMaker Model Registry: 

Catalog models for production.
Manage model versions.
Associate metadata, such as training metrics, with a model.
View information from Amazon SageMaker Model Cards in your registered models.
View model lineage for traceability and reproducibility.
Define a staging construct that models can progress through for your model lifecycle.
Manage the approval status of a model.
Deploy models to production.
Automate model deployment with CI/CD.
Share models with other users.

Model registry collections likecv ----fraud detection models, customer segmentation models etc

- Athena => query on s3 data by date - create table as select(CTAS) query and store result by transforming format to parquet or orc for faster access

- MAE - less sensitive to outlier as it give direct diff between actual & predicted outcome
- MSE - square the error


- Validation sets are optional, The validation set introduces new data to the trained model. You can use a validation set to periodically measure model performance as training is happening and also tune any hyperparameters of the model.
- Test set is used to determine how well the model generalizes: used on final trained model to assess its performance on unseen data.

- first-time, spikey, unpredictable, short-term workload ==== on demand instance
- real-time prediction ==== reserved instance - high available
- committed, steady usage === savings plan
- fault tolerance, heterogeneous, stateless, flexible start & end ==== spot instance

AWS Techniques====
Supervised Learning == LR, LR, Neural Network(predicting digit from handwritten image), Decision Tree
Un-Supervised Learning == clustering, association rule(Rule based relationship between input-output), probability density(probability-not sure-unsuper), dimensionality reduction


FM based
- RAG === not train model only extend its knowledge, no weight change. when data changes frequently like inventory, pricing 
- fine-tunning === further small training, so weight will be changed init. when specific requirement then do it
- Prompt engineering === not change weight of FM

- Use a tree-based algorithm like XGBoost, which offers a balance between accuracy and interpretability with feature importance

- choose F1 & recall when FP,FN both are important and two options are needed to choose while cant afford FN mainly

- If want to create baseline model - use Linear learner = simple model
- Autopilot - give best high performant model
- jumpstart - pre-built model, maybe not best fit for baseline model to particular requirement

- DW will drop unused columns with little/no coding as it is ml- based

- Hyperband = reallocation of resources, early stopping, parallel training jobs
- Grid Search === computationally intensive

- Gradient Decent = optimization algo, while training calculate params (weight/bias) to minimize loss 
- HyperParameters == batchsize, learning rate etc === define model, send before training with model

Endpoint type selection
- dedicated endpoint = lowest latency, high TPS
- multi-model = 
1. large no of models - frequent/infrequent(take time to load) used model,
2. reduce hosting cost, 
3. reduce deployment overhead - sm load model in memory and scale them based on traffic
- multi-container = isolation

CloudWatch can do logging?


ensembles models
Bagging : multiple model, parallel, random sampling with replacement
Boosting : multiple weak learner, sequential, cost based error correction of previous model
stacking : many heterogeneous model, output of all model will stacked & sent to meta model, good accuracy, meta model use cross validation, use strength of all model and work on weakness of all and give best output

- when data distribution change - in live data and train data ===== consider feature attribution drift --- can be
- SM Clarify has two monitoring job
1. check bias drift - check bias in model's output
2. compare feature attribution drift in live and train data(when data distribution changed)

when SM Mlops - pipeline is not in option
- to manage containerized application having many containers for training, pre-processing, deployment etc... you can use EKS for orchestring those containers. - it will do  building, storing, deploying, and maintaining c app
- your containers image you have to put in ECR first than use EKS
- to do automating CI/CD codepipeline can be used 

- When data loss is issue with EC2 instance reboot, don't keep data in EC2, except process it with other services like KDS/KDA

- Parameters : use to generate output and get response from model
- HP : customize the model and control training process

Prompt: input provided to model to guide the model to generate output for that given input

- IntervalInSeconds : in Firehose buffer size can be set to 0 so telling firehose to do data delivery as soon as it arrived no buffer it. For latency reduce requirement, this params is in buffering hint : default buffer interval is 60 sec

- when require good response time & high throughput ===== use auto-scaling 
- autoscaling work with cloudwatch check == metric(InvocationPerInstance) and target to target metrics
- one model dont use multi-modal

- when latency is less of concern then use asynchroneous inference otherwise not

- Kendra : Amazon Kendra provides search and Retrieval Augmented Generation (RAG) functionality for your application.

- predictable traffic pattern ==== > use provisioned concurrency (pre-specified-keep endpoint warm) with serverless inference
- not predictable traffic pattern ====> use auto-scaling 

- In EFS : for training data should be inside EFS... no s3 access is there

- All other mode will read data from s3 bucket.
how to choose mode for training : 
- small dataset (50-100gb) : filemode
- small dataset & all files are small : use serialization to reduce download overhead, file format like TFRecord, WebDataset, RecordIO
- for larger dataset & many small files : where serialization is not possible - FSx for luster gives hundreads of giga bytes/sec, millions of IOPS ideal when many small files
- for larger files : use fastfile mode 

- ECS --- standalone batch processing job
- Lambda/eventbridge --- schedule task

Cluster Size = no of GPU * no of instances

-pipeline execution is used to achieve true parallelism in model parallelism as it has loss of performance becuase in it data processed by GPU sequentially 

- Bedroack mainly used for NLP, generative task

Related to CloudFormation
- The AWS::SageMaker::Model CloudFormation resource is specifically designed to define an ML model hosted on Amazon SageMaker. include artifact, container, iam role
- AWS::EC2::Instance : manually host model on EC2 instance, not using SM hosting capability

- Write a training script in Python that adheres to TensorFlow's Estimator API, upload it to an S3 bucket, and use SageMaker script mode with the built-in TensorFlow container to execute the script  -- containers also provide some additional Python packages, such as Pandas and NumPy, so you can write your own code for training an algorithm. These frameworks also allow you to install any Python package hosted on PyPi by including a requirements.txt, specify sm version in container

- Nested types can be stored in: Parquet like array and object in column format
- XML not in canvas, csv,parquet,json,excel in canvas

- notebook instance will use IAM role to access bucket/service
- prefix added to bucket to make data compartment 
- iam role will have resource based policy specifying that prefix bucket

- sm canvas has data validation feature which check data quality after you import data n target column

- Model Pruning : Reduce Model size(quantization) - by removing some neuron(mostly not active) & magnitude of weight(neuron*neuron) which are not impactful to calculate output.- perform calculation in low precision like 8 bit rather than actual 32 bit ---- quantization   
- pruning and quantization will reduce computation requirement by model without much impacting on accuracy

- RANDOM_CUT_FOREST algorithm - A built-in anomaly detection algorithm within Apache Flink

- SM Canvas has now Data Wrangler init

- https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html

- ordered split & in unordered data split by key  will avoid data leakage 

- SageMaker Studio Classic offers an integrated environment for creating, training, debuging, deploying, and monitoring & visualization feature for ML models. 

- FM models are used for text, image & embedding generation & classification 

- base model : fm which is packaged ready to use

- model customization : fine tunning(new train dataset for particular task)  & continue pre-training(no labeled data)

- you can filter the cost by using tag on resource and based on those tag get detailed cost report

- CloudWatch Lambda Insights detailed metrics to monitor the Lambda function’s memory usage, CPU usage, and invocation times, cold start, lambda worker shutdown etc

- if model forgetting previous knowledge(catastrophic) then use transfer learning for new training data
- Catastrophic forgetting when new distribution data come and we don use incremental training/fine tunning in model.
- In fine tunning/incremental training will retrain the model with new data set not change the weight of model


- L1 reduce noice in model and produce more sparse model
- L2 stabilise weight when high co-relation between the features

- Renate is python library provided by aws to allow automatic model retraining taking care all problems like catastrophic forgetting n so performance degradation because of that

-  Data augmentation is more relevant for addressing overfitting

- Shadow testing model will not split traffic, it will copy live data and give inference on them in hidden mode, while a/b testing will split traffic on live environment, so shadow testing will not do user exposer, a/b will do user exposer.

-increase batch size and reduce epoch will reduce training time and avoid overfitting because of large batch size

- aws lambda is compute service which let you run your app code without managing server, by YAML script you can provide security, access configuration

neural network will need categorical data in one hot encoding format as if use label encoding it can assign more weightage as per label int... which is not right thing in NN --- NN will go with 0-1  based data more

- label encoding can lead to ordinal relationship between category's value... so be mindful before assigning it
- AWS Glue jobs cannot be directly integrated as processing steps in SageMaker Pipelines because Glue is an independent service, not a SageMaker-native processing task.

- The key features of XGBoost are parallelization, distributed computing, cache optimization, and out-of-core processing.

- In machine learning, while "residual error" and "loss" are related, they aren't exactly the same. Residual error represents the difference between the actual and predicted values, while loss functions quantify the negative consequences of those errors, used to optimize model performance. 

-  SageMaker ML Lineage Tracking - Automatically tracks lineage information, including input datasets, model artifacts, and inference endpoints, ensuring compliance and auditability.

- SageMaker Experiments tracks and organizes experiment trials

- Concept drift happens when the relationship between input features and the target variable changes over time.

- DeepAR is designed for forecasting future time series data - not anomaly data checking and predicting future failure
- DW - simplify data aggregation, cleaning, and feature engineering for machine learning workflows. It connects seamlessly to data sources like Amazon S3 and SQL databases, allowing engineers to preprocess large datasets and detect anomalies efficiently. It also provides built-in visualizations for analyzing trends, correlations, and outliers.

- SageMaker Pipelines with conditional steps to implement manual approval workflows for model deployment

- scaling  can be based on schedule too

- AWS Lambda is excellent for serverless applications to deploy - Lambda is better suited for lightweight, event-driven tasks rather than long-running, complex inference jobs.

- AWS Elastic Beanstalk is a managed service for deploying applications, but it is not designed for orchestrating complex ML workflows with multiple resource types like SageMaker, EC2, and RDS.'

- pipe line execution : 
1. create pipeline(batch/realtime inference)
2. give role to event bridge to strtpipeline execution
3. schedule rule based action(like cron job)

- if model is not giving proper output as expected while training , we can analyse training by debugging or tensprboard to check the process of training, get to know gradients and what happens there why result is wrong
This feature is for debugging the training of deep learning models using PyTorch or TensorFlow.

- for real-time prediction - reserved instance will be more suitable than on-demand as it is more cost effective

- if  provisioned concurrency is there than - it will keep endpoint warm to handle any request arriving... which inccur cost.

- multi-model => frequently/infrequently used model, on same infrastrucure, less no of instances to host all at once, little latency issue for infrequently used model, highr cost saving, cpu & gpu backed model support, similar latency, similar size of model then use it
- model should be load into memory if invocation is arrived.

- use multi-container --- multi diff ==> dependency/runtime/infra, &  isolation is required

- when models are best at something and not any weak point - use hybrid approach to combine all models based on weihted average and get best final model out of it 
- stacking, baggin, boosting resolve weakness and use streanth of models
- from model monitoring result either do retarining or set baseline again if found drift
- monolithic architecture will require whole all redeployment for any single update, micro-service is better

- troughput n response time is bad use auto-scaling
- Elastic network interface(virtual network card) : bridge between instances from vpc and network
- FastFile mode can reach a throughput of more than one GB/s when reading large files sequentially using multiple workers
- if handling imbalance dataset --- LL & xgboost ==== choose LL
- step function can handle complex workflow like large files... lambda can not do that
- small files === fsx for luster, large files === fastfile mode   ==== high throughput and low latency
- if ML high computation choose gpu instance
- cloudwatch log will capture all the log matrics(details log data)  ==== low latency issue identified by CW log
- check key usage check cloudtrail
- autoscaling+multimodal  === useful for 

- AWS Compute Optimizer : Get recommendations to optimize the performance and cost of your AWS resources.
Identifies and recommends cleaning up idle resources like EC2 instances, Auto Scaling groups, EBS volumes, ECS services on Fargate, and RDS instances to reduce costs.
AWS Compute Optimizer provides detailed recommendations for optimizing AWS compute resources, including EC2, ECS, and EBS, based on utilization metrics.

- tag based access control is best for assignign fine grained access control
- cloudwatch dashboard are for technical users, for non-technical user use quicksight.
- combination of f1 score + recall will be answer for trade-off question mostly
- Kendra ==== semantic search capability & support natural lang query & indexing capability for your data
- SM pipeline canbe triggered directly by eventBridge - by setting it as target to eventbridge rule - IMP
- AWS CodeDeploy with a rolling update strategy allows for incremental deployment across the EC2 instances, minimizing downtime and enabling easy rollback if issues are detected.
- QuickSight parameters and controls to create interactive filters === sel region and p category dynamically
- if containerazed ml workload batch job/workload then choose only ecs/eks
- IF eks application === use auto scaling with ==  HPS = horizontal pod autoscaler(by cpu n memory),,, fargate will be used by aws resources not eks == == choose based on that requirements
- if want to sort data and get result then use sort key for dynamodb
- global secondary index - GSI ==> when you want to query the data by some attribute - along with optional sort key, not primary key  ===== used with dynamlodb ========= best configuration for dnb table
- SM clarify has bias reports that can be run on training and live data to generate pre, post training bias metrics - to compare it and identify to retrain the model, rather than goiing for shapley values
- for identifying bias and fairness in model prediction use f1 score and auc-roc eveluation -by roc cheking TRP across different demographic user groups
- CodeBuild has buildspec file ===> for command and setting and important 
- codebuild will incorporate process of pre-processing and training and artifacting kind of 
- amazon lookput for equipment will be useful for model retraining
- Lambda === real time processing, 
- SM === real time ML running
- secure n automated process can be done by lambda
- Personalize has realtime and auto scaling already, no need to provide seperatly by aws features
- Adding a controlled amount of noise to images during training can often improve a model's ability to generalize and handle real-world variations, making it more robust
- simple model will memorise data more and complex model will not
-for chat bot real time ineteraction will be there if not mentioning
- integrating external data for model prediction use Glue for pre-processing and SM to train model based on it
- variance and bias is there use boosting to resolve it
- Amazon DevOps Guru uses machine learning to identify operational issues and suggest remediations
- after Glue DataBrew job will create data metrics that can be sent to somewhere like in CW
- elastic load balancer === distribute the traffic, but auto scaling will implement multiple instances

- step func == data ingestion, training orchestra
- SM === realtime
- step function === recokgnition, processing, storing
- sm realtime + vpc === high availability, scaling, privacy , compilance
- data sync==== Kendra + Lmabda
- latency === monitorin === cloudwatch
- lambda will call translate, transcribe, comprehend based on s3 event notification
- realiability ==== multiple models kind of / multiple models
- reduce computation with good performance use === transfer learning
- chatbot === normally will be real time
- security === encryption- high availability === auto scaling
- lookout for equipment has model retraining feature - with manual mode and managed mode
- Reckognition doesn't need lambda for image processing === it has that inbuilt
- lambda should be triggered by eventbridge or something else
- Macie discover sensitive data and also protect it by automatic remediation
- lambda === processing
- lex---(its log!!) ==> firehose ==> s3
- noicy data ====> do PCA
- manag and analyse customer transaction data (from polly) - kind of storing  === Dynamodb


- large dataset => put in s3 => built in streaming capabilities of SM
- Kendra == create metrics for operation of creating indexing of data ==> monitored by CW
- latency === for fast individual prediction(use gpu), throughput === overall no of datapoints accessing(per say auto-scaling)
- fraud detection == data goes to s3, lamda to process, then use on fraud detector
- dynamodb + lambda === event driven operational task not for realtime inference
- Amazon Personalize automatically retrains the models backing your recommenders every 7 days. Just provide data, it will do retraining so lambda can work to provide data 
- macie can be used to identify unusual data access patterns
- lambda + retries with DLQ - Dead Letter Queue by SQS - to handle failure - 
In AWS, a Dead-Letter Queue (DLQ) is a queue that captures messages that couldn't be processed successfully by a service after multiple attempts. It's commonly used with Amazon SQS, Lambda, SNS, and other services to help you debug and handle message processing failures.
- operational issues === DevOp guru
- translate + api gateway + lambda + dynamodb
- for ml dont rely on cpu only for auto scaling as i/o & memory requirement is also need to be considered
- EKS === nodes + pods both need to considered for auto scaling with custom metrics, Configure Cluster Autoscaler to add or remove nodes based on pod scheduling requirements and scale pods horizontally using HPA based on custom metrics tailored to the ML workload. for auto scaling
- polly ==== clarity + appropriateness => punctuation + grammer need to consider
- XGBoost ===  feature importance scores and other interpretability tools
- Automated fragmentation and indexing enhance the accessibility and processability of video data in real-time, crucial for effective machine learning pipelines.
- to sent notification on slack === aws chatbot will be used instead of SNS
- stacking can be useful === diff model give best inference on diff subset of data == combine them n use as 1
- bedrock == data validation rules for compilance
- Route 53's DNS routing capabilities ensure that requests are directed to the closest region where Polly is hosted  ===  terms of latency and availability
- AWS CodeDeploy includes an automatic rollback feature
- Amazon SQS to decouple data ingestion from preprocessing stages handled by a dedicated AWS Lambda function triggered by SQS
- SQS - serving as a message queue that temporarily holds data until it can be processed
- macie - examine for sensitive data and anomalies or diff access patterns
- machine learning job status changes, Amazon SNS sends a notification to all subscribers of the topic
- ECS Service Auto Scaling with target tracking scaling policies. - target like cpu/memory usage
- Amazon Redshift Concurrency Scaling automatically adds additional cluster capacity for autoscaling
- realtime === reserved instances
- AWS Elastic Beanstalk =deploying and managing web applications and services = capacity provisioning, load balancing, and auto-scaling, health monitoring
- S3 Select == retrieve only the data needed from s3 objects, reduce processing and loading, specific slices of data are needed for training models
- Devop Gurru === notifying about unusual patterns in the application metrics that could indicate issues like memory leaks or high response times, which can affect model performance.
- DGR Detect abnormal application behavior using machine learning
- DGR anomalous behavior along with actionable remediation recommendations.
- DGR analyze application metrics, logs, and events
- anomaly detection in seasonal event use large dataset so model can differenciate between anomaly and high traffic spike kind of normal event based on season
- apply row/column level security in QS for user to restrict on accessing secret data
- AWS CodeArtifact - artifact repository service = securely store, publish, and share software packages used in software development process. has versioning capability, Store and share artifacts across accounts, share private packages across organizations by publishing to a central organizational repository,approval workflows with CodeArtifact with CT, evenbridge, publish packages
- SageMaker Model Monitor  for monitoring the performance and usage of ml models in production
- aws chatbot : push/alert the messages to communication channels such as slack, and sns will be sending messages to subscribers to only within aws , provide monitoring and interaction
- to check bias : check the true positive rate across different demograhic user group - should be similar
- A callback step allows you to integrate any task or job outside Amazon SageMaker as a step in the model building pipeline. When a callback step is invoked, the current execution of a SageMaker model building pipeline will pause and wait for an external task or job to return a task token that was generated by SageMaker at the start of call back step execution. this kind of outsider job are like EMR job, Glue job for etl.
- you need role attached policy to run this emr callback step or any lamda job within SM pipeline
- in SM pipeline sm tools has tighter integration, outside job like glue, emr required callback step execution
- SM processing job then resources will be managed by sm, you need to provide python script to run or docer container to execute processing in SM processing job, but if you want to use processing already running on EMR cluster, you need to call that whole process as callback step in sm pipeline
- EKS, ECS === orchestrate, manage, scale, run, containerized application   ===== service for
- EKS has ci/cd support, it can hnadle complex deployment and scaling required for complex mocroservice architecture
- ECR = to register container images, not for for running it
- amazon lightsail container === low cost, simple container deployment/running service - not for complex/scalable container orchestration service
- Amazon Kendra to index the archived articles in the S3 bucket and enable semantic search with natural language queries
- kendra has GenAI index. New indexing to use RAG in bedrock knowledge base, tools, q business
- Transformed Data == features === can be put in feature store
- performing complex transformations on petabyte-scale data is most efficient with SageMaker Data Wrangler
- image , text are unstructured data
- AWS Compute Optimizer to analyze the specifications and utilization metrics of your AWS compute resources
Compute Optimizer is a service that analyzes your AWS resources' configuration and utilization metrics to provide you with rightsizing recommendations. 2 dashboard-level metrics: savings opportunity(cost based, monthly savings -  account level, resource type level, or resource level) and performance improvement opportunity(underprovisioned resources at the account level and resource type level).including EC2, ECS, and EBS, based on utilization metrics.
- Trusted Advisor has broad tools that focus on overall account health and optimization across multiple areas, including cloud cost optimization, performance, resilience, security, operational excellence, and service limits; Compute optimizer offers more specific tools that focuses on compute resource optimization.
- Masking PII - Redshift, Glue DataBrew, (comprehend + data wrangler)
- DataMasque offers a solution specifically designed for masking sensitive data within Amazon Relational Database Service (RDS) databases.
- Linear Learner can be used for binary classification, XGBoost is generally more effective for tabular data and predictive tasks.
- Real-Time Inference == millisecond latency = payload sizes up to 6 MB= processing times 60 seconds.
- async Inference == no subsec latency = payload sizes up to 1 GB = processing times 60min
- serverless == no infra set, intermittent workload == payload sizes up to 4 MB= processing times 60 seconds.
- Batch = payload sizes up to 100 MB, 15mins
- multi-model  use cpu & gpu backed model, little latency issue, so if need high IOPS or latency sensitivity then only go for dedicated endpoint
- to call serverless endpoint = invokeendpoint request
  - model download, server respond to ping(endpoint) within  3 minutes
  - response to == inference request == timeout 1 minutes invocation(endpoint)
- multi-model - very cost effective as it use very less no of resources and little latancy with cold start problem for infrequently used models
- model built outside SM canvas must be register in registery, & user to access model artifcat should have bucket permission where it is stored
- outsided model = model should be built in sm studio classic. user should be in that
- sm canvas and sm classic both user should be in same sm domain
- explainability, compliance, transparency ==== clarify
- Asynchronous Inference enables you to save on costs by autoscaling the instance count to zero when there are no requests to process, so you only pay when your endpoint is processing requests.
amazon recognition use face collecion to validate match with collection. No need to label match as collection has that label alredy. And do face matching. - face will be stored as vector, you have to create user to upload more images of same user
- Batch transform mainly use for large data processing when you dont need persistent endpoint
- Dynamodb can be used directly but it is appropriate to export data by Glue to s3 for faster accessing in ML
- Normalizing the sensor readings = ensuring that all features have the same scale.
- Using the entire dataset for training and relying solely on cross-validation does not provide an independent evaluation of the model’s performance. = not good practise
- On-demand instances provide flexibility for sporadic training jobs
- auto-scaling provisioned resources ensure efficient handling of traffic spikes for inference
- provisioning resources refers to the process of making AWS services and their underlying infrastructure available for use
- reserved instance === commitement of use
- to do translation === use step function ==  serverless and managed workflow 
- cloud formation will not create ci/cd pipeline
- if need more generalized model, with good hps, not biased model n autopilot is there choose it
- provisioned concurrency === cold start can be handled
-  DNNs remain complex and may not fully meet the interpretability
- Latency ensures predictions are delivered within acceptable timeframes, throughput measures the system's capacity to handle traffic
- pipemode = choose for large data without downloading it in local. other will download it
- pre-trained model NOT serve  === as a baseline
- CodeBuild + CodePipeline = automated build and test processes whenever changes are pushed to the source repository, ensuring ci/cd
- step scaling = predefined scaling, NOT dynamic
- SSMl - polly - provide all features like - pitches, pauses, news caster style, PRONOUNCIATION, whispering, emphasis. so its universal to use in polly with LAMBDA
- comprehend can identify PII data from all text(not from s3 bucket) easily 
- use macie for PII for in first place, unusual download activities or access from risky locations
- GuardDuty = detecting and responding to threats and security vulnerabilities = AWS environment. 
- Macie focuses on discovering, classifying, and protecting sensitive data stored in AWS, helping you maintain data privacy and compliance. -  Lambda function to remove the sensitive data.
- you could create a tag called "Project: MyProject" and then create a budget that monitors the costs associated with all resources tagged with "Project: MyProject". This allows you to track the costs of a specific project and receive alerts if it exceeds its budget. like that for add tag to user profile too
- NACL - subnet - allow, deny rule.. default deny rule not let anybody in = to block IP add = stateless
- SG - resource - allow.. default allow outbout traffic, for inboud require explicit allow = stateful
- WAF - not work with VPC(network layer), its application layer
- S3 Event Notifications = call only Lambda or SNS, SQS  to do something,while eventridge will do anything by calling evenrule
- Redshift = dynamic data masking policies
- invocation metrics = CPU utilization and request count(traffic) ======= auto scaling can happend based on
- dynamodb - GSIs enable flexible querying based on other attributes. one table composite primary key
- The bias versus variance trade-off refers to the challenge of balancing the error due to the models complexity (variance) and the error due to incorrect assumptions in the model (bias),
- CloudWatch Alarms = trigger auto-scaling
= Amazon FSx for Lustre file system- caching data locally for faster access, compute-intensive workloads
-  AWS Secrets Manager-(Lambda function - api key rotation.)(Managed Rotation -database credentials rotation)
-  AWS Trusted Advisor - underutilized or idle resources and reserved instance purchasing opportunities.
- SageMaker's batch transform is ideal for preprocessing and postprocessing
- AWS Batch is optimized for batch computing
- ml.c5.large,ml.m5.large = cpu based instances
- g4dn = moderate GPU workloads
- p4d instances (NVIDIA A100 GPUs) - large-scale deep learning workloads
- VPC endpoint for Amazon S3 in the 1 account - cross-account bucket policy in the 2 account
- vpc endpoint === gateway endpoint, interface endpoint (not using public internet)
- Transcribe == S3 for input and output storage.
- AWS Elastic Beanstalk = automatic scaling and deployment, not = rollback 
- AWS CodeDeploy with a rolling update strategy 
- scale the application based on CPU or memory utilization.
- Attach an IAM role =SageMaker notebook instance with s3:GetObject permissions 
- kms:Decrypt permissions for the KMS key.
- IAM roles and policies are more secure and scalable rather than ACL
- add the IAM role’s ARN to the KMS key policy for kms:Decrypt permissions.
- data aggregation  === use EMR, Flink
- step scaling policies == condition = memory usage.
- sometimes model monitor + shadow variant with updated data trained willbe helpful to detecting and addressing model drift
- automatic key rotation for KMS keys to ensure cryptographic best practices
- blue/green deployments has diff endpoint of model so it is best for auto rollback
- codedeploy has rolling update with auto rollback cap, but not seperate endpoint
- SageMaker's blue/green deployment strategy and endpoint traffic splitting provide robust solutions for deploying new model versions
- Deploy the model on SageMaker endpoints with VPC-only access+SG = network isolation and security
- explore data efficiently = DataBrew to create data profiles of the dataset, apply transformations to clean the data, and validate the transformations with a sample of the dataset.
- LL - simple, baseline, interpretable model
- lightweight model using AWS Lambda with API Gateway
- bagging, boosting works in same model family
- Step func =  Map states = parallelism = iterating over a list of items
- Step func =  parallel states = model training tasks concurrently
- increase batch size lead to less update to params n less computation time n cost
- reduce epochs with above combo reduce training time    = convergence and performance while addressing training time issues
- AWS CodeBuild with a buildspec file to define the build commands and settings, and integrate with Amazon S3 to store the build artifacts.
- private repository in Amazon ECR, configuring repository policies to control access, and enabling image scanning to detect vulnerabilities is the best approach to securely store and manage container images

EFS
- Mount the EFS file system on each SageMaker instanc, datasync is fro migration between data store
- Throughput mode – Elastic(unprdictable, spikey workload), Provisioned(when you know workload, cost occurance), or Bursting(storage based, throughput that scales with the amount of storage in your file system)
- Bursting Throughput mode can handle spiky workloads ---- busrty-spikey
- when multiple instances are accessing same data then bursty-pikey will work
- Performance mode – General Purpose or Max I/O
- Max I/O performance mode has higher per-operation latencies than General Purpose performance mode. use for highly parallelized workloads that can tolerate higher latencies 
- For faster performance, we recommend always using General Purpose performance mode.
- normally use general purpose performance with burstling through put mode
- SageMaker Clarify to generate pre-training and post-training bias metrics 
- Shapely value === feature importance
- CodeBuild - preprocessing, training when ci/cd require
- OpenZFS == general-purpose file storage
- NetApp ONTAP = advanced data management features
- FX for luster = high computation, high throughput,low latency, concurrent instance access, large dataset access
- AWS Trusted Advisor to detect underutilized SageMaker endpoints and suggest downsizing or termination to reduce costs - optimization - identify underutilized SageMaker resources and potential security risks, such as overly permissive IAM roles or exposed SageMaker notebooks.
- QuickSight calculated fields to create new fields
sns can send email too
- model - quantization decreases the precision of the weightsk
-pre-trained - bedrock, pre-built - jumpstart
- Glue job =Dynamic Frames allow seamless handling of semi-structured data, and the 'G.1X' worker type provides an optimal balance of performance and cost for moderate workloads.



F1  ===  2TP / (2TP+FP+FN)

RMSE = accuracy == care about right & wrong answers

-model parallelizam library

- data parallizm =====> shard calculation of gradient, shard trainable parameters
- model parallizm ====> shard optimization state means shard weight calculation

- all this sharding will be done on multiple GPU resources

====== Accelerate training 
- Elsatic Fabric Adpter = network device === use better network bandwidth
- use NVIDIa GPU, nvidia communication collective library


MISc = minimize communication scale of diff GPU instances --- use large
P4de  ---- largest instanse(high networking & memory) to do training of trillians of params model  === minimize c overhead

LLM
P === probaility for token inclusion
K = 1,2,3.... etc  no of token can be put for inclusion list
Temprature === randomness in token (high more random, less more consitent)
- continue pre training is also a type of fine tunning
- elasticSearch , openSearch work as vector database... store n search by vector
- aurora, memorydb can also work as vector db
How knowledge base can be created
- upload text docin s3
- create embedding of those text by cohere or titan
- store those embedding in vector db
- now do vector search from those db, when prompt invoke to ask something which is RAG based 
- creating vector and running prompt will be done by bedrock

Create RAG with knowledge base
- user query => vector search => get no of response(context)
- augment that latest context with u.query and send in to bedrock 
- get rephrased response from it

Add this RAG into Agent------- agentic RAG

- contextual grounding check === final output data relevent to context recieved from vector search
- relevency check = relevent to u query

LLM Agent - agentic AI
LLM can do anything outside of its knowledge - with agent
agent  == tools => have memory => have planning module 
memory ===> chat history of LLM & some external datastore
user request comes in ==> agent will use LLM to decompose that request by LLM and identify if
something is there in agent to help with that request
- agent has prompt how to use it
1. use knowledge base to give answer
2. Use some lambda func --- to do anything 

Agent Prompt ==> problem comes in ====> distil down by llm and idebtify what to use
1. action group (lambda func) = tool
define params for lambda fun ==== things will be done in plain text with help of LLM
2. knowledge base(agentic RAG)

result of 1 & 2 is Augmented Prompt

- Code Interpreter option in agent
agent can write code by its own to do something like (answer the ques/produce chart/calculation) --- self build agent

to deploy agent on out side world === alias ==== deployed snapshot
1. on demand Trhoughput ==== quoata set at account level
2. Provisioned Throughput ==== can purchase 

invokeAgent by alias ID, Agents for amazon Bedrock Runtime Endpoint

- serverless application manager will create infrastructure for serverless app  --- like lambda it can create


- SAM cli will help you to test cloudformation on local
- cdk app === which has lambda fun
- run cdk synth and create cf template of it 
- that template will be invoked by sam cli command

global services include: 
Amazon IAM (Identity and Access Management), 
Amazon CloudFront (Content Delivery Network), 
Amazon Route 53 (DNS service), 
AWS Global Accelerator, 
AWS Organizations (centralized governance), and 
AWS WAF (Web Application Firewall). 


A/B testing ===== comparing two version of models
Green/Blue deployment ===== reduce downtime to realese the new model along with comparing

- payload size is file size to handle for each inference request by model 
serverless === 4mb
asyn ==== 1 gb
batch === 100 mb
realtime === 6mb
- Sagemaker Data Wrangler import database from only   --- s3, rs,athena, emr,snowflake, databrick & 50 +
- for tabular data choose xgboost

-LLM model not giving same output everytime ---- and you want then use temparature and k params... dont use retraining as it is resource intensive
- If chatbot - will use lambda, but not deploy on lambda as it has latency issue
- Accuracy is not suitable for imbalanced datasets as it show high performance even if the model ignores the minority class
- AUC-ROC is a useful metric for imbalanced datasets, it is less effective in providing actionable insights compared to precision and recall in this scenario.
- LL  ---- Balance_multiclass_weights, Gives each class equal importance in loss functions, for imbalanced dataset --- built in capability
- CloudFormation WILL NOT define the CI/CD infrastructure as code
- for codebuild ==== need to define custom docker container to deploy then it will go into ci/cd pipeline
- AWS CodePipeline == build, test, and deployment stages & (AWS CodeBuild + custom Docker) = best CI/CD pipeline 
- Glue can handle === s3, rds, dynamodb, redshift, other sql db, on premise db, JDBC/ODBC
- high variance ==== sensitive to little change in training data as it learn noice from data
- high bias === model is so simple n not capturing patterns
- high risk borrower and low risk borrower ====== Using the F1 score balances the trade-off between precision and recall, which is important for considering both false positives and false negatives. Prioritizing recall ensures that high-risk borrowers are identified, reducing false negatives.
- tagging resources is more efficient and effective for tracking costs than relying solely on user profiles
- only track === CT, monitor & do something === CW
- Kinesis analytics can not ingest data direct from IOT 
- Budget is account based ==== means SM user profile based ==== if want to make it resource based by adding tags on resources, more modification will be needed to set monitor budget and get alert, so keep it simple to user profile base only, add tag to Sm user profile --- based on that budget will work automatically 
Adding tags to the SageMaker user profile allows costs to be tracked for specific resources. AWS Budgets can then be configured to monitor these tagged resources and send alerts when thresholds are breached.

- Customer Churn prediction === XGBoost, as in linear learner it will take one param and do binary classication, but for customer churn multiple dimesion dta will be followed to give prediction.

- dynamodb data can be used directly by boto3 api for notebook for analysis in ml... when need faster access

Data wrangler have class imbalance problem resolution... Glue Data Brew doesnot have it

- identify bias base on some feature like income ==== CDD evaluates the disparity in positive prediction rates across demographic groups, conditioned on a specific feature like income, to detect bias that may not be apparent when only considering overall outcomes------ Conditional Demographic Disparity (CDD) measures the difference in "positive prediction rates" between demographic groups, while conditioning on relevant features like income. This allows you to identify subtle biases that might be masked when looking only at overall predictions, ensuring that the model's decisions are fair across different groups given their specific circumstances. Dont follow accuracy(tp,tn) for cdd
- Adding tags to the SageMaker user profile allows costs to be tracked for specific resources. AWS Budgets can then be configured to monitor these tagged resources and send alerts when thresholds are breached.
- for recommandation model dont go for lambda as it is large model

RCF - identifying unexpected patterns in sensor data
DEEPAR - DeepAR is designed for forecasting future time series data
Answer === RCF ============= Good for real-time monitoring to catch unexpected behavior that could indicate a failure is happening or just happened.
Use case: “Sensor readings are suddenly off – raise an alert for potential failure.”
Answer === DEEPAR =============Best for predicting when a failure might occur by analyzing trends, patterns, and seasonality.
Use case: “Predict that equipment will likely fail in 3 days due to increasing vibration levels.”


- AWS Lake Formation is specifically designed for aggregating and managing large datasets from various sources, including Amazon S3, databases, and other on-premises or cloud-based sources. It simplifies the process of:
Integrating data from various locations.
Cataloging the data in a centralized data lake.
Managing permissions and security for the aggregated dataset.

- so aggregation, preprocessing, permission, sercuirty, lifecycle === LF   === on multiple dataset
- if big size single file --- for transformation go for datawrangler not glue
- cw log will not trigger notification, cw alarm will ====== both will do monitoring though
- to improve performance you can choose XGBoost based on condition though
- to aggreagate data EMR, FLink can be useful - i think they will increase operational overhead but in question/answer it is not- saying
- CodeDeploy supports =====> 
1. in place deployment(having rolling update- has downtime) 
2. Blue/green deployment - no downtime


Governance ====== Amazon SageMaker Model Registry, AWS CloudTrail, and AWS Config
Compliance  ====== AWS Artifact, Amazon Macie, Amazon SageMaker Clarify
- on serverless endpoint ===== you can not configure vpc

FSx for Lustre === has additional operational overhead.
Linear deployment updates progressively and sequentially --- no testing in-between
- cross account access can be done by IAM role
Higher Top P — less likely outputs.
- visually analyse training by tensorboard
- avro binary format -- data and schema init
- if binary - serialization possible
parquet - columnar, analytics, encoding, compression
EFS = linux  === use security group 
EFS Bursting TP == throughput based on the amount of data stored in EFS. more data more tp
EFS Elastic Throughput Mode == unpredictable, spiky workloads
EFS Provisioned Throughput Mode == exact throughput
EFS Performance Mode General = all app, low latency
EFS I/O Mode == high TP, High latency, high parallel

LF workflow
1. create analyst role 
2. create glue connection to diff dbs
3. create s3 bucket
4. create Lf connectio  to s3 bucket
5. create db in LF for glue data catalog
6. create blueprint of workflow(db snapshot)
7. run workflow
8. give SELECT permission to athena, rs, emr to access 

- CloudTrail ===== event/api call
- Governanace, audit, compilance ===========> cloud trail
- CloudTrail ==== region (one/all)
- operation happens on resources =====> management event (CT)
- CT insight === unusual activity checking by checking baseline -- check write event mainly and sent to s3, ct console, or generate eventBridge event
- 90 days retension in CT of CT events (managemnt, data, insight)

- Compilance of Resource ==== aws config
- config rules - custom/predefined, eveluated/trigger or both for any config change
- remediation - custom/managed ssm automation doc - can call lambda
- remediation retries
- monitor resource=> not compilant => trigger evenbridge/ trigger sns  direct too

- FastFile mode works well with large files, use it only when you know your instance store has enough storage space inside to fit whole db as afterwards it is going to store init
it wont support menifest/augmented menifest file format.
it will not combine with fsx for luster, it-FF will take data from s3

- AWS Compute Optimizer is a service that analyzes your AWS resources' configuration and utilization metrics to provide you with rightsizing recommendations. 
- speed matters - use lustre
- enrich, aggregate & transform streaming data ---- apache flink
- Amazon SageMaker Studio- (IDE) for building, training, and deploying ML models on AWS.
- SageMaker Studio: Canvas users can optionally share their models with data scientists using SageMaker Studio, allowing for refinement and deployment.

- Flink distributed processing framework for stream data - streaming ETL - transform,aggre,enrich data
- aggregate streaming data -- flink, data firehose --- for realtime go realtime
- from studio class we can share models in canvas
- tag user profile in sm domain and that tag will be useful in budget based on user particular tag based
access to resource or other service --- can be tracked ===== simple no setting for individual user 
All resources that the user profile creates will have a domain ARN tag and a user profile ARN tag. The domain ARN tag is based on domain ID, while the user profile ARN tag is based on the user profile name.
- domain will organiz - user profile, applications, their associated resources
EFS, authorized users, security/policy/apps/vpc configurations --- in domain
1. user profile -- have personel EFS,& shared resources across domain
2. shared spaces -- shared IDE app, EFS dir by all domain users
- SNS can call lambda function too
- SG --- allow permi, NACL - allow & deny permission for --- IP add
- model should have similar true positive rate across all dem group to check bias and f1 & auc roc
- callback steps => for manual approval steps or running custom workloads. in SM PIPELINE
- ROute table -- route network traffic to resources
- Kendra GenAI index == for q business & bedrock
Data Quality and Insights Report  ==== with data wrangler --- based on that you can do data transformation in dw, like missing, outlier, duplicate, skwed data  --- check it and trasform according
Redshift has dynamic data masking policy to obfuscate sensitive data - obscures sensitive columns at query time - this masking policy can be created based on user,role, column, cell level - partialy /completely redact data or hash it. n join can be applied on those data too without its access
- Materialized View in REDSHIFT
static, need refresh, consume additional storage - view capability, not to go to compute large query again n again
- bias = incorrect assumption of model
- variance = models complexity
- in image-everything stored as vector -- either reckognition or any unstructured data
- Reck - face colleciton(labeld), facematch
- create face-collection - indexface operation
- create user - associated faces
- Searchface, searchfacebyimage, searchuser,searchuserbyimage,startfacesearch
- dynamodb can not be imported by data wrangler
- lf tag will be attached with data catalog when it is infer from data, tag will manage permission for data via lf data catalog for principal
- automatic secret rotation is done by lambda because the new changes secret will be reflected in secret and the db/place where it is used like service - create rotation fun and at last set network access for rot fun
- CW log data stays for 15 months
- In notebook.. it should have permission to put/get object from bucket or decrypt data by kms:Decrypt
- to assign permission on kms you can use kms key arn/particular bucket arn too
- from SM studio you can visualize data quality metrics from monitor
- for large dataset n you dont want realtime then go for batch
- XGBoos : Its robust handling of a variety of data types, relationships, distributions.
The variety of hyperparameters that you can fine-tune
You can use XGBoost for regression, classification (binary and multiclass), and ranking problems.
- Once the file system is successfully mounted, you can upload your dataset to the mounted location, making it accessible for training your models within the SageMaker environment. This approach allows you to leverage the storage capacity and capabilities of FSx ONTAP while working with SageMaker for model development and training.
- Provisioned Throuhput : tp you can purchase to increase amount/rate token processed during inference -- IM
- CloudWatch Lambda Insights to monitor the Lambda function’s memory usage, CPU usage, and invocation times
- SM model Monitoring == realtime, batch, async batch(scheduled)
- Model Customization ; trained model, you have new training data which are useful for adjusting model params again examle: fine tunning, continue pre-training
- for data leakage issue -- use ordered split - for ordered data
- for unordered data - data leakage - use split by key
- A2I store the refined result in s3 after proving human reiew over inference.
- howmany instance to deploy under what condition === step scaling
- for scaling consider - metric & target value (matrics of invocation, actual value of invocation no)
scaling can be done based on custom metric too
- schedule/step scaling = by api/cli,  schedule ==one time / recurrent based on condition
- LR warmup step : training step when lr increase gradually to not have divergence/slow convergence
- green fleet works smoothly for baking periods(set eval period) - check n rollback if issue while
- Document classification is semi-supervised learning technique
- model drift ==== retrain the model, data drift === create new baseline(if updated recently)== MANAGE DRIFT
- You can take actions like retraining models, auditing upstream systems, or fixing quality issues without having to monitor models manually or build additional tooling.
-predictable burst in traffic ==== only then = provisioned concurrency with serverless
- automatic scaling === auto scaling
-aws SAM - serverless application model framework
- lambda will invoke template/containers and run architecture to deploy model on it
- lambda starts with api
- data/model distribution shift/drift can be identify by featrue attribution not bias 
- compute average of objective matrics == all traning jobs previously
then compute median of those averages - -- to match with current to find - at same epoch - stoptraining
- high correlation between features === L2 regularization
- prevent over/under fitting & inc generalization ==== L1 regularization
- fine tunning/incremental training will lead to catastrophic forgetting -- use transfer learning
to solve it.. in that storage of small old data / metadata will be consider in TL so old knowledge]
will be considered
epoch = whoe dataset passed in model
iteration =  single forward & backword pass by batch size(like 20 records in model with for/back pass)
cluster size = GPU*no of Instances
https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html
- to maintain model performance === data should be clean & consistant
so implement data validation rule in monitor to keep data clean N c, 
- in canvas new feature for - Data Validation - check quality of data like
missing, outlier, distribution change, 
- ECS can handle service (long running task), standalone task(batch), schedule task(task run with help of eventbridge)
- event / api logged by ct, ct lake data stored in orc format, 10 yras retension, lake dashboard kind feat
- cost hs anomaly detection too
- interpretability : model's inner working like weight/feature would be understandable
- explainability : in human terms exaplain how model make this prediction output/input consider
- by prompt engin -- we can customize FM model
- fine tunning will change weight of model
- fastfile mode accept only s3 file
- If your dataset is too large=, has many small files that you can't serialize easily, or uses FSx for Lustre 
Its file system scales to hundreds of gigabytes per second (GB/s) of throughput and millions of IOPS, which is ideal when you have many small files.
- larege dataset and many samll files use fsx for luster
- when large dataset and large files more than 50 mb use fastfile mode
- MaxWaitTimeInSeconds = training time + wait time
- changing data every epochs slightly so model will not overfit. - data augmentation like image augm
- data wrangler has dat quality/insight rules too
- share, manage resources access across diff accounts aws RAM from your acc/organisation
- policy can be attached to group of models in registry to manage access to it
- after training model will be registered in sm registry
- LF permissions are augmented on IAM permission policy
- Large scale ETL wont work with data wrangler
- serverless inference integrate with lambda
- precision and recall should be followed when there is imbalanced dataset
- fraud detection has complex relationship in features so xgboost can handle it better
- ImportModel will work with bedrock n other builtin aws algos, to import your own custom model
- howmany instances will be deployed under some condition = step scaling
- schedule policy = time based and recurring based
- standardization - mean = 0, standard deviation = 1 == map -1,0,1
- Use FastFile for cost-efficiency and when you don’t need high-throughput I/O.
- Use FSx for Lustre for high-performance needs.
- large data set + high trhoughput ===== NOt Use ===== FastFile/file mode
- dont mix two input mode with each other for same dataset in same training job
- most latency sensitive app == s3 express one zone can be helpfull - single digit milisec latency
- SM Lineage Tracking  create and store info about steps of ml workflow so reproduce, track model, dataset lineage.
- 3 savings plan : compute, ec2 instance & sm saving plans
- specific instance prediction --- check how feature value is effecting on it = shaply value = per instance explaination
- giving input and checking how target value respond === pdp plot






