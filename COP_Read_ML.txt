https://github.com/FabG/ml-aws-specialty-lab/blob/master/aws-notes
/aws-machine-learning-cheat-sheet-112020.pdf

https://ashanksm.medium.com/aws-certified-machine-learning-specialty-cheat-sheet-e3d1bd6acf7f

https://skillcertpro.com/wp-content/uploads/2020/07/AWS-Machine-Learning-Specialty-Master-Cheat-Sheet.pdf



ML & DL diff - Ml - select model to train, manual feature extraction, DL - select architecture of model, feature extraction is automatic

- data warehouse - apply data analytics, schema on import, database - apply transaction on DATA, data lake - structure & unstructured data(artifact, ml output, analytics output, stream data)

- RedShift - columnar storage and data compression

- S3 two level security, plain text key do encryption and then plain text key will be encrypted by master key and stored in S3 too

- AWS macie - find sensitive information from data in s3 and alert user for that, or bucket because public and about to accessed

- in kinesis - at rest KMS encryption is used

- use kinesis sdk api for faster access of data rather than KPL, KCL

- Kinesis analytics available for SQL and JAVA developers

- in time-series along with trends, seasonality, noise, level = average of time data ===== components

- Glue support format conversion into csv, json, avro, parquet, orc(apache hive optimized row columnar) , xml

- athena execute query in parallel, so its fast

- analytics ability helpful in ML

- EMRFS ==> s3://, HDFS  ==> hdfs://    <=== prefix for file in EMR

- EMRFS has read after write consistency, checked by dynamodb(ddb check metadata and identify whole data is complete)

- Athena and redshift spectrum are query service which use glue data catalog

- Normalisation --- 0-1
Min will map to 0 and max will map to 1, so no - minus values 
Called min max scaler like
Nm =x - min(x)/max(x)-min(x)

- Standardization -- means will be 0, so values will be dispersed around mean, having - minus values, mean n std deviation calculated n used
S = x - mean(x)/std(x)

- Deep learning will use knn or regression for imputation of missing value 

- Deepar can be used for imputing missing value 

- Binning n encoding can be combined 

- Log transform apply on positive values n handle skewed data n outlier 

- SMOTE -- use knn

- Bigram, trigram having unigram n bigram together respectively 

- OSB- set window of 4 n do 4 words sentences n make pair of two words by removing middle words in reputation mode, first word will stay there of sentence (different for each 4 words sentences), add _ like

The_quick, The_brown, The__fox

- Cartesian product means mix two or more features and form one feature - interaction between features - attach features with _

- ReLu used in hidden layers

- tanH prefred over sigmoid, tanH suffer from vanish gradient

- regularization will introduce additional error term on loss function so weight and bias will be updated more differently 

- Regularization will increase bias and improve variance

- (Ridge - square of slope, Lasso - slope) + alpha <===== new error term

- in cnn relu AF is used  ---- cnn work good on image size 32X32

- FP is type 1 error, FN is type 2 error

- accuracy = (TP+TN)/(TP+TN+FP+FN)

- error rate = (FP+FN)/(TP+TN+FP+FN)

- RMSE - unit matches the unit of output - means it do square of output and reverse it with root so preferred over MSE

- Decision tree are supervised- minimize entropy which provide optimum split

- EMR supports GPU and Apache MXNet instances

- XGBoost can be used for fraud detection algorithm

- in seq2seq => encoder will have LSTM or GRU , decoder will have RNN & LSTM, it work based on seq2seq

- DeepAR can work well for cold start problem and data having seasonality factor, inference on cpu instance only

- CBOW - predict one word from given sentence, skip-gram - predict sentence from given word

- Amazon Forecast needs user to move s3 data to amazon forecast, data in forecast - AF will give data encryption by customer managed key, and IAM access to data for security

- AWS Greengrass enable edge devices to communicate to aws cloud - edge devices can communicate with each other too by IG

- AWS Greengrass core handle communication between device and cloud, & execution of Lambda

- endpoints are encrypted with KMS

- cloudwatch can be used to detect anomalies

- cloudtrail can work in many region
cloudtrail log data are encrypted by server side encryption technique in s3

##################################################################################################################

- aws storage gateway - use to store backup of on-premise data in aws s3 by using aws storage gateway configuration - NFS(network file storage) distributed file system to store data in aws

- s3 intelligent tiering has small monthly monitoring and tiering fees
- s3 outpost storage will have 48TB to 96Tb and 100 buckets storage in outpost

- increase n-gram size if model underfit vice-versa in overfitting

- True Positive Rate === TP  /  TP + FN

- True Negative Rate === TN  /  TN + FP

- False Positive Rate === FP  /  FP  + TN

- False Negative Rate === FN  /  FN  + TP

- remember TPR & TNR  if FPR or FNR asked just replace T=>F, F=>T

- you can launch EC2 instance in deep learning AMI

- Amazon SageMaker can train from Amazon Elastic File System (EFS) with FSx, lustre along with s3

- XGBoost requires numeric features. It automatically handles numeric features of different scales and categorical numeric features.

- if target leakage is there(or just for normal data) don't do feature engineering on all data. First do train-test split and apply feature engineering on train data only
	- Using future information in training
	- Applying transformations blindly- on all train, test data 
	- Randomly splitting data - for timeseries data

- client request cached by endpoint - one endpoint can have multiple instances, multiple production variants

- to handle client request python sdk, boto3, lambda, (api gateway + lambda) can be useful in notebook

- SageMakerVariantInvocationsPerInstance = average no of request per minute per instance = max req per sec * safety factor * 60
  beyond above value instance scaling will be applied

- in production variant feature variant weight & target variant are parameter to send % traffic and request to particular variant

- each variant need separate instance and separate serving container

- in multi-modal - same container will be used to host diff models(using same algo) with same instance

- multi-modal with multi-container - same endpoint, same instance but diff containers having diff models like containers are in inference pipeline

- AWS IoT Greengrass, you can run Lambda functions, docker containers, execute predictions on machine learning models in your edge device even when not connected to the internet.

- neo compiled executable model artifact will be placed in s3
- greengrass will download that executable & lambda runtime and host on your edge device computer and generate inference


- endpoint configurations are read-only. So no update or modification apply.
- Create a new endpoint configuration to use the new instance type and apply it to the endpoint. SageMaker implements this change with no downtime. The existing endpoint configurations are read-only. So, you would need to create a new endpoint configuration

- Spot instances are currently supported for model training and hyperparameter tuning jobs.

- Single-model endpoint provides complete isolation as each instance has one serving container with a single model artifact. 
- A multi-model endpoint uses a shared serving container to host all the models in the instance. 
- The multi-container endpoint has different containers and models in the same instance - it can be used for inference pipeline

- With a multi-model endpoint, the engineer can release new models without any changes to the endpoint. The engineer needs to store the model artifacts to the S3 location (specified when setting up a multi-model endpoint). The serving container will automatically download the new model artifacts from S3 when an inference request is received for the new model

- Data Wrangler is a popular topic in the ML Specialty Exam. DW is more under SM canvas platform, so we will need domain for it
- DW show high warning about duplicate values in dataset, data leakage, anomalous data 
- SageMaker Data Wrangler helps reduce the time and effort spent on preparing data for machine learning, making it easier to clean, explore, and transform data at scale

- Amazon SageMaker AutoPilot --- provide all features like Data Analysis and Preprocessing, Model Selection, Hyperparameter Optimization, Model Training and Evaluation, Model Deployment like automatic endpoint generation

- SM debugger can identify overfitting of model too

- If you write a custom algorithm, AWS recommends that you use SageMaker Distributed Data Parallel
library and SageMaker Distributed Model Parallel library --- for distributed training
- You can approach distributed training in two ways: Data Parallel and Model Parallel 
- break-up data/model to run on distributed cpus or gpus

- prediction score === low score on negative samples, high score on positive samples, like (0.1, 0.8) but if 0.4 which is in middle so model can not identify it is either it is pos/neg this score can be low confidence score to check by A2I

- bias can be checked while pretraining, training, after deployment of model by using clarify

- SageMaker Experiments automatically tracks the inputs, parameters, configurations, and results of your iteration as trials. You can compare active experiments with past experiments to identify opportunities for further incremental improvements

- Clarify can help explain how models make predictions. It uses a model-agnostic feature-attribution approach to explain how a model arrived at a particular prediction

- SageMaker Studio share notebook in a few different ways to user by using AWS Single Sign-On (SSO). 

- if training take too much - AWS recommends that you try a larger instance before trying to increase the number of instances.

- identity based policy will be directly attached to user so principal component will not be specified in policy.
- groups are not supported as principal in any policy, still group can have policy
- policy are inline & managed. Inline attach to user/resource & can't be re-usable, user/resource destroy policy will be destroy
- managed policy has versioning feature to revert-back if needed
- arn - uniquely identify resource/principal in aws
- no-action, not-resource, not-principal are tag can be used in policy to define
- request context in policy will help with requestedRegion, principaltag, secureTransport, sourceIP, sourceVPce, sourceVPC, currentTime --- global environment data

- two type policy 
1. Role based policy - when a new resource will be aaded policy need to updated <= drawback
2. attribute(tag) based policy - tag can be attached to user, role, resource 

implicit - suggested, explicit - pressured


application access to aws resource
- access key are long term credential 
- IAM roles are used by ec2 instance to create temporary credential to access aws service/resource - this credential valid for few hours
- STS - security token service is used to create temporary credential
- app can access service/resource by IAM role
- on-premise app can access service/resource by access key that will ask for IAM role to aws to get access
- resource based policy can be useful for cross account resource 
- if service not support resource based policy then STS can be useful to assume role and get temporary credential
- Group is not considered identity, and you cannot grant access to a group in a resource-based policy. With Resource-based policy, you can grant access to users, roles and other accounts
- Credentials must not be shared. Each employee must have their own user and credentials. As a best practice, MFA must be enabled for administrative actions.  
- AWS IAM Federation - user can sign in aws with their existing corporate credential
- AWS cognito - users are mapped to IAM role and they get temporary credential to access services by that role
- for cross account access of resource aws organization is best choice to create to manage users account
- with AWS Organizations, you centrally enforce the policies using Service Control Policy. strict access/not access
- tag(attribute) based policy is least privilege policy 
- check Chandra lingam section 12 for more

- PAC can work on numeric data only, data should be normalized first
- work on large dimensional data very well
- RCF can work on time series data to impute missing values and forecasting
- RCF assign anomaly score, low score means normal data points, high score means anomaly data points

- ORC - columnar
- Avro - row format data
- parquet, orc, avro are binary format
- Glue ETL job script run on EMR
- DF convert data in parquet/orc - in built conversion &  backup into s3(original one)

- CRR - cross region replication - replicate bucket in another region - automatic & continuous replication

- Durability – S3 replicates data across multiple availability zones and multiple devices in each availability zone. 
- Cross-Region Replication is used for disaster recovery by keeping a copy of your bucket in another region. 
- Versioning protects from accidental and malicious deletes. 
- Server-Side encryption is used for storing the data encrypted at rest.

AWS Glue(ETL)

transform - merge, data type change, missing/incorrect value, organize data
- glue ETL job run on serverless apache spark environtment

Glue ETL DataFrame 
- use python / R
- store in computer memory
- it is for small/medium dataset
- single machine use
- data will be moved where code is - on server
apache spark dataframe
- distributed across cluster of server
- ETL code is moved to server where data is
- for large dataset, enable for in memory caching
- distributed structure
Glue DynamicFrame
- same as spark dataframe
- glue catalog integration
- easy integration to aws data source & destin
- can handle diff datatypes
handling datatype inconsistency[string, float]
- cast to single type
- make separate column for each
- have transformation context : persist state for operation if specified,  if not => state can't be tracked    <== for bookmark

Glue Job Types
- Ray - new framework for ML
- python shell job - ETL that not require apache spark
- streaming etl - for streaming data from kinesis and kafka
- spark job - run pn spark cluster for batch processing
- check 205 lecture in chandra lingam course

-Stochastic adjust weight on each example, other are mini batch and batch, mini batch is very popular in deep learning as it work in parallel

- Weights are adjusted based on error observed in a mini-batch. The training set is divided into mini-batches and when you increase the number of examples in a mini-batch, you have fewer mini-batches. And this would result in less-frequent weight adjustment
- The sigmoid activation function is used for binary classifiers as it outputs a value between 0 and 1. This output is used as the positive class probability
- For regression, if the model predicts any arbitrary value as output, you should not use an activation function. However, if regression predicts values in 0 to 1 range, then you can use a sigmoid activation

conversion
GDFrame ===> spark dataframe   @@@@@by toDF
GDFrame <=== spark dataframe   @@@@@by fromDF

Glue ETL Boomark
- track processed data
- bookmark = persistant state between job run
- for s3 = last modified time of object
- for jdbc = primary key of table as bookmark key

- so if ETL job is skipping data disable bookmark as it start processing data from last bookmark process(new data not processed).

- framework like sklearn, pytorch etc you can create your algo(write script) or use pre-built algo for those framework and train model. Local mode training & hosting are available by this framework container
- custome container images are hard to create
- SM_HOSTS, SM_CURRENT_HOSTS are related to containers for training

- instance store - if reboot happen on same host then data persist, data lost( if hardware fail, instance stop/terminate) in pay instance
- EBS volumes are outside of host computer of ec2 instance, separate pay than instance, long term use, snapshot backup, AMI can be created from snapshot to launch EC2 instance, it is AZ based , automatic replicate in AZ for disaster

Snapshot - incremental backup
- asynch, work in background
- flush cache first for consistency

EFS
- share between server
- private space for all user in organization 
- widely used by on-premise app
- EFS = Linux ec2
- FSx for windows = window ec2
- FSx for lustre = high performance computing for Linux ec2 with s3 use as file share(put data in s3) look like storage gateway
- standard, infrequent access storage tiers, life cycle policy like s3

- EBS volume support KMS & CMK encryption

- Cassandra, dynamodb, documentdb are NoSQL db

- failover - when primary db instance crash standby becomes primary it is called - DNS is used for assigning primary to standby

- aurora serverless - decouple storage from data processing, request come proxy fleet will start connection  for processing, aurora warm pool to make process faster


https://d1.awsstatic.com/whitepapers/augmented-ai-the-power-of-human-and-machine.pdf
https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/welcome.html
https://aws.amazon.com/sagemaker/faqs/?ml=sec&sec=prep
https://aws.amazon.com/rekognition/faqs/?ml=sec&sec=prep
https://aws.amazon.com/transcribe/faqs/?ml=sec&sec=prep
https://aws.amazon.com/comprehend/faqs/?ml=sec&sec=prep
https://aws.amazon.com/augmented-ai/faqs/?ml=sec&sec=prep

SageMaker stores code in ML storage volumes, secured by security groups and optionally encrypted at rest.

Bias can be measured before training (as part of data preparation), after training (using Amazon SageMaker Experiments), and during inference for a deployed model (with Amazon SageMaker Model Monitor) by clarify = 20 bias

Amazon SageMaker Model Cards => model information = model documentation , SageMaker Model Cards auto-populates training details
You can define minimum permissions in minutes with SageMaker Role Manager.
SageMaker Role Manager will then generate the IAM policy automatically. 

- Lineage ===> code lineage, data lineage, model lineage

code & data lineage has been handle by versioning already
model lineage:
all components in creation of model
many data preprocessing, training and post processing steps are involved

studio
feature store
lineage tracking - steps of machine learning workflow
pipelines
experiments : track, compare, evaluate models
ECR
s3
and more

Amazon DevOps Guru
- improve app availability by using ml powered cloud operation

amazon helthlake
- store healthcare unstructured data also apply dimensionality reduction

amazon lookout metrics
- can send alert if found anomaly in data using metrics

monitron
- install monitron sensors and gateway by app


- for linear models : feature and target should linear relationship, but features should not have multi-colinearlity(means they should not have linear relationship with each other - otherwise model can not understand relationship of feature with target proprly)

- gradient decent algo - optimization strategy
move towards right set of parameters
- learning rate 
step size - big or small step to update parameter

The ROC AUC score is a single number that summarizes how well a classifier can distinguish between positive and negative classes.

- in classification DT - gini index/entropy will be used as loss function to identify variable the minimize loss

entropy = measure of randomness in dataset
information gain = difference between parent and child

- SVM - statical approach
- logistic regression(classification) - probability based approach

- support vector : data points which are near to hyperplane
maximum distance between support vector and hyperplane is good  in svm

bagging classifier : bootstrap+aggregation

- adaboost : weightage of rows will be used to update model
- gradient boost : check residual of model and perform fit on residual - add more nodes until residual becomes small

- inertia in K-means: distance between each data point and centroid.
good model inertia low, k low. But actually k increase, inertia decrease
The silhouette in k-means : measures the similarity of a data point within its cluster (cohesion) compared to other clusters (separation).

- DBScan(density based can do anomaly point clusters too) & Hierarchial(bottom to up tree) are clustering technique

- in regression model output layer don't have activation function
- for each epochs parameters will be changed , epoch : whole forward and backward pass of NN with all data
- batch size : no of data rows used to complete one forward and backward pass
- iteration : one forward pass + one backward pass

100 rows
5 batch size
100/5 = 20 iteration for each epoch

epoch can be any you choose 
total iteration = epoch * iteration

optimizer HPs
LR : speed at which parameters are updated
batch size : choose that max value which can fit in your memory
epoch : can be any early stopping will help with problem

model HPs
no of hidden layers
activation function : start with Relu
Relu solve vanish gradient problem
dead neuron => leaky Relu
Relu used in Hidden layer only
sigmoid, tanH introduce vanish g p

- FF NN features are processed independently - so no relation track so RNN required

transfer learning in nlp
1. pretraining : large data, dummy task- predict next word  say this lang modeling
2. domain adaption:  fit on domain data
3. fine tunning : add classification layer, task on hand


ordinal data ===> LabelEncoding(0 & n-1 conversion of val)
nominal data ===> One Hot encoder

employee attrition === leaving job
categorical val == countplot
numeric val == bar plot

pca useful for feature extraction - apply scaling first before pca --- important


LDA - linear discriminate analysis ==== supervised learning(label is there) to reduce dimesion. classes should be as much possible as separate from each other.

s3 glacier deep archieve - digital preservation of data long term
s3 glacier - s3 managed data
- You can use Amazon S3 Inventory to help manage your storage. For example, you can use it to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs. not help with api request to your bucket
'
aws batch
- batch computing jobs as well as gpu based training job can be run on batch- scalable, containerized,
batch has priority based job scheduling, gpu scheduling, work with different workflow engine too
support tightly coupled workflow too, like multiple EC2 instances are required then they can be used here too. also support distributed training
- you can specify your reserved instances, spot instance(ec2) ect to work with batch to save cost 

aws step
- serverless orchestration engine, cor-ordinate micro-service and distributed apps\\
- 2 types
1 standard : one run workflow, execution extends upto 1 year - long running, auditing n visual debugging provided
2 express : one or more than one time run workflow, extends execution upto 5 mins like streaming data processing or IoT stream ingestion workflow <=== high event rate workflow
- provide parallel processing
- state transition == step execution


- for cost management(operational expence)  - instance(resource) scaling is imp, not load balancing
- load balancing is for optimizing performance
- UpdateEndpointWeightsAndCapacities operation to modify the DesiredWeightsAndCapacity for the new model version - This approach enables precise control over the traffic distribution between different model versions

- AMT uses the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that creates a model that performs the best, as measured by a metric that you choose.

- training job configurations : algorithm, data sources, output destinations, and initial hyperparameter values
- SageMaker notebook instances offer the flexibility to modify the instance type directly, after halting it

- K-means clustering is more commonly used for grouping data points based on similarity, rather than specifically targeting age groups for product alignment.

- Configuring security groups is more suitable for managing network access to AWS resources, such as EC2 instances, rather than regulating access to AWS Glue Data Catalog resources. Security groups control inbound and outbound traffic at the instance level, not resource-level access control.

- IAM resource-based policies - allows for granular control over access to these resources like  AWS Glue Data Catalog resources. 

- security groups is more suitable for managing network access to AWS resources, such as EC2 instances

-  K-means clustering is typically used for unsupervised learning tasks to identify natural groupings in data, not focusing on particular age group or kind of grouping/partitioning(binning can be useful for age mainly)

- Amazon Redshift is a data warehousing service optimized for analytics workloads, not for storing vast amounts of raw training data. Redshift data are once processed data for specific task.
- Redshift organizes data in a columnar format. Each column contains data of a specific type, such as integers, text, or dates.

- if age distribution fall in small group like elder people above 65 age will have missing data then use mean/median value to imputation

- In machine learning, optimization is the process of improving a model's accuracy and effectiveness

-Scaling out === add more instances and scaling in === reduce the number of instances
- augmentation === increment

- Amazon Kinesis Firehose is more suited for loading data into data lakes or data stores, rather than real-time data analysis. While it can deliver data to destinations like Amazon S3 or Redshift,
- KDS=== collect, process, and analyze large streams of data in real time, enabling the rapid ingestion and analysis

- Pearson r correlation can be calculated based on scattered plot points. But directly can't be visible from scattered plot

- scatter plot shows positive/negative/no co-relation between variable 

- CloudWatch Logs collect and store log files generated by AWS resources, applications, and services, enabling centralized log management and analysis. CloudTrail logs, on the other hand, record API activity and management events within an AWS account for auditing and compliance purposes.

- Utilize AWS CloudTrail for logging SageMaker API activities to S3, integrate a custom metric in CloudWatch for overfitting detection, and set up CloudWatch alarms with SNS notifications

- A heuristic approach in artificial intelligence (AI) is a problem-solving technique that uses shortcuts to find approximate solutions in a faster and more efficient way than traditional methods.

- You can use ACLs with services like: Amazon S3, AWS WAF, Amazon VPC, and MemoryDB. In AWS, an access control list (ACL) is a service policy that controls which principals can access a resource. ACLs are similar to resource-based policies, but they don't use the JSON policy document format.

imperative  == very important, 
substantial == considerably important(more), 
arbitrary == random choice

- IAM role is used for granting access to notebook instance

- One-hot encoding could result in the loss of ordinal information, not used for ordinal data

- implementing auto-scaling == sagemakerfullaccesspolicy required = as fluctuating workload so add/remove instance require full access

AWS Glue components
AWS Glue console, AWS Glue Data Catalog, AWS Glue crawlers and classifiers, AWS Glue ETL operations, Streaming ETL in AWS Glue, AWS Glue jobs system, Visual ETL components, ETL job menu, Visual ETL panels, Job canvas, Resource panel

- AWS Glue serverless so no cluster.
- data transfer charges associated with routing traffic over the internet, with vpc endpoint it wont

- if whole data cannot fit into memory, partition data into files

- AWS Glue is mainly ETL service - so cannot be used for feature engineering/extracting features or to run the script to apply diff logic on massive data like extracting geo location based on pixel.

- vpc endpoint on s3 make data transfer fast so cost become less for transfer
- gpu instance use make training fast not effecting on memory loading and pre-processing speed, partitioning will be helpful

- Resource-based policies == IP addresses restriction
- identity based policies not provide ip address thing

- CloudWatch Alerts are used to monitor metrics and trigger actions based on predefined thresholds
- CloudWatch Events are = state changes in AWS resources. 
- CloudTrail Logs provide a record of API calls made on an AWS account.

- SageMaker hosting specifying two or more instances automatically distributes across multiple availability zones. 

- S3 analytics focuses on monitoring and optimizing S3 storage usage, not accessing data

- high-performance compute-intensive model like genome mapping === gpu will be used

- Add new domain-specific features and more feature Cartesian products, and change the types of feature processing used (e.g., increasing n-grams size) , cartesian and n gram may introduce noice

- security groups ==> ec2 instance and network resource security more commonly not for aws resources
- NACLs are associated with subnets in a VPC and control inbound and outbound traffic at the subnet level. - network traffic to and from resources within the VPC not for security

- sagemaker.sklearn.estimator.Sklearn available, SageMaker's pre-built scikit-learn container
- sm python sdk have capability to run distributed training/inference by tensorflow code
- VPC Flow Logs capture information about the IP traffic going to and from network interfaces in a VPC

- InitialInstanceCount to two or more within the production variant of an endpoint configuration in Amazon SageMaker
- instance  === money 

- in UpdateEndpoint === set variantWeight parameter for variant weight on diff production variant for phased rolled out

- pr auc ==> evaluate trade of between p & r
- roc auc ==> evaluate between positive classes - correctly predicted, wrongly predicted, trade of between tpr, fpr, can specify that you can change threshold - 

- Cloudwatch event can capture state change of sm resources like training job
https://gmoein.github.io/files/Amazon%20SageMaker.pdf ===========  important for sagemaker --- must read

Date :  04/06/2024

==> Build your own alexa

-> input = speech , output = speech
-> amazon transcribe -> amazon lex automated designer -> amazon polly

==> Build your own universal translator

-> input = speech , output = speech
-> amazon transcribe -> amazon translate -> amazon polly

==> Build your own celebrity detector

-> input deeplence device and associate with recognition and fire alarm is show celebrity specified
-> aws deeplence -> amazon recognition

==> Build system to check people calling you are happy or sad or sentiment any

-> input = speech, output = sentiment with score
-> amazon transcribe -> amazon comprehend


@@@@@@@@@@@@@@@@@@@ Algo Content Type @@@@@@@@@@@@@@@@

ContentType============>Algorithm
application/x-image============>Object Detection Algorithm, Semantic Segmentation
application/x-recordio============>Object Detection Algorithm

application/x-recordio-protobuf============>Factorization Machines, K-Means, k-NN, LDA, Linear Learner, NTM, PCA, RCF, Sequence-to-Sequence

application/jsonlines============>BlazingText, DeepAR

image/jpeg============>Object Detection Algorithm, Semantic Segmentation

image/png============>Object Detection Algorithm, Semantic Segmentation

text/csv============>IP Insights, K-Means, k-NN, Latent Dirichlet Allocation, Linear Learner, NTM, PCA, RCF, XGBoost

text/libsvm============>XGBoost

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

@@@@@@@@@@@@ model implementing @@@@@@@@@@@2

accuracy => accuracy on training data
val_acc => accuracy on unseen data <- to check overfitting check this it should be high for no overfitting

- if overfitting do regularization based on algo we will choose
- if cnn then choose dropout layer to overcome overfitting

- high batch size will stuck model in local minima not good solution
- high learning rate is also same overshoot the correct solution
- small batch size required small learning rate

@@@@@@@@@@@@@@@@@@@@@ important points to remember @@@@@@@@@@@@@@@@@@@@@2

- mostly web application use port 8080

- standard http port is 80

- model artifact compressed in tar.gz file format

- if binary classification problem and threshold settings is there in option it can be most approx.  answer for FN/FP related issue

- Encryption needs to be implemented at the application layer (such as SSL/TLS) or by using AWS services designed for encryption

- Boosting- model training technique - increasing the accuracy by giving more weight to difficult to predict observations in sequential model training.

- if skewed(feature) dataset, and you need to uniform it by binning-apply quantile binning to convert it into bins having equal no of observation

- Training for more epochs often leads to overfitting, as the model starts to memorize rather than generalize from the training data.

- silhouette score if >1 data point very far from nearest cluster, 0 on one cluster or on boundary of two cluster, -1 assign in wrong cluster

- EMRFS -Hadoop file system API, allowing direct interaction between EMR-hosted applications like MapReduce & S3. By using the `s3://` file prefix.

- S3A (Amazon S3A File System) Hadoop-compatible interface for accessing data in S3-accessing public data sets without requiring AWS credentials.

- Amazon Comprehend use LDA feature - for topic modeling

- if NN has more layers than use ReLU as it solve vanishing gradient problem because of more layers for convergence

- if accuracy is not as required you can change activation function 

- if convergence issue than it is indicating problem with gradient loss - use Relu to solve it - specially if Network is complex already

- if outliers are in dataset we can use median values for imputation

- labeling by ground truth incorporates both precision and speed as it provide automatic labeling by self learning algo and human reviewing

- Reckognition in aws if custom logo or character you need to identify, for train reckognition with labeled dataset(even a small one). final model will be created and trained and you will get api for that model to use for your specific use case - you need labeled dataset for custom reckognition

- blazing text can handle n-gram technique so it break word in subword and handle OOV by aggregating subword vector so good performance on morphologically rich language.

- access to s3 in sagemaker handled by IAM roles & policy not by bucket access control list(ACL) - resource based access control 

- To protect your data in transit within the AWS Cloud, Amazon Redshift uses hardware accelerated SSL to communicate with S3 or DynamoDB for COPY, UNLOAD, backup, and restore operations.

-Trusted Advisor checks security groups for rules that allow unrestricted access to a resource. Unrestricted access increases opportunities for malicious activity, such as hacking, denial-of-service attacks, or loss of data. 

- while transferring data from firehose to destination, data transformation will be happen in lambda function

- The lambda timeout value default is 3 seconds - very less to do some transformation of data sometimes.

- Firehose supports Amazon S3 server-side encryption with AWS Key Management Service (SSE-KMS)

- AWS auto scaling lets you scale other resources &services like ECS, dynamodb tables, aurora replicas etc just like EC2 instance scaling(add more as need)

- The AWS Glue Data Catalog is an Apache Hive metastore-compatible catalog

- to attach kinesis video stream to sagemaker direct amazon kinesis video stream inference template(KIT) is used.

- Glue classifier send certainty string either 1.0=> all schema are 100% sure or Unknown or 0.0 => all schema are not classified from given data

- Reckognition can fail to identify face if it is trained on single image per person - it need to train on multiple image of same person with different aspects like beard, glasses etc

- random forest(Supervised algo-multiple dT classification & regression) prevent overfitting just like bagging technique

- Entropy uses the probability of a certain outcome in order to make a decision on how the node should branch. Unlike the Gini index, it is more mathematical intensive due to the logarithmic function used in calculating it.

- AWS Security Token Service (AWS STS) is a web service that enables you to request temporary credentials for use in your code, CLI, or third-party tools

- Access Advisor to check when service or resource last accessed and by whom

- AWS Systems Manager Parameter Store - to store parameters like credential, licence key or configuration parameters

- AWS Secrets Manager - helps you manage, retrieve, and rotate database credentials, application credentials, OAuth tokens, API keys, and other secrets throughout their lifecycles. - built-in integration for MySQL, PostgreSQL, and Amazon Aurora on Amazon RDS

- Hinge loss is related to support vector classifier, and it not specify probability - Hinge loss is also known as max-margin loss. 

- Heuristic approach is problem-solving methods(mathematical) that are based on practical experience and knowledge, when you are not applying ML approach

- recorIO => 4 bytes representation

- the macro average F1-measure is used to evaluate the predictive success of a multiclass classifier. 

- model quantization reduce model size and inference time for edge device with little loss of accuracy.

- for edge device cloud based inference will introduce latency

- knowledge distillation is creating new model having smaller size, but need to train it and optimize for edge device

- image down-sampling will risk loosing smaller details

- logarithmic, robust standardization transformation will remove effect of outliers

- CloudWatch will show matrices at 1 minute frequency 

- comprehend can work on different languages like Arabic, Chinese (simplified), Chinese (traditional), English, French, German, Hindi, Italian, Japanese, Korean, Portuguese, and Spanish

- review residual for regression problem

- residual = true target - predicted target

- resource based policy are only inline, identity based policy are inline and managed 

- resource level permission is associated with ARN

- AWS organisation service control policy will limit permission of root user

- connect to SM api/runtime by interface endpoint only(it is Elastic Network Interface - powered by aws PrivateLink) from VPC - you don't need any gateway, direct connect, public internet, VPN connection for this. it assign private IP address to your VPC for accessing directly. connect aws service and VPC

- Unload command is used to write result of query from redshift to some textfiles

- Kinesis data stream application = consumer of streaming data

- Lex is useful for creating conversational interface by using text and voice

- Ip insight generate the score for anomalous pattern of given event/observation.

- tanH activation function is used mainly for classification between two classes, return  : -1, 1

- image classification algo is multi-label classification algorithm

- for image classification/other algo If want to use GPU, include CUDA toolkit in container, not import NVIDIA driver in docker image, your container should  be NVIDIA-docker compatible 

- aws batch is work with EC2/spot/fargate/farget spot instance ans assign compute resources

- AWS glue generate python/scala code automatically - it is visual/code based interface for etl job

- validation data set associated with development state of model. evaluate model performance at development stage

- in framework developer can do more customization like custom training code can be created/use diff validation technique like k-fold... like TF, MXN kind of framework, while in built in algorithm it is not possible

- drop feature if it has LOW variance(how the value of feature spread around means) - if it is so nearer from each-other, model will not learn pattern well

- redshift spectrum will query data from directly S3 without loading data into redshift, the spectrum layer will do all processing and aws provide all resources for that so like this multiple redshift cluster can access s3 data concurrently-it is fast too because of parallelism

- for KNN feature_dim- no of features, k-no of neighbour, predictor_type, sample_size are required parameters

- For kinesis firehose delivery stream take 2000 transaction, 5000 records or 5MB data - by PutRecordBatch or PutRecord api operation

- CloudWatch monitor/log/event work and help you to monitor aws services, along with cloudtrail log(which log api calls)
  monitor => monitor and set alarm
  log => you can monitor, store, access log from ce2, cloudtrail and aws services
  event => deliver the stream of system events show aws resource changes

- Amazon Inspector : security assessment service

- AWS Config : manage aws resource inventory, configuration history, configuration changes notification useful for security and governance. How resource was configured at given point of time.

- Lambda will not handle ETL

- Data Pipeline will do data processing and data movement by schedule, it has BUIL-In capability of moving data between s3 and RDS, running query on s3 log data

- Specify all containers and order of them to run on related EC2 instances in inference pipeline as http request, for normal only one PrimaryContainer has to be specified

- Inference Pipeline is immutable, but UpdateEndpoint will help to deploy new endpoint to update IP for experiment

- Direct data stream can not be captured by kinesis analytics, it takes data from kinesis data stream or Amazon MKS(Kafka)'

- Glue Crawler can accept exclude and include pattern folder name to crawl through

- when SM training start by api SM will replicate entire data set on ML compute instance, to replicate subset of data on each compute instance s3datadistributiontype = shardedbys3key

- SM debugger will track value of parameters if they are getting too large or small while training

- your docker container image path will be specified in CreateTrainingJob api call

- levenstein distance is Minimum number of single character edit in one word to convert it into another word. Distance between two words

- for model do 1. assessment(at least once in 6month), 2. retrain(after checking if performance below threshold), 3. rebuild(performance not getting better)

- HP tuned against validation DS as it helps in learning process, parameters are derived from training like weight and bias.

- No inter-node communication for batch processing(getting inference)

- everything is encrypted 'at rest' and 'in transit'

- command-control communication in between service control panel&training job instances, communication between nodes in distributed training & processing job. <= NOT  Encrypted

- if MEDIAN value of objective metrics of current running job is bad than previous one at same epoch than stop training

- data from /SageMaker folder will persist(stay) between start-stop of notebook instance session.

- hold-out set is 20-30% of training data - to evaluate model while training and getting HP

- model.tar.gz => model_algo-1 = serialized Apache MXNet object

- auc-roc score will predict higher score for positive example rather than negative example

- if you want to identify cyclic pattern based on date feature, convert it into hd, dw, wm, wy, dm, my place them on circle with x, y coordinate with sin, cos transformation.

- one-hot encoding useful for categorical data transformation, label encoding is useful converting label into incremental integer format.

- bivariate - 2 attributes, multi-variate- more than 1 attribute anaysed, check their pattern, distribution, relation with each other.

- without SM - if network isolation then container can not perform inbound/outbound connection to even to amazon service like s3.
- with SM - with help of execution role download and upload operation is possible in s3
- in distributed training - in/out bound traffic are enabled for related training/ inference containers

- Amazon personnelize will create custom model api by using your data and create model train, deploy and create api for it for your recommandation system, just give data you will have api for recommandation, it identify features and train model by itself and create custom model api.

- LDA is a BOW model, in it specify the probability of topic distribution over the word - its generative probability model, not discriminative(logical) model(which specify how output map to input), so it will not give semantic meaning of word

- if Glue is available in given region  than glue data catalog will be useful for querying data from s3, or athena has its own data catalog to access for querying data from s3 if glue is not available in given region.

- do categorical encoding if it is ordinal - according to order like size s,m,l,xl etc
- categorical encoding === assign numerical value to category, 1,2,3...
- label encoding === assign numeric label to category like "1","2","3" or it can be 1,2,3
- target encoding === calculate numeric val -- based on relationship between categorical feature and target value, assign that to feature
- VPC can be enabled for training/inference by specifying VpcConfig params in createTrainingJob, createHyperparametersTunningJob, CreateModel api call

- Network Isolation can be enabled for training/inference by specifying EnableNetworkIsolation params in createTrainingJob, createHyperparametersTunningJob, CreateModel api call

- in model from Aws MarketPlace network isolation and VPC required

- chainer & reinforcement learning not support network Isolation

- KPL library simplify creation of producer application development
  for large complicated data writing it manage all logic, batching, threading, de-aggregation - so give high performance
  on customer side if KCL integrated the KPL integrate without much effort, if KCL not integrated(api-getrecord  is used) even though KPL can be used to get per user record
  KPL emits throughput, error, other metrics to cloudwatch
  Asynchronous architecture : it create batches and not expect the receiver should be ready all the time to receive, it create intermediary object that receive the data and continue getting new record. whenever requester is ready to receive it will same behaviour as asynch client in aws SDK 
  RecordMaxBufferedTime is param for specify delay that app can handle, if app cant handle this delay use aws SDK

- EFS is for Linux(and for heterogeneous app) not require any management, FSx is for windows - require management - both can handle multiple LAMBDA

- if you want to use EMR spark for your SM - you can use SM spark library/livy/sparkmagic - where EMR is there already

- oRC -tabular data(row column formate), parquet - columnar text data storage

- test million of sample rapidly called throughput if need to improve IOPS than change throughput, throughput is associate with EFS
  two throughput mode associate with EFS 1. Busrting throughput mode  2. Provisioned throughput mode
  1. bursting TP mode :  if IO limit reach 100% then increase storage size automatically
  2. Provisioned TP mode : adjust provisioned throughputmode setting to expand max IO limit

- max I/o performance aggregate throughput which is related to multiple connection to EFS system , it can introduce latency for file operation

- AWS Client VPN useful for getting secure connection between client device to aws services/resource over public internet

- Establishing a NAT Gateway enables outbound internet connectivity for resources within private subnets, without restricting public internet access. Contrary to the objective of securing SageMaker notebooks by mitigating public internet exposure, a NAT Gateway permits instances in private subnets to initiate connections to external services, while preventing any inbound attempts, thus maintaining a balance between connectivity and security. NAT Gateway will not do any data encryption.

- AWS PrivateLink use only aws private network only by creating VPC endpoints, not public internet

- SM support FastFile mode as input mode too, same code as file mode no code changes, it directly access data from s3 now so it is very fast. if data downloaded local or streaming from s3 it can work with in both, no additional charges, convienent access of local file and performance of s3 data access both feature provided by FastFile mode. It use EBS storage so cost associated with it.

- if normal file mode is used data will be downloaded in encrypted format in amazon EBS volume whole than can be accessible.

- EnableSageMakerMetricsTimeSeries  associated with monitoring - data for cloudwatch

- choosing KNN, Linear regression for imputing missing value - if features have linear relationship specified than we can use LR, otherwise most probably KNN will be better approach, KNN can handle complex relationship if dataset have.

- Lex can handle speech-to-text and conversational logic

- Shuffling will avoid bias and improve generalization,However, in time-series or sequential learning tasks, preserving the order of data is important.

- in blazingtext skip-gram technique extend to n-gram technique too which break large word into subword and create vector of it and aggregate those sub-word vector for handling OOV, morphologically rich language issue

- by default SM role attach to sm notebook give access to those bucket which has 'sagemaker' word inside their name. 

- skip gram can understand sematic relationship between words 

- ssl/TSL is data encryption technique for in transit.Encryption needs to be implemented at the application layer (such as SSL/TLS) or by using AWS services designed for encryption. Not on network connection rules.

- skewness can be handled by quantile binning as all bin will have same no of data points in it.

- Quick Sight has outlier/anomaly detection, forecasting feature, impute missing values, auto narratives to create graph kind of built in ML features

- 8080 port is often used for web applications, including custom inference containers in SageMaker.

-  big batch_size will lead to bad generalization as it make GD path smoother and prevent model explore loss effectively

- Run command will work when you build process of docker image start not container start

- set ENV variable if you want to set entrypoint for container execution

- RBF kernel in SVM is used to do non linear classification

- SVM can  not work with sequential data

- RNN is for sequential data like speech, text while CNN is for image, video data where spatial hierarchies it can capture.

- max 30 hP can be given to tune in HP tunning in SM

- if cost of being positive is very high we can not mis classify it as negative , we can not afford FN, take care more for it.

- Precision tuning in SageMaker can be set up using hyperparameter optimization jobs targeting the precision metric.

- Kinesis analytics provide on the fly transformation by using sql on streaming data

- variability of negative class if poorly presented in data set then data augmentation technique can be useful

- sci-kit process as single node (instance) process for data pre-processing or training. not distributed on multiple instances - so not useful for vast data reprocessing

- KDS & KDA can not directly send data to s3

- firehose and lambda can send data to s3

- InvokeEndpoint can not capture by CloudTrail

- If a random variable is a continuous variable, its probability distribution is called a
continuous probability distribution.

Security - use SM without optming up Internet
- Disable internet access when specifying VPC for your network
- Use VPC interface endpoint (Private Link) to allow connections needed to
train and host yourmodel
- Modify your instance’s security group to allow outbound connections training
and hosting.

- Security - control access to SageMaker notebooks to specific IAM groups
=> Attach tags to the groups of SM resources to be kept private to specific groups,
and use ResourceTag conditions in IAM policies.

- Amazon Athena recently added support for federated queries and user-defined functions (UDFs),
both in Preview.


- Athena to run extract, transform, and load (ETL) jobs
- we were able to process data from DynamoDB, convert that data to Parquet, apply Snappy compression, create the correct partitions in our AWS Glue Data Catalog, and ingest data to Amazon S3 by using AthenA

- In collaborative filtering, we are given partial information, and the task is to fill up the
missing entries (e.g. Netflix problem). ===== KNN is collaborative
In clustering, typically entire information is made available. In other words, features and for
all objects are given. The task is to group objects together ====k-means

- By streaming in your data directly from Amazon S3 in Pipe mode, you reduce the size of
Amazon Elastic Block Store volumes of your training instances.

- Local run of SageMaker (when no internet)
 pre-built TensorFlow and MXNet containers
 latest version of the SageMaker Python SDK with pip install -U sagemaker
 CPU (single and multi-instance) and GPU (single instance)
 it uses Docker compose and NVIDIA Docker.
 pull the Amazon SageMaker TensorFlow or MXNet containers from Amazon ECS with public Amazon ECR repository in local

- increasing the learning rate with an increase in mini-batch size has been shown to reduce the impact of having a large mini-batch on model quality.

- GAN (Generative Adversial Networks) - oversampling - best

- add more data to reduce overfitting, ensembling technique is also for remiving overfitting.... noice will be in both over & under fitting   ======= overfitting --- sprinkle some noice,   underfitting --- remove noice

- Vault Lock for enforcing policies --- write once read many policy - for long term enforcing lock on data - compilance cotnrol vault lock is on policy not on bucket - immutable policy

- IAM is global resource - like all roles, users, group, policy you create they are available across all region which you want to access(cloudwatch, sagemaker, s3 are region specific to get to work with)

- XGBoost dont require scaling, normalization, one hot encoding, if requried for binary value  give 0,1(label encoding) for categorical values do label encoding.





















- Speed up batch job == max payload size and no of concurrent transformation

- if scaling not working properly increase cool down period after scale out operation so it can get enough time to get ready for fully operational instance

- Package a TensorFlow Estimator with the Inception model inside a Docker container, then deploy this container for model training in SageMaker.

- Develop custom SageMaker-compatible TensorFlow code to integrate the Inception model for the training process.

- KDS == collect, process, and analyze large streams of data in real time