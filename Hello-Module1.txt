Date : 03-04-2024

s3

- objects(files) stored in buckets(directory)
- objects have key which is full path of file 
- max ob size 5TB
- object tag(key=> val pair) for security and lifecycle
- support any file format
- data partitioning : to query data more faster by putting in diff day, moth, year or product id folder
- aws glue will partition data for us
- by lifecycle you can change storage calss for object

s3 standard
- sustain 2 concurrent failure(default 3 availability zone)

s3 IA
- less frequent but rapid access 
- data recovery and backup

s3 glacier
- price - for storage and retrival 	
* instant retrival : milisec retrival - when need to access data once a quarter
* flexible retrival : expedited(1-5 min), standard(3-5 hr), bulk(5-12 hr) - free, 90 days min duration
* deep archive : standard(12 hr), bulk(48 hr) - 180 days min duration

s3 inteligent tiering
- not retrival charge as no retrival

FAT : default
IFAT : not used since 30 days
archive instant access tier : since 90 days
archive access tier : 90-700+days
deep archive access tier : 180 - 700+

Lifecycle rules
- to transit object from one tier to another
transition action : based on condition move obj
expiration action : delete obj if no access, delete if incomplete upload since many days, delete old version of obj
rules can be created for prefix and object tag


use case questions

- if say easily recreated file => can be put in s3 one zone IA
- if want to delete some file => set lifecylcle configuration for that condition
- if file need to access immediately => s3 standard
- if days are concern than = add lifecycle rule with s3 Standard
- if waiting is there for access retrival => use glacier as per waiting time period

To recover after deleting

- enble versioning
- under version delete is marker => from that you can retrive file
- noncurrent version = not topmost version
- to recover => put in standard IA (if less than 90 days)
- to recover => put in s3 glacier deep archiev if days are more than 180

How can you decide which lifecycle rule will be better 

- amazon s3 analytics will help you to decide when to move object to right storage class
- work for standard and standard IA 
- not work for one zone and glacier
- update daily
- take 24-48 hours to analysis


s3 
- 'Block all public access' - setting while creating bucket to stop public access ensurity


Amazon s3 Security

User based
- by IAM policy(api call are allowed or not)

Resource Based
- bucket policy
- object Access control list
- Bucket access control list

s3 obj can be accessible if
( IAM policy(user) or resource policy(resource) allowed it ) and not explicit deny on it


Encryption : encrypt object for security

4 methods

1. Server Side Encryption(SSE) by Amz S3 manged key(SSE - S3) - 
	- default enabled (key will owned, managed and handled by AWS only)
	- user will never get it
	- encryption type AES- 256
2. Server Side Encryption(SSE) by Amz Key mangement service(SSE - KMS)
	- you can manage key by KMS (amz service)
	- user can control + audit by cloudtrail(amz log)
	- encryotion type aws:kms
	- for generating and decrypting KMS key api call happen
	- if s3 having massive no of objects than all are by KMS
	- access to objects can be go in thread link kind of case(multiple api calls)(limitation of KMS)
	- AWS/S3 KMS default key is free,  not cost you any money(its usage will decrease api call to KMS service)
	- DSSE-KMS : Dual layer KMS encryption, more stronger. 

3. Server Side Encryption(SSE) by Customer provided key(SSE - C)
	- keys are only managed outside of aws - customer provided
	- not stored on aws
	- https must to use
	- key must passed to every http request in http header
	- can be accessible by CLI only not in console

4. Client side Encryption
	- client entrypt the data before sending to s3
	- client can use client library like amazon s3 client side encryption library
	- cleint side decryption after getting file from s3

Encryption in transit/flight(SSL/TLS)
	- HTTPS - for in transit/flight
        - HTTP - not encrypted
	- for SSE-C https is must

Force Encryption in Transit
	- with help of bucket policy
	- aws:SecureTransport <- condition should be true otherwise deny access to object


s3 bucket policy

- json based policy
	resource : bucket or obj
	effect : allow/denny
	action : set of api for allow /denny
	principal : account /user on which policy will apply
- use case
	grant public access to bucket
	object to encrypt at uplaod
	cross account access


Default Encryption VS Bucket Policy

- default encryption is by default apply on s3 object (can change that encryption type like s3/kms/dkms)
- we can enforce encryption by using bucket policy(which apply first do access/deny calculate based on that)
- set condition in b policy like sserver side encryption is kms or set condition like customer algorithm true

EC2 Instance
	- set IAM role for granting access permission


VPC(virtual private cloud) Endpoint Gateway for S3

- to access s3 by public access - internet gateway- will be used and AWS:SourceIP(PublicIP) will be specified in bucket policy
- to access s3 by VPC private -VPC endpoint gateway- will be used and AWS:SourceVPCe(one or few endpoit) or AWS:SourceVPC(all possible endpoint) will be specified in bucket policy (in private connection no public traffic will be there)  


$$$ AWS Kinesis $$$$

- managed alternative to APache Kafka
- use for gathering application log, IoT, click stream, metrics
- realtime meand kinesis
- data replicated in 3 AZ automatically

Kinesis Stream 
	- low latency , streaming ingest at scale
Kinesis Analytics 
	- perform realtime anlytics on stream using sql
Kinesis Firehose
	- data delivery to s3, redshift, elastisearch
Kinesis Video Strems
	- realtime video stream service

Streams are devided into shard
- default data retension is 24 hours can go upto 365 days
- reprocess/replay data
- multiple app can consume stream at once
- once data inserted can not be deleted
- records upto 1 mb - small stream

Kinesis Capacity Mode

Provisioned mode(sepcified mode)
- choose no of shard can scale up/down by api or manually
- shard IN - 1MB / 1000 records per sec
- Shard OUt - 2MB
- pay per shard provisioned per hour

On Demand Mode
- shard IN - 4MB/ 4000 records per second
- scale based on last 30 days usage
- pay per stream per hour and data in/out per GB

Kinesis Data Stream

Producer(write)
- 1mb/1000 records per shard
- beyond above capacity exception will rise

Consumer(read)
- 2mb/shard accross all consumer
- 5 api call/sec per shard accross all consumer 

- so to get more capacity need to add shard 


Kinesis Data Firehose

- data delivery service
- kinesis data stream/ cloudwatch/ IoT device to firehose
- firehose read 1mb per sec and create batchs(data  transformation can be done by aws lambda )
- all batches will write on destination like s3, redshift(by copy command from s3 only),amazon elasticsearch(data with index)
- can write on 3rd party destination(mondodb, splunk) or secure http endpoint 
- failed delivery/ transformation data will be stored in s3
- as it create batch (in buffer) so near real time service
- data conversion(csv/json to parquet/orc)/ transformation by aws lambda csv=> json
- support compression for s3 destination
- pay for only data go through it


Kinesis Data Stream Vs Firehose

stream 
- to built realtime application(cusom code) by producer and consumer to read and write data
- data storage 1-365 days, multiple consumer can replay 
- realtime 70 ms- 200 ms latency
- auto sacle on demand
firehose
- delivery/ingestion service
- near real time, not real time
- no data storage

Kinesis Analytics
- streaming ETL : select column and make simple transformation on streaming data
- create metrics on streaming data
- responsive analytics : check some critria and set some alert/filter based on threshold value

features:
	pay for recource consumed not cheap
	serverless
	IAM permission to access source and destin
	SQL/Flink	
	schema discovery
	lambda can be used for preprocessing


ML on Kinesis Data Analytics(Manged Apache flink)

two algo

1. Random cut Forest
- sql function is used to find anomaly(something different) in numeric column in stream
- work on recent history
- so if data change algorithm will change over time
- freqent changing algo

2. HOTSPOTS
- locate and return dence region in your data
- less changing algo 

Kinesis Data Analytics + Lambda

- destin will be labda
- tranforming data, translating n enriching data,
- aggreagation data, encryption data
- data can go in any service which lambda can talk like aurora, red shift, dynamodb, sqs, cloudwatch, sns, s3

Apache FLink
- Managed service for Apache flink(Kinesis data analytics)integrate flink with AWS
- its a framework for processing data stream
- supports python and scala
- you can develop entire flink application and load it to managed service for apache flink from s3
- load app in s3 which is manged by aws for hosting and loading and other all server things
- in MSAF table API are there for sql access &data stream api are already for use there 


kinesis video stream
- for sending video from different video generation device like
- security camera, smartphone camera, aws deep lence, radar data, rtsp camera, audio feed, image, body worn camera - all called producer
- one video stream per producer
- kvs has playback capability
- counsumers are : ML app by mxnet, tensorflow, sagemaker, amazon reckognition video
- data retension for 1 hour to 10 years

KVS use case in ML

- KVS consume videos from fargate or ec2 instance application
- check point by dynamodb to check all video consumed completely
- now send video frame in decode format to sagemaker to perform inference
- based on result create kinesis data  stream that can be used by other aws service like lmbda func
- and create alert or perform prefered action based on that data(like threat detection n all)


KDS => creating realtime application by ml like evaluating stream of data by sagemaker
KDF => to ingest massive data near real time data, which can be used by ML
kDA =>  real time ETL/ real time ML algo like random cut forest or hotspot fiding on stream, sql can be apply on them too
KVS => time video stream for ML alog analysis



GLue Data Catalog

- metadata repository
- schema are version of tables
- integratted with athena and redshift 
- glue crawler will create glue data catalog

Glue Data Crawler

- crawl through data and infere schema and partition
- work on json, parquet, csv, relational store
- work for redshift, s3, amazon rds
- run on schedule or on demand
- need IAM role/ credential to access data store
- partition : partition data on device or time based, check which kind of data (query)access you want
	      device based or time based
  	      device based : device/yyyy/mm/dd , time based : yyyy/mm/dd/device

Glue ETL

- code in python scala, pyspark script or spark
- target will be GDC/redshift/s3/RDS
- cost effective, pay for resources used
- if you have spark job, they can run on spark paltform - all process managed by Glue ETL
- glue schedular and glue trigger will handle schedule and event trigger to do etl job
- provide data transformation
	Bundle Transformation : drop field, drop null field, filter, join, map(add/delete/map to some field)
	ML tansformation :  FindMatches ML - find duplicate or matching records in your dataset, even when records do not have common
			    Unique identifier or fields are not matching exactly
- support format conversion into csv, json, avro, parque, orc , xml
- apache spark transformation : k-means algo

Glue Data Brew

- user interface based normalization and cleaning of data
- 80%faster
- 250+ already made transformation like - corect invalidval, data conversion, filter anomalies
- source can be any

$$$ parquet
- column oriented data storage format in apache hadoop
- nested structure can be stored in it - to store complex data
- relatively small file than csv


Data Stores For ML

### Redshift
- data warehouse
- sql base - online analytics process
- paralllel query 
- load data from s3 to redshift
- redshift spectrum to query data in s3 direct no loading to redshift
- used for analytics in ML

### RDS, Aurora
- both realtional db
- sql base- online transaction processing
- data stored at row level
- not directly used for ML


### DyamoDB
- noSQL db
- serverless
- specify read/write capacity for it to work
- store ML model on dynamodb
- output of ML model will be stored on DnDB

### S3
- object soter
- serverless
- infinite store
- center for aws service

### Opensearch - ElasticSearch
- indexing of data
- to search
- clickstream analytics

### Elastics Cache
- data cacheing meachnism
- fast access
- not for ML


Data PIpeline

- Move data from one place to another
- etl service
- s3, rds, redshift, EMR, Dynamodb
- actual ETL  will not happen in pipeline
- it will happen in ec2 instance managed by pipeline
- retries, notification on failure'
- datasource on-premise
- it wont fail.

example : 
you have RDS database/ dynamoDB and you want to move it to s3
- data pipeline will orchestra that process
- it will use EC2 instance managed by pipeline to transfer data to s3
- from s3 you can use sagemaker and built ML appl

Diff between GLue ETL and PIpeline ETL

Glue ETL
- glue etl- run apache spark code, scala/python do etl, focus on ETL
- not worring about managing or configuring resources
- data catalog make data availabel for athena and redshift spectrunm
- resources are belongs to aws 

Pipeline ETL
- orchestration service
- more control on environment , resouces and code
- can access EC2, or EMR instance -  as all instances created from your own account

AWS Batch
- run batch job as docker images
- dynamic instance provision for you like EC2/Spot instance
- no manage cluster, serverless
- pay for instance only
- by cloudwatch event schedule batch job
- or by step fucntion run the batch job

Diff glue and batch

glue - apache spark code, run on scala/python, focus etl, no need to worry managing resources, data calaog make data available for redshift/athena

batch - any computing job in sense any.. regardless job(docker image is must)
	resources from your account, so batch can acess it anytime 
	if you have data to(to transform) do ETL job than glue will work
	but if non-ETL related work, batch oriented then do batch job

DMS - Data Migration Service
- quick & secure migrate db to aws
- source db remains available while migration
- homogenous migration : like oracle to oracle
- heterogeneous migration : like MSQL to Aurora
- we have to create EC2 instance to replicate the migration task
- continouse data replication

AWS Step Function
- orcestra or desing workflow
- visulization
- error handling , try, retry cache outside the code
- max time for step execution is 1 year
- we can check workflow history
- if one task happen, then another, then another aws step function is there to orchestration of those task


AWS DataSync
- migration for on-premise data to aws storage service
- use data sync agent and transfer encrypted data by aws direct connect/ internet to aws datasync to destine datastore

MQTT
- internet of Things protocol
- getting data from sensors everywhere and feed data to repository
- standard messaging protocol used in IoT world
- MQTT is protocol to connect aws IoT device sdk
	