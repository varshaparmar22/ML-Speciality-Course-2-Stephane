https://gmoein.github.io/files/Amazon%20SageMaker.pdf ===========  important for sagemaker --- must read

Date :  04/06/2024

==> Build your own alexa

-> input = speech , output = speech
-> amazon transcribe -> amazon lex automated designer -> amazon polly

==> Build your own universal translator

-> input = speech , output = speech
-> amazon transcribe -> amazon translate -> amazon polly

==> Build your own celebrity detector

-> input deeplence device and associate with recognition and fire alarm is show celebrity specified
-> aws deeplence -> amazon recognition

==> Build system to check people calling you are happy or sad or sentiment any

-> input = speech, output = sentiment with score
-> amazon transcribe -> amazon comprehend


@@@@@@@@@@@@@@@@@@@ Algo Content Type @@@@@@@@@@@@@@@@

ContentType============>Algorithm
application/x-image============>Object Detection Algorithm, Semantic Segmentation
application/x-recordio============>Object Detection Algorithm

application/x-recordio-protobuf============>Factorization Machines, K-Means, k-NN, LDA, Linear Learner, NTM, PCA, RCF, Sequence-to-Sequence

application/jsonlines============>BlazingText, DeepAR

image/jpeg============>Object Detection Algorithm, Semantic Segmentation

image/png============>Object Detection Algorithm, Semantic Segmentation

text/csv============>IP Insights, K-Means, k-NN, Latent Dirichlet Allocation, Linear Learner, NTM, PCA, RCF, XGBoost

text/libsvm============>XGBoost

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

@@@@@@@@@@@@ model implementing @@@@@@@@@@@2

accuracy => accuracy on training data
val_acc => accuracy on unseen data <- to check overfitting check this it should be high for no overfitting

- if overfitting do regularization based on algo we will choose
- if cnn then choose dropout layer to overcome overfitting

- high batch size will stuck model in local minima not good solution
- high learning rate is also same overshoot the correct solution
- small batch size required small learning rate

@@@@@@@@@@@@@@@@@@@@@ important points to remember @@@@@@@@@@@@@@@@@@@@@2

- mostly web application use port 8080

- standard http port is 80

- model artifact compressed in tar.gz file format

- if binary classification problem and threshold settings is there in option it can be most approx.  answer for FN/FP related issue

- Encryption needs to be implemented at the application layer (such as SSL/TLS) or by using AWS services designed for encryption

- Boosting- model training technique - increasing the accuracy by giving more weight to difficult to predict observations in sequential model training.

- if skewed(feature) dataset, and you need to uniform it by binning-apply quantile binning to convert it into bins having equal no of observation

- Training for more epochs often leads to overfitting, as the model starts to memorize rather than generalize from the training data.

- silhouette score if >1 data point very far from nearest cluster, 0 on one cluster or on boundary of two cluster, -1 assign in wrong cluster

- EMRFS -Hadoop file system API, allowing direct interaction between EMR-hosted applications like MapReduce & S3. By using the `s3://` file prefix.

- S3A (Amazon S3A File System) Hadoop-compatible interface for accessing data in S3-accessing public data sets without requiring AWS credentials.

- Amazon Comprehend use LDA feature - for topic modeling

- if NN has more layers than use ReLU as it solve vanishing gradient problem because of more layers for convergence

- if accuracy is not as required you can change activation function 

- if convergence issue than it is indicating problem with gradient loss - use Relu to solve it - specially if Network is complex already

- if outliers are in dataset we can use median values for imputation

- labeling by ground truth incorporates both precision and speed as it provide automatic labeling by self learning algo and human reviewing

- Reckognition in aws if custom logo or character you need to identify, for train reckognition with labeled dataset(even a small one). final model will be created and trained and you will get api for that model to use for your specific use case - you need labeled dataset for custom reckognition

- blazing text can handle n-gram technique so it break word in subword and handle OOV by aggregating subword vector so good performance on morphologically rich language.

- access to s3 in sagemaker handled by IAM roles & policy not by bucket access control list(ACL) - resource based access control 

- To protect your data in transit within the AWS Cloud, Amazon Redshift uses hardware accelerated SSL to communicate with S3 or DynamoDB for COPY, UNLOAD, backup, and restore operations.

-Trusted Advisor checks security groups for rules that allow unrestricted access to a resource. Unrestricted access increases opportunities for malicious activity, such as hacking, denial-of-service attacks, or loss of data. 

- while transferring data from firehose to destination, data transformation will be happen in lambda function

- The lambda timeout value default is 3 seconds - very less to do some transformation of data sometimes.

- Firehose supports Amazon S3 server-side encryption with AWS Key Management Service (SSE-KMS)

- AWS auto scaling lets you scale other resources &services like ECS, dynamodb tables, aurora replicas etc just like EC2 instance scaling(add more as need)

- The AWS Glue Data Catalog is an Apache Hive metastore-compatible catalog

- to attach kinesis video stream to sagemaker direct amazon kinesis video stream inference template(KIT) is used.

- Glue classifier send certainty string either 1.0=> all schema are 100% sure or Unknown or 0.0 => all schema are not classified from given data

- Reckognition can fail to identify face if it is trained on single image per person - it need to train on multiple image of same person with different aspects like beard, glasses etc

- random forest(Supervised algo-multiple dT classification & regression) prevent overfitting just like bagging technique

- Entropy uses the probability of a certain outcome in order to make a decision on how the node should branch. Unlike the Gini index, it is more mathematical intensive due to the logarithmic function used in calculating it.

- AWS Security Token Service (AWS STS) is a web service that enables you to request temporary credentials for use in your code, CLI, or third-party tools

- Access Advisor to check when service or resource last accessed and by whom

- AWS Systems Manager Parameter Store - to store parameters like credential, licence key or configuration parameters

- AWS Secrets Manager - helps you manage, retrieve, and rotate database credentials, application credentials, OAuth tokens, API keys, and other secrets throughout their lifecycles. - built-in integration for MySQL, PostgreSQL, and Amazon Aurora on Amazon RDS

- Hinge loss is related to support vector classifier, and it not specify probability - Hinge loss is also known as max-margin loss. 

- Heuristic approach is problem-solving methods(mathematical) that are based on practical experience and knowledge, when you are not applying ML approach

- recorIO => 4 bytes representation

- the macro average F1-measure is used to evaluate the predictive success of a multiclass classifier. 

- model quantization reduce model size and inference time for edge device with little loss of accuracy.

- for edge device cloud based inference will introduce latency

- knowledge distillation is creating new model having smaller size, but need to train it and optimize for edge device

- image down-sampling will risk loosing smaller details

- logarithmic, robust standardization transformation will remove effect of outliers

- CloudWatch will show matrices at 1 minute frequency 

- comprehend can work on different languages like Arabic, Chinese (simplified), Chinese (traditional), English, French, German, Hindi, Italian, Japanese, Korean, Portuguese, and Spanish

- review residual for regression problem

- residual = true target - predicted target

- resource based policy are only inline, identity based policy are inline and managed 

- resource level permission is associated with ARN

- AWS organisation service control policy will limit permission of root user

- connect to SM api/runtime by interface endpoint only(it is Elastic Network Interface - powered by aws PrivateLink) from VPC - you don't need any gateway, direct connect, public internet, VPN connection for this. it assign private IP address to your VPC for accessing directly. connect aws service and VPC

- Unload command is used to write result of query from redshift to some textfiles

- Kinesis data stream application = consumer of streaming data

- Lex is useful for creating conversational interface by using text and voice

- Ip insight generate the score for anomalous pattern of given event/observation.

- tanH activation function is used mainly for classification between two classes, return  : -1, 1

- image classification algo is multi-label classification algorithm

- for image classification/other algo If want to use GPU, include CUDA toolkit in container, not import NVIDIA driver in docker image, your container should  be NVIDIA-docker compatible 

- aws batch is work with EC2/spot/fargate/farget spot instance ans assign compute resources

- AWS glue generate python/scala code automatically - it is visual/code based interface for etl job

- validation data set associated with development state of model. evaluate model performance at development stage

- in framework developer can do more customization like custom training code can be created/use diff validation technique like k-fold... like TF, MXN kind of framework, while in built in algorithm it is not possible

- drop feature if it has LOW variance(how the value of feature spread around means) - if it is so nearer from each-other, model will not learn pattern well

- redshift spectrum will query data from directly S3 without loading data into redshift, the spectrum layer will do all processing and aws provide all resources for that so like this multiple redshift cluster can access s3 data concurrently-it is fast too because of parallelism

- for KNN feature_dim- no of features, k-no of neighbour, predictor_type, sample_size are required parameters

- For kinesis firehose delivery stream take 2000 transaction, 5000 records or 5MB data - by PutRecordBatch or PutRecord api operation

- CloudWatch monitor/log/event work and help you to monitor aws services, along with cloudtrail log(which log api calls)
  monitor => monitor and set alarm
  log => you can monitor, store, access log from ce2, cloudtrail and aws services
  event => deliver the stream of system events show aws resource changes

- Amazon Inspector : security assessment service

- AWS Config : manage aws resource inventory, configuration history, configuration changes notification useful for security and governance. How resource was configured at given point of time.

- Lambda will not handle ETL

- Data Pipeline will do data processing and data movement by schedule, it has BUIL-In capability of moving data between s3 and RDS, running query on s3 log data

- Specify all containers and order of them to run on related EC2 instances in inference pipeline as http request, for normal only one PrimaryContainer has to be specified

- Inference Pipeline is immutable, but UpdateEndpoint will help to deploy new endpoint to update IP for experiment

- Direct data stream can not be captured by kinesis analytics, it takes data from kinesis data stream or Amazon MKS(Kafka)'

- Glue Crawler can accept exclude and include pattern folder name to crawl through

- when SM training start by api SM will replicate entire data set on ML compute instance, to replicate subset of data on each compute instance s3datadistributiontype = shardedbys3key

- SM debugger will track value of parameters if they are getting too large or small while training

- your docker container image path will be specified in CreateTrainingJob api call

- levenstein distance is Minimum number of single character edit in one word to convert it into another word. Distance between two words

- for model do 1. assessment(at least once in 6month), 2. retrain(after checking if performance below threshold), 3. rebuild(performance not getting better)

- HP tuned against validation DS as it helps in learning process, parameters are derived from training like weight and bias.

- No inter-node communication for batch processing(getting inference)

- everything is encrypted 'at rest' and 'in transit'

- command-control communication in between service control panel&training job instances, communication between nodes in distributed training & processing job. <= NOT  Encrypted

- if MEDIAN value of objective metrics of current running job is bad than previous one at same epoch than stop training

- data from /SageMaker folder will persist(stay) between start-stop of notebook instance session.

- hold-out set is 20-30% of training data - to evaluate model while training and getting HP

- model.tar.gz => model_algo-1 = serialized Apache MXNet object

- auc-roc score will predict higher score for positive example rather than negative example

- if you want to identify cyclic pattern based on date feature, convert it into hd, dw, wm, wy, dm, my place them on circle with x, y coordinate with sin, cos transformation.

- one-hot encoding useful for categorical data transformation, label encoding is useful converting label into incremental integer format.

- bivariate - 2 attributes, multi-variate- more than 1 attribute anaysed, check their pattern, distribution, relation with each other.

- without SM - if network isolation then container can not perform inbound/outbound connection to even to amazon service like s3.
- with SM - with help of execution role download and upload operation is possible in s3
- in distributed training - in/out bound traffic are enabled for related training/ inference containers

- Amazon personnelize will create custom model api by using your data and create model train, deploy and create api for it for your recommandation system, just give data you will have api for recommandation, it identify features and train model by itself and create custom model api.

- LDA is a BOW model, in it specify the probability of topic distribution over the word - its generative probability model, not discriminative(logical) model(which specify how output map to input), so it will not give semantic meaning of word

- if Glue is available in given region  than glue data catalog will be useful for querying data from s3, or athena has its own data catalog to access for querying data from s3 if glue is not available in given region.

- do categorical encoding if it is ordinal - according to order like size s,m,l,xl etc
- categorical encoding === assign numerical value to category, 1,2,3...
- label encoding === assign numeric label to category like "1","2","3" or it can be 1,2,3
- target encoding === calculate numeric val -- based on relationship between categorical feature and target value, assign that to feature
- VPC can be enabled for training/inference by specifying VpcConfig params in createTrainingJob, createHyperparametersTunningJob, CreateModel api call

- Network Isolation can be enabled for training/inference by specifying EnableNetworkIsolation params in createTrainingJob, createHyperparametersTunningJob, CreateModel api call

- in model from Aws MarketPlace network isolation and VPC required

- chainer & reinforcement learning not support network Isolation

- KPL library simplify creation of producer application development
  for large complicated data writing it manage all logic, batching, threading, de-aggregation - so give high performance
  on customer side if KCL integrated the KPL integrate without much effort, if KCL not integrated(api-getrecord  is used) even though KPL can be used to get per user record
  KPL emits throughput, error, other metrics to cloudwatch
  Asynchronous architecture : it create batches and not expect the receiver should be ready all the time to receive, it create intermediary object that receive the data and continue getting new record. whenever requester is ready to receive it will same behaviour as asynch client in aws SDK 
  RecordMaxBufferedTime is param for specify delay that app can handle, if app cant handle this delay use aws SDK

- EFS is for Linux(and for heterogeneous app) not require any management, FSx is for windows - require management - both can handle multiple LAMBDA

- if you want to use EMR spark for your SM - you can use SM spark library/livy/sparkmagic - where EMR is there already

- oRC -tabular data(row column formate), parquet - columnar text data storage

- test million of sample rapidly called throughput if need to improve IOPS than change throughput, throughput is associate with EFS
  two throughput mode associate with EFS 1. Busrting throughput mode  2. Provisioned throughput mode
  1. bursting TP mode :  if IO limit reach 100% then increase storage size automatically
  2. Provisioned TP mode : adjust provisioned throughputmode setting to expand max IO limit

- max I/o performance aggregate throughput which is related to multiple connection to EFS system , it can introduce latency for file operation

- AWS Client VPN useful for getting secure connection between client device to aws services/resource over public internet

- Establishing a NAT Gateway enables outbound internet connectivity for resources within private subnets, without restricting public internet access. Contrary to the objective of securing SageMaker notebooks by mitigating public internet exposure, a NAT Gateway permits instances in private subnets to initiate connections to external services, while preventing any inbound attempts, thus maintaining a balance between connectivity and security. NAT Gateway will not do any data encryption.

- AWS PrivateLink use only aws private network only by creating VPC endpoints, not public internet

- SM support FastFile mode as input mode too, same code as file mode no code changes, it directly access data from s3 now so it is very fast. if data downloaded local or streaming from s3 it can work with in both, no additional charges, convienent access of local file and performance of s3 data access both feature provided by FastFile mode. It use EBS storage so cost associated with it.

- if normal file mode is used data will be downloaded in encrypted format in amazon EBS volume whole than can be accessible.

- EnableSageMakerMetricsTimeSeries  associated with monitoring - data for cloudwatch

- choosing KNN, Linear regression for imputing missing value - if features have linear relationship specified than we can use LR, otherwise most probably KNN will be better approach, KNN can handle complex relationship if dataset have.

- Lex can handle speech-to-text and conversational logic

- Shuffling will avoid bias and improve generalization,However, in time-series or sequential learning tasks, preserving the order of data is important.

- in blazingtext skip-gram technique extend to n-gram technique too which break large word into subword and create vector of it and aggregate those sub-word vector for handling OOV, morphologically rich language issue

- by default SM role attach to sm notebook give access to those bucket which has 'sagemaker' word inside their name. 

- skip gram can understand sematic relationship between words 

- ssl/TSL is data encryption technique for in transit.Encryption needs to be implemented at the application layer (such as SSL/TLS) or by using AWS services designed for encryption. Not on network connection rules.

- skewness can be handled by quantile binning as all bin will have same no of data points in it.

- Quick Sight has outlier/anomaly detection, forecasting feature, impute missing values, auto narratives to create graph kind of built in ML features

- 8080 port is often used for web applications, including custom inference containers in SageMaker.

-  big batch_size will lead to bad generalization as it make GD path smoother and prevent model explore loss effectively

- Run command will work when you build process of docker image start not container start

- set ENV variable if you want to set entrypoint for container execution

- RBF kernel in SVM is used to do non linear classification

- SVM can  not work with sequential data

- RNN is for sequential data like speech, text while CNN is for image, video data where spatial hierarchies it can capture.

- max 30 hP can be given to tune in HP tunning in SM

- if cost of being positive is very high we can not mis classify it as negative , we can not afford FN, take care more for it.

- Precision tuning in SageMaker can be set up using hyperparameter optimization jobs targeting the precision metric.

- Kinesis analytics provide on the fly transformation by using sql on streaming data

- variability of negative class if poorly presented in data set then data augmentation technique can be useful

- sci-kit process as single node (instance) process for data pre-processing or training. not distributed on multiple instances - so not useful for vast data reprocessing

- KDS & KDA can not directly send data to s3

- firehose and lambda can send data to s3

- InvokeEndpoint can not capture by CloudTrail

- If a random variable is a continuous variable, its probability distribution is called a
continuous probability distribution.

Security - use SM without optming up Internet
- Disable internet access when specifying VPC for your network
- Use VPC interface endpoint (Private Link) to allow connections needed to
train and host yourmodel
- Modify your instance’s security group to allow outbound connections training
and hosting.

- Security - control access to SageMaker notebooks to specific IAM groups
=> Attach tags to the groups of SM resources to be kept private to specific groups,
and use ResourceTag conditions in IAM policies.

- Amazon Athena recently added support for federated queries and user-defined functions (UDFs),
both in Preview.


- Athena to run extract, transform, and load (ETL) jobs
- we were able to process data from DynamoDB, convert that data to Parquet, apply Snappy compression, create the correct partitions in our AWS Glue Data Catalog, and ingest data to Amazon S3 by using AthenA

- In collaborative filtering, we are given partial information, and the task is to fill up the
missing entries (e.g. Netflix problem). ===== KNN is collaborative
In clustering, typically entire information is made available. In other words, features and for
all objects are given. The task is to group objects together ====k-means

- By streaming in your data directly from Amazon S3 in Pipe mode, you reduce the size of
Amazon Elastic Block Store volumes of your training instances.

- LDA work based on count of words similar in all documents

- Local run of SageMaker (when no internet)
 pre-built TensorFlow and MXNet containers
 latest version of the SageMaker Python SDK with pip install -U sagemaker
 CPU (single and multi-instance) and GPU (single instance)
 it uses Docker compose and NVIDIA Docker.
 pull the Amazon SageMaker TensorFlow or MXNet containers from Amazon ECS with public Amazon ECR repository in local

- increasing the learning rate with an increase in mini-batch size has been shown to reduce the impact of having a large mini-batch on model quality.

- GAN (Generative Adversial Networks) - oversampling - best

- add more data to reduce overfitting, ensembling technique is also for remiving overfitting.... noice will be in both over & under fitting   ======= overfitting --- sprinkle some noice,   underfitting --- remove noice

- Vault Lock for enforcing policies --- write once read many policy - for long term enforcing lock on data - compilance cotnrol vault lock is on policy not on bucket - immutable policy

- IAM is global resource - like all roles, users, group, policy you create they are available across all region which you want to access(cloudwatch, sagemaker, s3 are region specific to get to work with)

- XGBoost dont require scaling, normalization, one hot encoding, if requried for binary value  give 0,1(label encoding) for categorical values do label encoding.

- Glue ETL: ETL Jobs as Spark programs, run on a serverless Spark Cluster
- MICE use regression to do imputation
- Accuracy metric are published into Cloudwatch

 Accessing SageMaker Endpoints from an App
 AWS api/sdk à SageMaker Endpoint is one way
 API Gateway à Lambda à SageMaker Endpoint is another


SageMaker instance profiles, e.g. to grant permissions to S3
 SageMaker doesn’t support resource-based policies e.g. an S3 bucket policy
 From Notebooks, we can see S3 buckets, and files, but we can’t copy them by
default

EMAIL
- The main protocols are SMTP for sending emails, and POP3 and IMAP for receiving emails. 
SMTP handles email transmission between servers, while 
POP3 downloads emails to a device and 
IMAP synchronizes emails across multiple devices.

- Data standardization when model require scaling (imp) like SVM , KNN
- Normalization negative value will not come - log, one hot encoding, min max scaler, can handle skewed dataset
- standadization std deviation will be used, subtract mean devide std deviation so -1,0,1 range mostly 
- Standardization can help to reduce the impact of outliers
- logistic regression require numeric input

- GLue is the giant which can be useful for all type of transformation of data
- for NLP tasks
  tokenization provide by amazon comprehend inbuilt
  all other techniques like lemmetization, BOW, n-gram, stop word, word embedding(word2vec) need to implement by ski-kit or other tools

Optimization: Gradient Descent   ===== Objective: Find the minima of the “sum of squares” error

overhead == It's the resources required to set up an operation. It might seem unrelated, but necessary.how much "extra stuff" you need to do in order to get something.

- Remove feature which has low co-relation with target variable in multi-variable linear regression

Range: The difference between the maximum and minimum values in the dataset 

Variance: The average of the squared differences from the mean 

Standard deviation: The square root of the variance, which is expressed in the same units as the data 

- AWS Batch supports GPU instance with some exception like G5g, G4ad

Interquartile range (IQR): The range of the middle 50% of the data points 

Mitigate Statistical dispersion varies among features Problem =====
Dividing the dataset into training, validation, and test sets 
=> applying feature scaling to the training set can help mitigate the impact of varying statistical dispersion among features.
=> replicating the scaling parameters on the validation and test sets, the model can make more accurate predictions on unseen data

- if  imbalance is there then stratified cross validation will be best approach

- for DNN workload use AWS deep learning container not AWS ECS
- best cost effectiveness if spot instance

- Naive Bayesian assumes conditional dependency between feature, while full allow statical dependency between features.

- TFRecord can be supported by SM directly, no need to change format

- SM script mode = no need to manage & create containers, done by script

- Restrict the access to particular instance by ENI security group(restrict instance access)  - essential too along with endpoint gateway policy(which can restrict the user) - for securing SM service api call
- security group ====> resource level(EC2 instance), ACL ====> subnet level

- to tansfer data daily dont use DMS - instead GLue can be used with VPN site-site transfer is available in option

- Enabling PerformAutoML and PerformHPO can significantly enhance the model's predictive accuracy === Amazon Forecast

- can use lifecycle configuration script to ensure pre-install requried s/w package needed on notebook instance to word

- if ques is related to word frequency then most probably ans will be tf-idf related - work on  frequency and rarity in the corpus of word
- PCA is dimensionality reduction and finding out co-relation between features

- SM role is attached to s3 bucket & KMS keys' policy ===> for ensure security of data access and decryption by key  == role IMP
- L1, Recursive Feature Elimination will remove less imp features from data, so no feature transformation even give good performance

- for custom algorithm you are creating in SM then ECR & S3 are important, if containerized app then ECS will be imp

- if two features are linearly dependent on each-other then they can cause singular matrix in optimization step

- deploying any model in SM - containerization - (ECR step) is must- register and upload image to ECR

- dataset size is important for optimization before considering HP optimization to correct result

- LSTM, GRU ==> model are not considering word embedding - so if issue is not capturing meaning of data then go for word-embedding opt

- LSTM = more params, take more train time, for small dataset
- GRU = less params, take less train time, for large dataset
result will be significantly same

- Amazon CloudWatch for real-time monitoring of SageMaker endpoints, including GPU and CPU utilization, and capturing error metrics for invoked endpoints.
- AWS CloudTrail for monitoring API calls, including model deployments and endpoint invocations, providing an audit trail 
- AWS Trusted Advisor  optimizing AWS infrastructure and cost management
- AWS Config for configuration management and compliance auditing of AWS resources

egress === leakage

- 1. VPC interface endpoints with AWS PrivateLink 2.network isolation for SageMaker jobs 3. restricting notebook access to specific IPs ====== safeguarding financial data within Amazon SageMaker. 

- to increase ingestion rate in S3 ===
  increase no of shard in kinesis data stream
  so they do parallel processing of shard
  so data can go fast in s3

- Bounding box annotations are commonly used in medical imaging for precise object localization

- for fradualnt transaction we need to consider TPR - recall & PR auc curve
